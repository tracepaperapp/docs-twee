{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Tracepaper # Introducing Tracepaper by Draftsman, a revolutionary tool designed to simplify the complexities of building business applications. While the landscape is rich with low-code tools to aid development, the challenges of managing these tools in production can be daunting. From maintaining performance to ensuring granular access control and backups, the operational demands can strain businesses lacking a robust engineering department. Enter serverless services offered by major cloud providers like AWS, promising to alleviate this operational burden. \"Serverless is how the cloud wants you to build applications,\" as Gregor Hohpe puts it. But is this approach practical? In practice, serverless architecture introduces a high level of runtime granularity, where each line in your domain model translates into network calls fraught with access control, latency, and security considerations. It requires a shift from operational competence to distributed system design proficiency. However, if your expertise lies in your domain rather than distributed systems intricacies, you likely prefer to reason about your domain holistically\u2014a \"Monolith,\" in industry terms. This is where Tracepaper shines, drawing inspiration from Domain Driven Design principles. We empower you to model your domain using distinct boxes connected by lines, representing various elements such as API access points, domain behaviors, and materialized views. Our modeling tool prioritizes data structures and the connections between these boxes, allowing you to focus on the essence of your domain. While we provide modeling concepts for the business logic within these boxes, the true strength lies in the ability to inject custom Python code. Transitioning from pseudo-code to Python is seamless, freeing you from concerns about the connections between your logic. Let us handle the lines, so you can concentrate on the boxes\u2014the core of your domain expertise. The model seamlessly converts into a Python project defined with CloudFormation, AWS's Infrastructure as Code specification. This ensures deployability within your AWS account, granting you full control over runtime parameters such as access and cost. Additionally, both the model and the generated project are stored in your GitHub account, offering complete oversight and ownership. With Tracepaper, we streamline the process of modeling your domain as a distributed system. Let us handle the intricacies, while you focus on what truly matters\u2014your domain.","title":"Welcome to Tracepaper"},{"location":"#welcome-to-tracepaper","text":"Introducing Tracepaper by Draftsman, a revolutionary tool designed to simplify the complexities of building business applications. While the landscape is rich with low-code tools to aid development, the challenges of managing these tools in production can be daunting. From maintaining performance to ensuring granular access control and backups, the operational demands can strain businesses lacking a robust engineering department. Enter serverless services offered by major cloud providers like AWS, promising to alleviate this operational burden. \"Serverless is how the cloud wants you to build applications,\" as Gregor Hohpe puts it. But is this approach practical? In practice, serverless architecture introduces a high level of runtime granularity, where each line in your domain model translates into network calls fraught with access control, latency, and security considerations. It requires a shift from operational competence to distributed system design proficiency. However, if your expertise lies in your domain rather than distributed systems intricacies, you likely prefer to reason about your domain holistically\u2014a \"Monolith,\" in industry terms. This is where Tracepaper shines, drawing inspiration from Domain Driven Design principles. We empower you to model your domain using distinct boxes connected by lines, representing various elements such as API access points, domain behaviors, and materialized views. Our modeling tool prioritizes data structures and the connections between these boxes, allowing you to focus on the essence of your domain. While we provide modeling concepts for the business logic within these boxes, the true strength lies in the ability to inject custom Python code. Transitioning from pseudo-code to Python is seamless, freeing you from concerns about the connections between your logic. Let us handle the lines, so you can concentrate on the boxes\u2014the core of your domain expertise. The model seamlessly converts into a Python project defined with CloudFormation, AWS's Infrastructure as Code specification. This ensures deployability within your AWS account, granting you full control over runtime parameters such as access and cost. Additionally, both the model and the generated project are stored in your GitHub account, offering complete oversight and ownership. With Tracepaper, we streamline the process of modeling your domain as a distributed system. Let us handle the intricacies, while you focus on what truly matters\u2014your domain.","title":"Welcome to Tracepaper"},{"location":"one-pager/","text":"Elevate Your Development with Tracepaper by Draftsman # Transform the way you develop, deploy, and manage your applications with Tracepaper by Draftsman\u2014a powerful modeling tool designed to streamline your workflow and enhance productivity. Key Features # 1. Intuitive Web Modeler: Easily model your application's domain, logic, and interactions using a user-friendly interface. Simplify complex systems with visual modeling and automatic code generation. 2. Seamless Integration: Leverage seamless integration with your existing GitHub repositories and AWS environments. Maintain control of your code and deployment processes while benefiting from automated workflows. 3. Robust Build and Deployment Pipeline: Automatically convert models to Python and CloudFormation code. Conduct unit tests, package applications, and deploy to staging and production environments with minimal manual intervention. 4. Asynchronous Processing: Benefit from robust asynchronous command handling. Receive real-time updates on command status, ensuring responsive and reliable application performance. 5. Comprehensive Testing: Execute API tests to verify application behavior and data integrity before production deployment. Ensure your application meets quality standards and performs as expected. 6. Flexible Role-Based Access: Model complex multi-tenant systems with role-based access controls. Convert query variables to specific roles, ensuring secure and tailored data access. 7. Ownership Clarity: Maintain ownership of your GitHub repositories and deployment environments while utilizing Tracepaper's powerful modeling and build capabilities managed by Draftsman. 8. Generated GUI Assist Framework (Beta): Experience our new GUI Assist Framework, designed to enhance user interface development. Built with AlpineJS and vimesh-ui, this framework offers pre-made components and easy customization. Integrate seamlessly with your GraphQL API for a dynamic and responsive user experience. Why Choose Tracepaper? # Efficiency: Save time with automated code generation and deployment, allowing your team to focus on innovation and quality. Reliability: Ensure consistent and error-free deployments with comprehensive testing and real-time monitoring of command processing. Scalability: Easily scale your applications with infrastructure as code, supporting growing business needs and complex environments. Collaboration: Facilitate collaboration between developers, domain experts, and operations teams with clear, visual models and shared repositories. Get Started # Unlock the full potential of your development process with Tracepaper by Draftsman. Visit our website to learn more and start transforming your application development today. Visit Tracepaper by Draftsman .","title":"One pager"},{"location":"one-pager/#elevate-your-development-with-tracepaper-by-draftsman","text":"Transform the way you develop, deploy, and manage your applications with Tracepaper by Draftsman\u2014a powerful modeling tool designed to streamline your workflow and enhance productivity.","title":"Elevate Your Development with Tracepaper by Draftsman"},{"location":"one-pager/#key-features","text":"1. Intuitive Web Modeler: Easily model your application's domain, logic, and interactions using a user-friendly interface. Simplify complex systems with visual modeling and automatic code generation. 2. Seamless Integration: Leverage seamless integration with your existing GitHub repositories and AWS environments. Maintain control of your code and deployment processes while benefiting from automated workflows. 3. Robust Build and Deployment Pipeline: Automatically convert models to Python and CloudFormation code. Conduct unit tests, package applications, and deploy to staging and production environments with minimal manual intervention. 4. Asynchronous Processing: Benefit from robust asynchronous command handling. Receive real-time updates on command status, ensuring responsive and reliable application performance. 5. Comprehensive Testing: Execute API tests to verify application behavior and data integrity before production deployment. Ensure your application meets quality standards and performs as expected. 6. Flexible Role-Based Access: Model complex multi-tenant systems with role-based access controls. Convert query variables to specific roles, ensuring secure and tailored data access. 7. Ownership Clarity: Maintain ownership of your GitHub repositories and deployment environments while utilizing Tracepaper's powerful modeling and build capabilities managed by Draftsman. 8. Generated GUI Assist Framework (Beta): Experience our new GUI Assist Framework, designed to enhance user interface development. Built with AlpineJS and vimesh-ui, this framework offers pre-made components and easy customization. Integrate seamlessly with your GraphQL API for a dynamic and responsive user experience.","title":"Key Features"},{"location":"one-pager/#why-choose-tracepaper","text":"Efficiency: Save time with automated code generation and deployment, allowing your team to focus on innovation and quality. Reliability: Ensure consistent and error-free deployments with comprehensive testing and real-time monitoring of command processing. Scalability: Easily scale your applications with infrastructure as code, supporting growing business needs and complex environments. Collaboration: Facilitate collaboration between developers, domain experts, and operations teams with clear, visual models and shared repositories.","title":"Why Choose Tracepaper?"},{"location":"one-pager/#get-started","text":"Unlock the full potential of your development process with Tracepaper by Draftsman. Visit our website to learn more and start transforming your application development today. Visit Tracepaper by Draftsman .","title":"Get Started"},{"location":"overview/","text":"Concept overview # In the diagram we mapped the relations between our concepts. Gray elements represent technical concepts included to provide cohesion to the overall design, while all colored sections depict modeling concepts. The red and white areas indicate custom code. Commands # Commands represent actions triggered by users or external systems. They encapsulate requests to perform specific operations within the system. They are part of the API model in green. Aggregates and Behavior (pale red) # Event Sourcing : This approach ensures that domain events, which capture immutable truths about past actions, remain unchanged over time. It separates the factual recording (event sourcing) from how we interpret and reason about these events (projection). Technical Key : This key is immutable and crucial for maintaining consistency in the system. Views and Queries # Projection of Truth : While projecting the truth to execute domain logic is essential, querying involves a different perspective. It allows deviations in the data model and supports querying based on functional keys that can change over time (green). Materialized Views : These are separate read models optimized for queries. They are decoupled from the write model and offer benefits like converting technical keys to functional ones, optimizing data aggregations, and establishing relationships between entities that are otherwise decoupled (blue). Automations (also known as Notifiers) # Notifiers orchestrate commands in response to domain events without maintaining state. They have the capability to invoke a wide range of web APIs, including AWS, REST, GraphQL, and the platform's own API (orange). Projections # Projections enhance queries by combining data from APIs, materialized views, and custom Python logic. They facilitate transformations such as calculations, advanced filtering, or generating time-sensitive attributes (tokens). Custom code # The models can be augmented by Python code (bright red) to create functionality beyond the modelling concepts. Advanced business logic should be implemented through programming because modeling it can add more complexity than it solves, and coding is faster for this purpose. However, it must be understandable for domain experts. Python is an ideal choice because it is widely used by academics in fields like chemistry, biology, and data science. This increases the likelihood that a domain expert can learn it, especially when we add \"liberating constraints\" from a methodology like \" tracepaper.\"","title":"Concept overview"},{"location":"overview/#concept-overview","text":"In the diagram we mapped the relations between our concepts. Gray elements represent technical concepts included to provide cohesion to the overall design, while all colored sections depict modeling concepts. The red and white areas indicate custom code.","title":"Concept overview"},{"location":"overview/#commands","text":"Commands represent actions triggered by users or external systems. They encapsulate requests to perform specific operations within the system. They are part of the API model in green.","title":"Commands"},{"location":"overview/#aggregates-and-behavior-pale-red","text":"Event Sourcing : This approach ensures that domain events, which capture immutable truths about past actions, remain unchanged over time. It separates the factual recording (event sourcing) from how we interpret and reason about these events (projection). Technical Key : This key is immutable and crucial for maintaining consistency in the system.","title":"Aggregates and Behavior (pale red)"},{"location":"overview/#views-and-queries","text":"Projection of Truth : While projecting the truth to execute domain logic is essential, querying involves a different perspective. It allows deviations in the data model and supports querying based on functional keys that can change over time (green). Materialized Views : These are separate read models optimized for queries. They are decoupled from the write model and offer benefits like converting technical keys to functional ones, optimizing data aggregations, and establishing relationships between entities that are otherwise decoupled (blue).","title":"Views and Queries"},{"location":"overview/#automations-also-known-as-notifiers","text":"Notifiers orchestrate commands in response to domain events without maintaining state. They have the capability to invoke a wide range of web APIs, including AWS, REST, GraphQL, and the platform's own API (orange).","title":"Automations (also known as Notifiers)"},{"location":"overview/#projections","text":"Projections enhance queries by combining data from APIs, materialized views, and custom Python logic. They facilitate transformations such as calculations, advanced filtering, or generating time-sensitive attributes (tokens).","title":"Projections"},{"location":"overview/#custom-code","text":"The models can be augmented by Python code (bright red) to create functionality beyond the modelling concepts. Advanced business logic should be implemented through programming because modeling it can add more complexity than it solves, and coding is faster for this purpose. However, it must be understandable for domain experts. Python is an ideal choice because it is widely used by academics in fields like chemistry, biology, and data science. This increases the likelihood that a domain expert can learn it, especially when we add \"liberating constraints\" from a methodology like \" tracepaper.\"","title":"Custom code"},{"location":"01_Write_Domain/","text":"About the Write domain # Introduction # The Write Domain is a fundamental part of our model-driven development environment. It focuses on processing and recording data within a system. This domain encompasses all components responsible for invoking and executing behaviors that change the state of the application. The Write Domain is essential for maintaining the integrity and consistency of data. Components of the Write Domain # The Write Domain consists of several key components: Commands, Subdomains, Aggregates, Behavior Flows, Domain Events, and Automations. Each of these components plays a specific role in capturing and processing events and actions that affect the state of the application. Commands # Commands represent interactions from users or systems with the application. These events are triggered by actions of actors (such as users or external systems) and initiate changes in the application's state. Subdomains # Subdomains help to organize and structure the Write Domain by grouping related aggregates. This modular approach allows for better separation of concerns and clearer boundaries between different parts of the system. Each subdomain focuses on a specific aspect of the business logic, making it easier to manage and maintain. Aggregates # Aggregates are a fundamental concept within the Write Domain that represent a cluster of related objects treated as a single unit for data changes. Each aggregate has a document model that represents state with a defined boundary that ensures the consistency of its data changes. Aggregates encapsulate both data and behavior, providing a clear structure for managing complex business logic. Behavior Flows # Behavior Flows are the core of the Write Domain. They model a set of actions that take place after an event is received. These actions include data-validation rules or business rules, and they may alter the state of the application by publishing a domain event. Domain Events # Domain events represent significant occurrences within the system that reflect a change in state. They capture the essential information about what happened, providing a way to track and react to changes in the application. Event Handling : Domain events are used to update the state of aggregates. Event handlers process these events to apply the necessary changes and maintain consistency within the system. A domain event may also trigger other behavior flows or automations. Automations # Automations are responsible for performing specific actions when certain conditions or events occur. They are often used for system activities that need to take place in response to specific events within the Write Domain. Automations can fail silently if necessary. Purpose and Benefits of the Write Domain # The Write Domain plays a crucial role in ensuring the integrity and consistency of data within an application. By strictly separating data and behavior logic, the Write Domain offers the following benefits: Consistency : By encapsulating data and behavior within behavior flows and aggregates, it ensures that changes are applied consistently and atomically. Traceability : Commands provide a clear and auditable trail of all actions and events that lead to changes in the application's state. Maintainability : By separating different responsibilities (commands, behavior flows, aggregates, subdomains, domain events, automations), complexity is reduced, making the code more maintainable and extendable. Modularity : Subdomains and aggregates provide a structured way to organize the system, promoting modularity and reducing the risk of interdependencies that can lead to errors. Reactivity : Domain events enable the system to react to significant changes in state, allowing for more responsive and dynamic behavior. Conclusion # The Write Domain is an essential concept within our model-driven development environment. It provides a clear structure and maintains the integrity of the application through well-defined components such as Commands, Behavior Flows, Aggregates, Subdomains, Domain Events, and Automations. By effectively using these components, we can build robust, consistent, and well-maintained applications.","title":"About the Write domain"},{"location":"01_Write_Domain/#about-the-write-domain","text":"","title":"About the Write domain"},{"location":"01_Write_Domain/#introduction","text":"The Write Domain is a fundamental part of our model-driven development environment. It focuses on processing and recording data within a system. This domain encompasses all components responsible for invoking and executing behaviors that change the state of the application. The Write Domain is essential for maintaining the integrity and consistency of data.","title":"Introduction"},{"location":"01_Write_Domain/#components-of-the-write-domain","text":"The Write Domain consists of several key components: Commands, Subdomains, Aggregates, Behavior Flows, Domain Events, and Automations. Each of these components plays a specific role in capturing and processing events and actions that affect the state of the application.","title":"Components of the Write Domain"},{"location":"01_Write_Domain/#commands","text":"Commands represent interactions from users or systems with the application. These events are triggered by actions of actors (such as users or external systems) and initiate changes in the application's state.","title":"Commands"},{"location":"01_Write_Domain/#subdomains","text":"Subdomains help to organize and structure the Write Domain by grouping related aggregates. This modular approach allows for better separation of concerns and clearer boundaries between different parts of the system. Each subdomain focuses on a specific aspect of the business logic, making it easier to manage and maintain.","title":"Subdomains"},{"location":"01_Write_Domain/#aggregates","text":"Aggregates are a fundamental concept within the Write Domain that represent a cluster of related objects treated as a single unit for data changes. Each aggregate has a document model that represents state with a defined boundary that ensures the consistency of its data changes. Aggregates encapsulate both data and behavior, providing a clear structure for managing complex business logic.","title":"Aggregates"},{"location":"01_Write_Domain/#behavior-flows","text":"Behavior Flows are the core of the Write Domain. They model a set of actions that take place after an event is received. These actions include data-validation rules or business rules, and they may alter the state of the application by publishing a domain event.","title":"Behavior Flows"},{"location":"01_Write_Domain/#domain-events","text":"Domain events represent significant occurrences within the system that reflect a change in state. They capture the essential information about what happened, providing a way to track and react to changes in the application. Event Handling : Domain events are used to update the state of aggregates. Event handlers process these events to apply the necessary changes and maintain consistency within the system. A domain event may also trigger other behavior flows or automations.","title":"Domain Events"},{"location":"01_Write_Domain/#automations","text":"Automations are responsible for performing specific actions when certain conditions or events occur. They are often used for system activities that need to take place in response to specific events within the Write Domain. Automations can fail silently if necessary.","title":"Automations"},{"location":"01_Write_Domain/#purpose-and-benefits-of-the-write-domain","text":"The Write Domain plays a crucial role in ensuring the integrity and consistency of data within an application. By strictly separating data and behavior logic, the Write Domain offers the following benefits: Consistency : By encapsulating data and behavior within behavior flows and aggregates, it ensures that changes are applied consistently and atomically. Traceability : Commands provide a clear and auditable trail of all actions and events that lead to changes in the application's state. Maintainability : By separating different responsibilities (commands, behavior flows, aggregates, subdomains, domain events, automations), complexity is reduced, making the code more maintainable and extendable. Modularity : Subdomains and aggregates provide a structured way to organize the system, promoting modularity and reducing the risk of interdependencies that can lead to errors. Reactivity : Domain events enable the system to react to significant changes in state, allowing for more responsive and dynamic behavior.","title":"Purpose and Benefits of the Write Domain"},{"location":"01_Write_Domain/#conclusion","text":"The Write Domain is an essential concept within our model-driven development environment. It provides a clear structure and maintains the integrity of the application through well-defined components such as Commands, Behavior Flows, Aggregates, Subdomains, Domain Events, and Automations. By effectively using these components, we can build robust, consistent, and well-maintained applications.","title":"Conclusion"},{"location":"01_Write_Domain/01_commands/","text":"Commands # Overview # Commands are actions triggered by users or external systems to request the system to perform a specific operation. Commands encapsulate all the necessary information required to carry out the action, including authorization details and data fields. They are the primary means through which external entities interact with the system. Command Definition # A command is defined by specifying its name, authorization requirements, and associated data fields. Commands are triggered by actors (users or external systems) and may lead to state changes within the system. Attributes of a Command # Name : A unique identifier for the command. Authorization : Defines who can trigger the command. This includes: Authorization Type : Indicates if the command requires a specific role, can be triggered by any user ( anonymous ), or requires the user to be authenticated ( authenticated ). Role : In case role-based access is selected, specifies the user role required (e.g., administrator ). GraphQL Configuration # Commands are exposed via a GraphQL API, allowing clients to interact with the system using standard GraphQL operations. The configuration for GraphQL exposure includes: GraphQL Namespace : Organizes related commands within a logical grouping. GraphQL Name : Defines the specific mutation name for the command. This configuration enables seamless integration and interaction with the system, providing a clear and structured API for external clients. Fields # Fields represent the data required to execute the command. Each field is defined with the following attributes: Name : The name of the field. Type : The data type of the field. Possible types include: String : Textual data. Int : Integer values. Boolean : True/false values. Float : Floating-point numbers. Pattern : (Optional) A regular expression pattern that the field value must match (e.g., DateTime input in a String field). Default : (Optional) Default value for the field if none is provided. When a default is provided, the field will become optional in the GraphQL schema. Auto-fill : (Optional) Instructions for automatically generating the field value (e.g., uuid for unique identifiers or username ). Using auto-fill will remove this field from the GraphQL schema. Nested Collections # Commands can have complex data structures, which can be represented using collections of nested objects. Nested objects allow the inclusion of sub-structures within a command, providing a way to encapsulate related data. A nested collection has a name and a set of fields. Summary # Commands in an event-driven architecture encapsulate actions triggered by actors, specifying the necessary data and authorization requirements. By defining commands with fields, nested collections, and GraphQL configurations, systems can ensure a robust, secure, and structured interaction model for handling user and system interactions.","title":"Commands"},{"location":"01_Write_Domain/01_commands/#commands","text":"","title":"Commands"},{"location":"01_Write_Domain/01_commands/#overview","text":"Commands are actions triggered by users or external systems to request the system to perform a specific operation. Commands encapsulate all the necessary information required to carry out the action, including authorization details and data fields. They are the primary means through which external entities interact with the system.","title":"Overview"},{"location":"01_Write_Domain/01_commands/#command-definition","text":"A command is defined by specifying its name, authorization requirements, and associated data fields. Commands are triggered by actors (users or external systems) and may lead to state changes within the system.","title":"Command Definition"},{"location":"01_Write_Domain/01_commands/#attributes-of-a-command","text":"Name : A unique identifier for the command. Authorization : Defines who can trigger the command. This includes: Authorization Type : Indicates if the command requires a specific role, can be triggered by any user ( anonymous ), or requires the user to be authenticated ( authenticated ). Role : In case role-based access is selected, specifies the user role required (e.g., administrator ).","title":"Attributes of a Command"},{"location":"01_Write_Domain/01_commands/#graphql-configuration","text":"Commands are exposed via a GraphQL API, allowing clients to interact with the system using standard GraphQL operations. The configuration for GraphQL exposure includes: GraphQL Namespace : Organizes related commands within a logical grouping. GraphQL Name : Defines the specific mutation name for the command. This configuration enables seamless integration and interaction with the system, providing a clear and structured API for external clients.","title":"GraphQL Configuration"},{"location":"01_Write_Domain/01_commands/#fields","text":"Fields represent the data required to execute the command. Each field is defined with the following attributes: Name : The name of the field. Type : The data type of the field. Possible types include: String : Textual data. Int : Integer values. Boolean : True/false values. Float : Floating-point numbers. Pattern : (Optional) A regular expression pattern that the field value must match (e.g., DateTime input in a String field). Default : (Optional) Default value for the field if none is provided. When a default is provided, the field will become optional in the GraphQL schema. Auto-fill : (Optional) Instructions for automatically generating the field value (e.g., uuid for unique identifiers or username ). Using auto-fill will remove this field from the GraphQL schema.","title":"Fields"},{"location":"01_Write_Domain/01_commands/#nested-collections","text":"Commands can have complex data structures, which can be represented using collections of nested objects. Nested objects allow the inclusion of sub-structures within a command, providing a way to encapsulate related data. A nested collection has a name and a set of fields.","title":"Nested Collections"},{"location":"01_Write_Domain/01_commands/#summary","text":"Commands in an event-driven architecture encapsulate actions triggered by actors, specifying the necessary data and authorization requirements. By defining commands with fields, nested collections, and GraphQL configurations, systems can ensure a robust, secure, and structured interaction model for handling user and system interactions.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/","text":"Aggregates # An aggregate reflects a domain concept, a collection of data (state) and behavior to manipulate the data. The data is internally encapsulated, so the only way to induce a state change is by invoking behavior. Modeling an aggregate involves two activities: Modeling the data model. Modeling behavior on the model. A concept from DDD opinionated to optimize cognitive load, focusing on the concepts rather than the underlying technology. It abstracts the Command/Compute part of our CQRS architecture. Aggregate Data Model # The data model consists of two parts, the actual part: the events , and the projection: the document . The Document # The document is a projection of the data, a mental model of the state so that we can implement validations in the behavior to ensure data integrity. Because it is a projection, this model can evolve over time without manipulating the underlying data. Events from the past are immutable by definition. Thus, the document serves both as a mental model for ourselves on how we want to think about state and as a way to model how we present data to the viewstore. It is the data contract between the aggregate and the viewstore. One of the document fields (String) will serve as the business key of the aggregate instance The Events # The domain events are the recording of facts. They reflect a bundle of data that encompasses the delta between two states. Therefore, the state is not stored in the database, merely a log of deltas that exist alongside each other on a timeline. This is called the event log. Event Handling # To populate our mental model, the projection, we need event handlers. These model the mapping between the event log and the document. Side Notes on Views # Views will be discussed later, but regarding the contract, the viewstore essentially models the external contract, the GraphQL API. So while the document is a data contract, this contract remains within the domain. The viewstore can do various things with the data: Store As Is # Essentially caching a snapshot of the aggregate state. In this case, the internal model becomes publicly accessible, being queryable and read-optimized. Enrich and Store # Enrichment can take various forms, such as combining data from different aggregates into one document or modifying data for storage, i.e., determining derived data and caching it in the viewstore. Enrich During Data Reading # Modifying the API response before it is sent to the client. This involves executing logic on the combination of request data and cached data. Essentially, this is an on-the-fly projection where the view model is virtual. The logic has access to the request data and a fluent API to the viewstore, allowing the creation of a response object using Python scripting from this combination. Behavior Flows # Overview # Behavior flows define the lifecycle and interactions of aggregates within a system. They encapsulate the business logic required to handle events and commands, ensuring consistent state transitions and proper handling of complex workflows. By defining how an aggregate responds to various inputs, behavior flows help maintain system integrity and align with business rules. General # Behavior is a response to an event emited within the domain this may be an ActorEvent (Command) or a DomainEvent (From an other entity), which can lead to changes in the state of an aggregate. Name : A unique identifier for the command. Create Command : (Optional) Indicates if this should be the initial behavior of a new instance of the aggregate. If the instance with the specific business key is already present in the database the execution will fail. Trigger # A trigger specifies the conditions under which a command is activated. It listens for specific events and uses the data from these events to execute the corresponding behavior flow. Source : The event that activates the command. Key Field : The primary field used to correlate the incoming event with the correct aggregate instance. Mapping : Transfers data from the event to the flow variable fields. Optionally event-fields can be marked as part of the idempotency key giving you the ability to model functional idempotency. Processors # Processors are essential components within behavior flows that perform specific operations. They manage validations, data transformations, event emissions, and other logical actions to ensure that the behavior executes correctly and that aggregates maintain consistent state transitions. Each processor type serves a unique function and has specific attributes to support its operations. Processor Types # Emit Event # The emit-event processor is responsible for generating and emitting domain events, which signal that a significant change or action has occurred within the system. This processor type includes a reference specifying the event to be emitted, and a mapping mechanism that transfers flow-variables to the event fields. Code # The code processor allows for custom logic to be executed within the behavior flow. This processor is used when predefined processor types do not cover the required operation. It includes the code (inline) or script (global) to be executed, and optionally, inputs or parameters required by the script. Validator # The validator processor checks specific conditions and ensures they are met before proceeding. If the condition fails, an exception or error is triggered, preventing the behavior from executing further. This processor type includes the condition to be checked, typically expressed as a logical expression, and the message or exception to be raised if the condition is not met. If a functional exception is raised, no events will be persisted to the event store. So a validator may also validate on the instance state after emit-event processors are already applied, it will still prevent the state transition. Set Variable # The set-variable processor assigns values to variables (in memory) within the behavior flow. This can be used to store intermediate results, configuration values, or other data needed for subsequent processing steps. This processor type includes the name of the variable to be set, and the expression or value used to determine the variable's value. Update Key # The update-key processor updates the business key of an aggregate instance. This is useful for operations that require changing the identifier or key used to access an aggregate instance. This processor type includes the field in the aggregate to be updated, and the new value to be assigned to the key field. Test Case # Test cases validate the behavior of a command by defining inputs, expected domain events, and resulting state changes. They ensure that the command performs as intended and that the aggregate's state transitions correctly. Name : A unique identifier for the test case. Trigger Event : The event that initiates the test case. Input : Defines the input data for the command. Name : The name of the input field. Value : The value assigned to the input field. Type : The data type of the input field. Expected Domain Event : Specifies the expected event to be emitted by the command. Field : Defines the expected values for fields in the emitted event. State : (Optional) The initial state of the aggregate before executing the command. Expected State : The expected state of the aggregate after executing the command. Primary Key (pk) : The key identifying the aggregate instance. State Data : The expected state data of the aggregate. Summary # Behavior flows are essential for managing the lifecycle and interactions of aggregates. By defining commands, triggers, mappings, processors, and test cases, behavior flows ensure that business logic is consistently applied, state transitions are correctly managed, and the system's behavior aligns with the intended domain model. Understanding and modeling behavior flows are crucial for implementing robust, maintainable, and scalable systems. Advanced Features of an Aggregate # Event Store Time-to-Live (TTL) # The Event Store Time-to-Live (TTL) is an advanced configuration setting for aggregates that specifies the duration, in seconds, for which events associated with the aggregate should be retained in the event store. This feature is particularly useful in scenarios where certain events become irrelevant or obsolete after a specified period. The default TTL is -1 seconds which translates to indefinitely. The TTL setting allows system architects and developers to manage the lifecycle of events within the event store effectively. By setting an expiration time for events related to an aggregate, the system can automatically purge older events beyond the specified TTL. This helps in maintaining a lean and efficient event store by removing outdated data that no longer contributes to the current state or history of the aggregate. Benefits # Optimized Event Storage : Helps in managing storage space by automatically removing events that are no longer relevant. Compliance and Governance : Supports compliance requirements by ensuring that sensitive or outdated data is removed from the system in a timely manner. Performance : Improves performance by reducing the volume of data that needs to be processed and queried over time. Considerations # Event Retention Policies : Define appropriate TTL values based on business needs and regulatory requirements. Data Archival : Consider integrating TTL with data archival strategies to maintain historical data beyond the event store. Summary # The Event Store Time-to-Live (TTL) configuration for aggregates enhances the management and efficiency of event-driven architectures. By setting TTL values, systems can automatically manage event data retention, ensuring that only relevant and current information is maintained in the event store while optimizing storage resources and improving overall system performance. Snapshot Interval # The Snapshot Interval is an advanced configuration setting for aggregates that determines how frequently snapshots of the aggregate's state should be taken. Snapshots capture the current state of an aggregate at a specific point in time, providing a checkpoint that can optimize event sourcing and reconstruction processes. In event sourcing architectures, aggregates can accumulate a large number of events over time, which can impact performance during state reconstruction. Snapshots serve as efficient checkpoints by storing the aggregate's current state periodically. Instead of replaying all events from the beginning to reconstruct the aggregate's state, the system can load the latest snapshot and replay only the events that occurred after the snapshot was taken. Usage # When configuring an aggregate with Snapshot Interval: Snapshot Interval : Specifies the number of events after which a new snapshot of the aggregate's state should be taken. For example, a snapshot interval of 100 means that every 100 events, a snapshot of the aggregate's state will be captured. Benefits # Performance Optimization : Reduces the time and computational resources required for state reconstruction by loading snapshots and applying only subsequent events. Efficient Event Handling : Improves overall system performance by minimizing the number of events that need to be processed during state recovery. Scalability : Facilitates scalability by reducing the processing overhead associated with large aggregates and long event histories. Considerations # Snapshot Size : Evaluate the size and complexity of aggregate states to determine an appropriate snapshot interval. Event Volume : Adjust the snapshot interval based on the frequency and volume of events generated by aggregates. Default Value # The default Snapshot Interval is typically set to 100 events, providing a balance between capturing frequent snapshots and minimizing overhead. This default value can be adjusted based on specific application requirements and performance considerations. Summary # The Snapshot Interval configuration enhances the efficiency and performance of event-sourced aggregates by periodically capturing snapshots of their states. By reducing the computational effort required for state reconstruction, snapshots optimize event handling and support scalability in event-driven architectures. Adjusting the snapshot interval allows systems to achieve optimal performance while effectively managing aggregate states and event histories. Backup to Blob Storage (S3) # The Backup to Blob Storage feature allows event-sourced aggregates to automatically back up their state to cloud storage, specifically Amazon S3. This capability ensures that critical data remains secure and accessible, providing resilience against data loss and supporting disaster recovery strategies. In event sourcing architectures, ensuring data durability and recoverability is paramount. Backup to Blob Storage leverages cloud infrastructure, such as Amazon S3, to store snapshots of aggregate states at regular intervals. Additionally, it manages the retention of these backups based on specified policies, providing flexibility in data management and compliance with retention requirements. The event store (DynamoDB) has point-in-time recovery enabled, therefore, this feature is disabled by default to reduce the area where data needs to be protected. If this feature should be enabled for a specific aggregate is a business consideration. Configuration # Interval : Specifies the frequency, specified in days, at which snapshots of aggregate states should be backed up to S3. . Retention Period : Defines the duration for which backup snapshots should be retained in S3 storage, measured in days. After the retention period expires, older snapshots are automatically deleted. Usage # The Backup to Blob Storage feature offers several advantages: Data Resilience : Enhances data durability by securely storing aggregate state snapshots in Amazon S3, which is designed for high availability and redundancy. Disaster Recovery : Facilitates rapid recovery of aggregate states in the event of data corruption, system failures, or disasters by maintaining up-to-date backups. Although the required script to recover from cold storage are not (yet) provided by Draftsman. Compliance : Supports compliance with regulatory requirements and data retention policies by managing backup retention periods effectively. Implementation Considerations # Security : Ensure proper access controls and encryption mechanisms are in place to protect sensitive data stored in S3. Cost Management : Monitor storage costs associated with S3 backups and optimize usage based on storage needs and budget constraints. Integration : Integrate with existing backup and recovery processes to streamline data management and operational workflows. Summary # Backup to Blob Storage (S3) is a critical feature in event-driven architectures that ensures the resilience and recoverability of event-sourced aggregates. By automatically backing up aggregate states to Amazon S3 at defined intervals and managing retention periods, this feature enhances data durability, supports disaster recovery strategies, and enables compliance with data retention policies. Configuring backup intervals and retention periods optimally balances data protection with operational efficiency, thereby safeguarding business-critical information in event sourcing environments.","title":"Aggregates"},{"location":"01_Write_Domain/02_aggregate/#aggregates","text":"An aggregate reflects a domain concept, a collection of data (state) and behavior to manipulate the data. The data is internally encapsulated, so the only way to induce a state change is by invoking behavior. Modeling an aggregate involves two activities: Modeling the data model. Modeling behavior on the model. A concept from DDD opinionated to optimize cognitive load, focusing on the concepts rather than the underlying technology. It abstracts the Command/Compute part of our CQRS architecture.","title":"Aggregates"},{"location":"01_Write_Domain/02_aggregate/#aggregate-data-model","text":"The data model consists of two parts, the actual part: the events , and the projection: the document .","title":"Aggregate Data Model"},{"location":"01_Write_Domain/02_aggregate/#the-document","text":"The document is a projection of the data, a mental model of the state so that we can implement validations in the behavior to ensure data integrity. Because it is a projection, this model can evolve over time without manipulating the underlying data. Events from the past are immutable by definition. Thus, the document serves both as a mental model for ourselves on how we want to think about state and as a way to model how we present data to the viewstore. It is the data contract between the aggregate and the viewstore. One of the document fields (String) will serve as the business key of the aggregate instance","title":"The Document"},{"location":"01_Write_Domain/02_aggregate/#the-events","text":"The domain events are the recording of facts. They reflect a bundle of data that encompasses the delta between two states. Therefore, the state is not stored in the database, merely a log of deltas that exist alongside each other on a timeline. This is called the event log.","title":"The Events"},{"location":"01_Write_Domain/02_aggregate/#event-handling","text":"To populate our mental model, the projection, we need event handlers. These model the mapping between the event log and the document.","title":"Event Handling"},{"location":"01_Write_Domain/02_aggregate/#side-notes-on-views","text":"Views will be discussed later, but regarding the contract, the viewstore essentially models the external contract, the GraphQL API. So while the document is a data contract, this contract remains within the domain. The viewstore can do various things with the data:","title":"Side Notes on Views"},{"location":"01_Write_Domain/02_aggregate/#store-as-is","text":"Essentially caching a snapshot of the aggregate state. In this case, the internal model becomes publicly accessible, being queryable and read-optimized.","title":"Store As Is"},{"location":"01_Write_Domain/02_aggregate/#enrich-and-store","text":"Enrichment can take various forms, such as combining data from different aggregates into one document or modifying data for storage, i.e., determining derived data and caching it in the viewstore.","title":"Enrich and Store"},{"location":"01_Write_Domain/02_aggregate/#enrich-during-data-reading","text":"Modifying the API response before it is sent to the client. This involves executing logic on the combination of request data and cached data. Essentially, this is an on-the-fly projection where the view model is virtual. The logic has access to the request data and a fluent API to the viewstore, allowing the creation of a response object using Python scripting from this combination.","title":"Enrich During Data Reading"},{"location":"01_Write_Domain/02_aggregate/#behavior-flows","text":"","title":"Behavior Flows"},{"location":"01_Write_Domain/02_aggregate/#overview","text":"Behavior flows define the lifecycle and interactions of aggregates within a system. They encapsulate the business logic required to handle events and commands, ensuring consistent state transitions and proper handling of complex workflows. By defining how an aggregate responds to various inputs, behavior flows help maintain system integrity and align with business rules.","title":"Overview"},{"location":"01_Write_Domain/02_aggregate/#general","text":"Behavior is a response to an event emited within the domain this may be an ActorEvent (Command) or a DomainEvent (From an other entity), which can lead to changes in the state of an aggregate. Name : A unique identifier for the command. Create Command : (Optional) Indicates if this should be the initial behavior of a new instance of the aggregate. If the instance with the specific business key is already present in the database the execution will fail.","title":"General"},{"location":"01_Write_Domain/02_aggregate/#trigger","text":"A trigger specifies the conditions under which a command is activated. It listens for specific events and uses the data from these events to execute the corresponding behavior flow. Source : The event that activates the command. Key Field : The primary field used to correlate the incoming event with the correct aggregate instance. Mapping : Transfers data from the event to the flow variable fields. Optionally event-fields can be marked as part of the idempotency key giving you the ability to model functional idempotency.","title":"Trigger"},{"location":"01_Write_Domain/02_aggregate/#processors","text":"Processors are essential components within behavior flows that perform specific operations. They manage validations, data transformations, event emissions, and other logical actions to ensure that the behavior executes correctly and that aggregates maintain consistent state transitions. Each processor type serves a unique function and has specific attributes to support its operations.","title":"Processors"},{"location":"01_Write_Domain/02_aggregate/#processor-types","text":"","title":"Processor Types"},{"location":"01_Write_Domain/02_aggregate/#emit-event","text":"The emit-event processor is responsible for generating and emitting domain events, which signal that a significant change or action has occurred within the system. This processor type includes a reference specifying the event to be emitted, and a mapping mechanism that transfers flow-variables to the event fields.","title":"Emit Event"},{"location":"01_Write_Domain/02_aggregate/#code","text":"The code processor allows for custom logic to be executed within the behavior flow. This processor is used when predefined processor types do not cover the required operation. It includes the code (inline) or script (global) to be executed, and optionally, inputs or parameters required by the script.","title":"Code"},{"location":"01_Write_Domain/02_aggregate/#validator","text":"The validator processor checks specific conditions and ensures they are met before proceeding. If the condition fails, an exception or error is triggered, preventing the behavior from executing further. This processor type includes the condition to be checked, typically expressed as a logical expression, and the message or exception to be raised if the condition is not met. If a functional exception is raised, no events will be persisted to the event store. So a validator may also validate on the instance state after emit-event processors are already applied, it will still prevent the state transition.","title":"Validator"},{"location":"01_Write_Domain/02_aggregate/#set-variable","text":"The set-variable processor assigns values to variables (in memory) within the behavior flow. This can be used to store intermediate results, configuration values, or other data needed for subsequent processing steps. This processor type includes the name of the variable to be set, and the expression or value used to determine the variable's value.","title":"Set Variable"},{"location":"01_Write_Domain/02_aggregate/#update-key","text":"The update-key processor updates the business key of an aggregate instance. This is useful for operations that require changing the identifier or key used to access an aggregate instance. This processor type includes the field in the aggregate to be updated, and the new value to be assigned to the key field.","title":"Update Key"},{"location":"01_Write_Domain/02_aggregate/#test-case","text":"Test cases validate the behavior of a command by defining inputs, expected domain events, and resulting state changes. They ensure that the command performs as intended and that the aggregate's state transitions correctly. Name : A unique identifier for the test case. Trigger Event : The event that initiates the test case. Input : Defines the input data for the command. Name : The name of the input field. Value : The value assigned to the input field. Type : The data type of the input field. Expected Domain Event : Specifies the expected event to be emitted by the command. Field : Defines the expected values for fields in the emitted event. State : (Optional) The initial state of the aggregate before executing the command. Expected State : The expected state of the aggregate after executing the command. Primary Key (pk) : The key identifying the aggregate instance. State Data : The expected state data of the aggregate.","title":"Test Case"},{"location":"01_Write_Domain/02_aggregate/#summary","text":"Behavior flows are essential for managing the lifecycle and interactions of aggregates. By defining commands, triggers, mappings, processors, and test cases, behavior flows ensure that business logic is consistently applied, state transitions are correctly managed, and the system's behavior aligns with the intended domain model. Understanding and modeling behavior flows are crucial for implementing robust, maintainable, and scalable systems.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#advanced-features-of-an-aggregate","text":"","title":"Advanced Features of an Aggregate"},{"location":"01_Write_Domain/02_aggregate/#event-store-time-to-live-ttl","text":"The Event Store Time-to-Live (TTL) is an advanced configuration setting for aggregates that specifies the duration, in seconds, for which events associated with the aggregate should be retained in the event store. This feature is particularly useful in scenarios where certain events become irrelevant or obsolete after a specified period. The default TTL is -1 seconds which translates to indefinitely. The TTL setting allows system architects and developers to manage the lifecycle of events within the event store effectively. By setting an expiration time for events related to an aggregate, the system can automatically purge older events beyond the specified TTL. This helps in maintaining a lean and efficient event store by removing outdated data that no longer contributes to the current state or history of the aggregate.","title":"Event Store Time-to-Live (TTL)"},{"location":"01_Write_Domain/02_aggregate/#benefits","text":"Optimized Event Storage : Helps in managing storage space by automatically removing events that are no longer relevant. Compliance and Governance : Supports compliance requirements by ensuring that sensitive or outdated data is removed from the system in a timely manner. Performance : Improves performance by reducing the volume of data that needs to be processed and queried over time.","title":"Benefits"},{"location":"01_Write_Domain/02_aggregate/#considerations","text":"Event Retention Policies : Define appropriate TTL values based on business needs and regulatory requirements. Data Archival : Consider integrating TTL with data archival strategies to maintain historical data beyond the event store.","title":"Considerations"},{"location":"01_Write_Domain/02_aggregate/#summary_1","text":"The Event Store Time-to-Live (TTL) configuration for aggregates enhances the management and efficiency of event-driven architectures. By setting TTL values, systems can automatically manage event data retention, ensuring that only relevant and current information is maintained in the event store while optimizing storage resources and improving overall system performance.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#snapshot-interval","text":"The Snapshot Interval is an advanced configuration setting for aggregates that determines how frequently snapshots of the aggregate's state should be taken. Snapshots capture the current state of an aggregate at a specific point in time, providing a checkpoint that can optimize event sourcing and reconstruction processes. In event sourcing architectures, aggregates can accumulate a large number of events over time, which can impact performance during state reconstruction. Snapshots serve as efficient checkpoints by storing the aggregate's current state periodically. Instead of replaying all events from the beginning to reconstruct the aggregate's state, the system can load the latest snapshot and replay only the events that occurred after the snapshot was taken.","title":"Snapshot Interval"},{"location":"01_Write_Domain/02_aggregate/#usage","text":"When configuring an aggregate with Snapshot Interval: Snapshot Interval : Specifies the number of events after which a new snapshot of the aggregate's state should be taken. For example, a snapshot interval of 100 means that every 100 events, a snapshot of the aggregate's state will be captured.","title":"Usage"},{"location":"01_Write_Domain/02_aggregate/#benefits_1","text":"Performance Optimization : Reduces the time and computational resources required for state reconstruction by loading snapshots and applying only subsequent events. Efficient Event Handling : Improves overall system performance by minimizing the number of events that need to be processed during state recovery. Scalability : Facilitates scalability by reducing the processing overhead associated with large aggregates and long event histories.","title":"Benefits"},{"location":"01_Write_Domain/02_aggregate/#considerations_1","text":"Snapshot Size : Evaluate the size and complexity of aggregate states to determine an appropriate snapshot interval. Event Volume : Adjust the snapshot interval based on the frequency and volume of events generated by aggregates.","title":"Considerations"},{"location":"01_Write_Domain/02_aggregate/#default-value","text":"The default Snapshot Interval is typically set to 100 events, providing a balance between capturing frequent snapshots and minimizing overhead. This default value can be adjusted based on specific application requirements and performance considerations.","title":"Default Value"},{"location":"01_Write_Domain/02_aggregate/#summary_2","text":"The Snapshot Interval configuration enhances the efficiency and performance of event-sourced aggregates by periodically capturing snapshots of their states. By reducing the computational effort required for state reconstruction, snapshots optimize event handling and support scalability in event-driven architectures. Adjusting the snapshot interval allows systems to achieve optimal performance while effectively managing aggregate states and event histories.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#backup-to-blob-storage-s3","text":"The Backup to Blob Storage feature allows event-sourced aggregates to automatically back up their state to cloud storage, specifically Amazon S3. This capability ensures that critical data remains secure and accessible, providing resilience against data loss and supporting disaster recovery strategies. In event sourcing architectures, ensuring data durability and recoverability is paramount. Backup to Blob Storage leverages cloud infrastructure, such as Amazon S3, to store snapshots of aggregate states at regular intervals. Additionally, it manages the retention of these backups based on specified policies, providing flexibility in data management and compliance with retention requirements. The event store (DynamoDB) has point-in-time recovery enabled, therefore, this feature is disabled by default to reduce the area where data needs to be protected. If this feature should be enabled for a specific aggregate is a business consideration.","title":"Backup to Blob Storage (S3)"},{"location":"01_Write_Domain/02_aggregate/#configuration","text":"Interval : Specifies the frequency, specified in days, at which snapshots of aggregate states should be backed up to S3. . Retention Period : Defines the duration for which backup snapshots should be retained in S3 storage, measured in days. After the retention period expires, older snapshots are automatically deleted.","title":"Configuration"},{"location":"01_Write_Domain/02_aggregate/#usage_1","text":"The Backup to Blob Storage feature offers several advantages: Data Resilience : Enhances data durability by securely storing aggregate state snapshots in Amazon S3, which is designed for high availability and redundancy. Disaster Recovery : Facilitates rapid recovery of aggregate states in the event of data corruption, system failures, or disasters by maintaining up-to-date backups. Although the required script to recover from cold storage are not (yet) provided by Draftsman. Compliance : Supports compliance with regulatory requirements and data retention policies by managing backup retention periods effectively.","title":"Usage"},{"location":"01_Write_Domain/02_aggregate/#implementation-considerations","text":"Security : Ensure proper access controls and encryption mechanisms are in place to protect sensitive data stored in S3. Cost Management : Monitor storage costs associated with S3 backups and optimize usage based on storage needs and budget constraints. Integration : Integrate with existing backup and recovery processes to streamline data management and operational workflows.","title":"Implementation Considerations"},{"location":"01_Write_Domain/02_aggregate/#summary_3","text":"Backup to Blob Storage (S3) is a critical feature in event-driven architectures that ensures the resilience and recoverability of event-sourced aggregates. By automatically backing up aggregate states to Amazon S3 at defined intervals and managing retention periods, this feature enhances data durability, supports disaster recovery strategies, and enables compliance with data retention policies. Configuring backup intervals and retention periods optimally balances data protection with operational efficiency, thereby safeguarding business-critical information in event sourcing environments.","title":"Summary"},{"location":"01_Write_Domain/03_automations/","text":"Automations # Overview # Automations , also referred to as notifiers , are predefined processes that are triggered in response to specific events or conditions within a system. They are designed to perform actions automatically without direct user intervention, often to notify users or other systems about important events or changes. Automations enhance system responsiveness and improve user experience by providing timely and relevant information. Automation Definition # An automation is defined by specifying its name, trigger conditions, and actions to be performed. Automations react to events occurring within the system, executing predefined actions when certain conditions are met. Attributes of an Automation # Name # A unique identifier for the automation. Trigger Conditions # Conditions or events that initiate the automation. These triggers can be system events, changes in data, or specific time-based conditions. Examples of trigger conditions include: Event-based : Automations can be triggered by specific events such as Commands (ActorEvents) originating from the GraphQL API. Or domain events published by aggregates. Time-based : Automations can be set to trigger at specific times or after certain intervals. After-deployment : Automations can trigger after the application is deployed. Actions # Automation Activity Types # Automations in a system can perform a variety of tasks, known as activities. Each activity type represents a specific action that the automation can execute when triggered. Below is a documentation of the possible activity types that can be used within automations (notifiers). Overview # Automations consist of a series of activities that define the specific actions to be performed. These activities are triggered by events or conditions within the system and can interact with various system components or external services. Activity Types # Identity and Access Management (IAM - AWS Cognito) Activities # create-iam-group Description : Creates a new IAM group within the system. Use Case : Used when a new group of users with specific permissions needs to be created. delete-iam-group Description : Deletes an existing IAM group. Use Case : Used when an IAM group is no longer needed and should be removed. add-user-to-iam-group Description : Adds a user to an IAM group. Use Case : Used to grant a user the permissions associated with the IAM group. remove-user-from-iam-group Description : Removes a user from an IAM group. Use Case : Used to revoke a user's permissions associated with the IAM group. retrieve-email-from-iam Description : Retrieves a user's email address from the IAM system. Use Case : Used to get the email address of a user for communication purposes. iam-create-systemuser Description : Creates a system user in the IAM system. Use Case : Used to create a non-human user that interacts with the system programmatically. iam-create-user Description : Creates a new user in the IAM system. Use Case : Used for onboarding new users. iam-delete-user Description : Deletes a user from the IAM system. Use Case : Used when a user account needs to be permanently removed. Communication and Notification Activities # render-template Description : Renders a template with specified data. Use Case : Used to create personalized messages or documents. send-email Description : Sends an email to specified recipients. (AWS SES) Use Case : Used for sending notifications, alerts, or other communications. send-graphql-notification Description : Sends a notification via the GraphQL endpoint. Use Case : Used for notifying clients or systems through the GraphQL API. File and Data Operations # write-file Description : Writes data to a file. Use Case : Used to save data in a remote file system (AWS S3). fetch-property Description : Fetches a specific property or data point from the system. Use Case : Used to retrieve configuration settings or other data. Token Management Activities # get-token Description : Retrieves a token for authentication or authorization purposes. Use Case : Used to get tokens required for accessing secured resources. get-systemuser-token Description : Retrieves a token for a system user. Use Case : Used for system-to-system authentication. Variable and State Management # set-variable Description : Sets a flow variable to a specified value. Use Case : Used to store temporary data or state within an automation. API and External Service Interactions # call-internal-api Description : Calls an internal GraphQL API endpoint. Use Case : Used to invoke internal system functions (Commands/Queries/Projections). HTTP Description : Makes an HTTP request to an external service. Use Case : Used to interact with external APIs or services. Miscellaneous Activities # code Description : Executes custom code. Use Case : Used for complex logic that cannot be handled by predefined activities. invalidate-cdn Description : Invalidates a CDN cache. Use Case : Used to ensure that updated content is served from the AWS CloudFront CDN. loop Description : Repeats a set of activities a specified number of times. Use Case : Used for iterating over a collection of items or repeating an action multiple times. Summary # Automations leverage a diverse set of activity types to perform various tasks automatically. By combining these activities, systems can create sophisticated workflows that enhance functionality, improve efficiency, and provide timely responses to events and conditions. Understanding these activity types is essential for designing effective automations that meet specific business needs.","title":"Automations"},{"location":"01_Write_Domain/03_automations/#automations","text":"","title":"Automations"},{"location":"01_Write_Domain/03_automations/#overview","text":"Automations , also referred to as notifiers , are predefined processes that are triggered in response to specific events or conditions within a system. They are designed to perform actions automatically without direct user intervention, often to notify users or other systems about important events or changes. Automations enhance system responsiveness and improve user experience by providing timely and relevant information.","title":"Overview"},{"location":"01_Write_Domain/03_automations/#automation-definition","text":"An automation is defined by specifying its name, trigger conditions, and actions to be performed. Automations react to events occurring within the system, executing predefined actions when certain conditions are met.","title":"Automation Definition"},{"location":"01_Write_Domain/03_automations/#attributes-of-an-automation","text":"","title":"Attributes of an Automation"},{"location":"01_Write_Domain/03_automations/#name","text":"A unique identifier for the automation.","title":"Name"},{"location":"01_Write_Domain/03_automations/#trigger-conditions","text":"Conditions or events that initiate the automation. These triggers can be system events, changes in data, or specific time-based conditions. Examples of trigger conditions include: Event-based : Automations can be triggered by specific events such as Commands (ActorEvents) originating from the GraphQL API. Or domain events published by aggregates. Time-based : Automations can be set to trigger at specific times or after certain intervals. After-deployment : Automations can trigger after the application is deployed.","title":"Trigger Conditions"},{"location":"01_Write_Domain/03_automations/#actions","text":"","title":"Actions"},{"location":"01_Write_Domain/03_automations/#automation-activity-types","text":"Automations in a system can perform a variety of tasks, known as activities. Each activity type represents a specific action that the automation can execute when triggered. Below is a documentation of the possible activity types that can be used within automations (notifiers).","title":"Automation Activity Types"},{"location":"01_Write_Domain/03_automations/#overview_1","text":"Automations consist of a series of activities that define the specific actions to be performed. These activities are triggered by events or conditions within the system and can interact with various system components or external services.","title":"Overview"},{"location":"01_Write_Domain/03_automations/#activity-types","text":"","title":"Activity Types"},{"location":"01_Write_Domain/03_automations/#identity-and-access-management-iam-aws-cognito-activities","text":"create-iam-group Description : Creates a new IAM group within the system. Use Case : Used when a new group of users with specific permissions needs to be created. delete-iam-group Description : Deletes an existing IAM group. Use Case : Used when an IAM group is no longer needed and should be removed. add-user-to-iam-group Description : Adds a user to an IAM group. Use Case : Used to grant a user the permissions associated with the IAM group. remove-user-from-iam-group Description : Removes a user from an IAM group. Use Case : Used to revoke a user's permissions associated with the IAM group. retrieve-email-from-iam Description : Retrieves a user's email address from the IAM system. Use Case : Used to get the email address of a user for communication purposes. iam-create-systemuser Description : Creates a system user in the IAM system. Use Case : Used to create a non-human user that interacts with the system programmatically. iam-create-user Description : Creates a new user in the IAM system. Use Case : Used for onboarding new users. iam-delete-user Description : Deletes a user from the IAM system. Use Case : Used when a user account needs to be permanently removed.","title":"Identity and Access Management (IAM - AWS Cognito) Activities"},{"location":"01_Write_Domain/03_automations/#communication-and-notification-activities","text":"render-template Description : Renders a template with specified data. Use Case : Used to create personalized messages or documents. send-email Description : Sends an email to specified recipients. (AWS SES) Use Case : Used for sending notifications, alerts, or other communications. send-graphql-notification Description : Sends a notification via the GraphQL endpoint. Use Case : Used for notifying clients or systems through the GraphQL API.","title":"Communication and Notification Activities"},{"location":"01_Write_Domain/03_automations/#file-and-data-operations","text":"write-file Description : Writes data to a file. Use Case : Used to save data in a remote file system (AWS S3). fetch-property Description : Fetches a specific property or data point from the system. Use Case : Used to retrieve configuration settings or other data.","title":"File and Data Operations"},{"location":"01_Write_Domain/03_automations/#token-management-activities","text":"get-token Description : Retrieves a token for authentication or authorization purposes. Use Case : Used to get tokens required for accessing secured resources. get-systemuser-token Description : Retrieves a token for a system user. Use Case : Used for system-to-system authentication.","title":"Token Management Activities"},{"location":"01_Write_Domain/03_automations/#variable-and-state-management","text":"set-variable Description : Sets a flow variable to a specified value. Use Case : Used to store temporary data or state within an automation.","title":"Variable and State Management"},{"location":"01_Write_Domain/03_automations/#api-and-external-service-interactions","text":"call-internal-api Description : Calls an internal GraphQL API endpoint. Use Case : Used to invoke internal system functions (Commands/Queries/Projections). HTTP Description : Makes an HTTP request to an external service. Use Case : Used to interact with external APIs or services.","title":"API and External Service Interactions"},{"location":"01_Write_Domain/03_automations/#miscellaneous-activities","text":"code Description : Executes custom code. Use Case : Used for complex logic that cannot be handled by predefined activities. invalidate-cdn Description : Invalidates a CDN cache. Use Case : Used to ensure that updated content is served from the AWS CloudFront CDN. loop Description : Repeats a set of activities a specified number of times. Use Case : Used for iterating over a collection of items or repeating an action multiple times.","title":"Miscellaneous Activities"},{"location":"01_Write_Domain/03_automations/#summary","text":"Automations leverage a diverse set of activity types to perform various tasks automatically. By combining these activities, systems can create sophisticated workflows that enhance functionality, improve efficiency, and provide timely responses to events and conditions. Understanding these activity types is essential for designing effective automations that meet specific business needs.","title":"Summary"},{"location":"02_Read_Domain/","text":"About the Read domain # Overview # The View domain within the architecture serves the primary function of providing optimized and tailored data access for querying purposes. It complements the write operations handled by the Command domain by offering a structured approach to retrieving and presenting data from the system. This separation ensures that querying operations do not impact the integrity or performance of the write operations, facilitating efficient data retrieval and presentation. Key Concepts # Projection of Truth : Views in the architecture are designed to project the truth of the system's state into optimized read models. While the command side focuses on recording domain events and maintaining the system's state, the view side is concerned with interpreting this state for query purposes. Materialized Views : These are pre-computed views optimized for specific querying needs. Materialized views store aggregated or transformed data in a way that enhances query performance, enabling faster retrieval compared to recalculating data on the fly. Decoupling of Models : The View domain decouples the read model from the write model. This separation allows flexibility in how data is structured and queried, accommodating different perspectives and needs without affecting the core data integrity. Functional Aspects # Query Execution : Views facilitate efficient querying by providing access to materialized views and optimized data structures. This capability supports complex querying scenarios, including aggregations, filtering, and combining data from different sources. Functional Keys : Unlike technical keys used in the write model, views often employ functional keys that can evolve over time to meet changing business requirements. This flexibility allows the system to adapt to new data access patterns without disrupting existing operations. Just in time Projection : The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results. Benefits # Performance Optimization : By pre-computing and optimizing data for specific queries, views enhance overall system performance by reducing query latency and resource consumption. Flexibility and Adaptability : The decoupling of read and write models allows for independent evolution of data access patterns and query optimizations, ensuring that the system can adapt to new requirements without extensive refactoring. Improved Scalability : Separating read operations from write operations improves scalability by distributing the workload more efficiently across different components or services. Our implementation relies on AWS AppSync to Conclusion # In conclusion, the View domain plays a crucial role in the architecture by providing efficient, optimized, and flexible data access for querying purposes. By leveraging materialized views, decoupled models, and optimized query execution, the View domain ensures that querying operations are performant, scalable, and adaptable to evolving business needs.","title":"About the Read domain"},{"location":"02_Read_Domain/#about-the-read-domain","text":"","title":"About the Read domain"},{"location":"02_Read_Domain/#overview","text":"The View domain within the architecture serves the primary function of providing optimized and tailored data access for querying purposes. It complements the write operations handled by the Command domain by offering a structured approach to retrieving and presenting data from the system. This separation ensures that querying operations do not impact the integrity or performance of the write operations, facilitating efficient data retrieval and presentation.","title":"Overview"},{"location":"02_Read_Domain/#key-concepts","text":"Projection of Truth : Views in the architecture are designed to project the truth of the system's state into optimized read models. While the command side focuses on recording domain events and maintaining the system's state, the view side is concerned with interpreting this state for query purposes. Materialized Views : These are pre-computed views optimized for specific querying needs. Materialized views store aggregated or transformed data in a way that enhances query performance, enabling faster retrieval compared to recalculating data on the fly. Decoupling of Models : The View domain decouples the read model from the write model. This separation allows flexibility in how data is structured and queried, accommodating different perspectives and needs without affecting the core data integrity.","title":"Key Concepts"},{"location":"02_Read_Domain/#functional-aspects","text":"Query Execution : Views facilitate efficient querying by providing access to materialized views and optimized data structures. This capability supports complex querying scenarios, including aggregations, filtering, and combining data from different sources. Functional Keys : Unlike technical keys used in the write model, views often employ functional keys that can evolve over time to meet changing business requirements. This flexibility allows the system to adapt to new data access patterns without disrupting existing operations. Just in time Projection : The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results.","title":"Functional Aspects"},{"location":"02_Read_Domain/#benefits","text":"Performance Optimization : By pre-computing and optimizing data for specific queries, views enhance overall system performance by reducing query latency and resource consumption. Flexibility and Adaptability : The decoupling of read and write models allows for independent evolution of data access patterns and query optimizations, ensuring that the system can adapt to new requirements without extensive refactoring. Improved Scalability : Separating read operations from write operations improves scalability by distributing the workload more efficiently across different components or services. Our implementation relies on AWS AppSync to","title":"Benefits"},{"location":"02_Read_Domain/#conclusion","text":"In conclusion, the View domain plays a crucial role in the architecture by providing efficient, optimized, and flexible data access for querying purposes. By leveraging materialized views, decoupled models, and optimized query execution, the View domain ensures that querying operations are performant, scalable, and adaptable to evolving business needs.","title":"Conclusion"},{"location":"02_Read_Domain/01_views/","text":"Views # Views are defined entities that represent a set of attributes and may contain transformation of data from underlying aggregates (data sources). Each view is named and configured with specific parameters. Fields # Views represent collections of documents, fields are part of the document schema. Fields are configured with a name and a type (String, Int, Float, Boolean, StringList). Relations # The document schema of a view may have relations to other views. These may be OneToOne, OneToMany, ManyToOne, and ManyToMany. A special type of relation is the ObjectList. While the other relations point to other documents in the view store, ObjectList represents a nested object in the document. Relations are part of the document schema of a view and have a name, relation type, and referenced view-model as minimal configuration. Depending on the relation type, it may be mandatory to configure the foreign key (this may be a canonical key, an advanced feature explained later). Optionally, it is possible to require a specific role (authorization) to access the relational data. The default authorization method is inherit from the API query , meaning if the client has access to the view, they have access to the relational data. Business Key # One of the fields may be appointed to serve as the business key (also called the primary key). The field that is used as the business key must be of the type String . Only when a business key is appointed, the view can be stored in the view-store as a root document. Only root documents may have queries exposed in the GraphQL API. If no business key is appointed, the view model can only serve as ObjectList relations in other views. Data Mapping # Views with an appointed business key need to be filled with data published by aggregates. There are two types of event-handlers that are configured to listen for snapshot-events from a specific aggregate. A view may have multiple event handlers observing different aggregates or multiple handlers observing the same aggregate. This is useful when the aggregate instance has its business key updated, allowing one handler to create a new instance with the new key and another handler to remove the old view instance. A snapshot-event is streamed to the view-store every time an aggregate emits a domain-event. Mapper # The first type of event-handler is a simple mapper. It enables you to map aggregate fields and collections to view-fields and ObjectList relations. Additionally, you must configure which aggregate field (String) serves as the business-key of the view-instance. Note that the business-key for the view may differ from the business-key from the aggregate. Furthermore, you can select a processing strategy, either item or dictionary . Item : The mapping is executed once for every aggregate snapshot. Dictionary : In this case, you select one of the aggregate's nested collections to be processed. This is useful when a nested collection in an aggregate must be autonomously queryable. Additionally, you can configure multiple delete-expressions, causing deletion of the view instance when any of the expressions evaluate to true. Custom Mapper # Instead of relying on the standard mapping, you can use Python code to convert aggregate data into the view instance. This allows for advanced use cases like pre-filtering and enrichment beyond the capabilities of the standard mapper. For example, you could create multilevel nesting in the document (ObjectList relation) which the standard mapper only provides for one level. However, for maintainability, simpler is usually better. GraphQL Queries # Queries associated with views define access patterns for retrieving data via GraphQL. The GraphQL namespace is configured at the view level, meaning all queries in a specific view are bundled in the same namespace. Each query has a unique name in this namespace, and the needed access control method can be configured: anonymous : Requires an API key that can be configured in the client. Because this key may become publicly available, the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. user : Requires a predefined query parameter to match the signed-in user, making the data accessible only to the current signed-in user. role-based : Requires the user to have a specific role to access the data. There are three types of queries: get : Returns one instance of the view based on the business key. This type can be configured once per view. filter : Returns a list of instances based on defined filter clauses, e.g., field x equals a query parameter . During modeling, the possible filter clauses are defined. Filters may also provide canonical search, making the key_begins_with query parameter mandatory. This type can be configured only once per view. named query : Similar to the filter query but with a configurable name during modeling, and the filter clauses are mandatory for the client. Data Retention and Exclusion # Views may specify: Data Retention : Period for which data within the view remains accessible. Defined in days, the default is -1, translating to indefinitely. Notification Exclusion : By default, the view store will publish a GraphQL notification to which clients can subscribe. The notification contains the view name and the primary key, allowing the client to refetch the data when an update occurs. Subscriptions to the notification may be anonymous. This option prevents the publication of this notification, e.g., because the business key contains identifiable data like a username or an email address. Benefits # Data Consistency : Views provide consistent and up-to-date representations of data entities. Customization : Custom handlers enable tailored logic and functionalities within views. Integration : Seamless integration with underlying aggregates ensures data accuracy and synchronization. Security : Authorization controls ensure data access is restricted as per defined policies.","title":"Views"},{"location":"02_Read_Domain/01_views/#views","text":"Views are defined entities that represent a set of attributes and may contain transformation of data from underlying aggregates (data sources). Each view is named and configured with specific parameters.","title":"Views"},{"location":"02_Read_Domain/01_views/#fields","text":"Views represent collections of documents, fields are part of the document schema. Fields are configured with a name and a type (String, Int, Float, Boolean, StringList).","title":"Fields"},{"location":"02_Read_Domain/01_views/#relations","text":"The document schema of a view may have relations to other views. These may be OneToOne, OneToMany, ManyToOne, and ManyToMany. A special type of relation is the ObjectList. While the other relations point to other documents in the view store, ObjectList represents a nested object in the document. Relations are part of the document schema of a view and have a name, relation type, and referenced view-model as minimal configuration. Depending on the relation type, it may be mandatory to configure the foreign key (this may be a canonical key, an advanced feature explained later). Optionally, it is possible to require a specific role (authorization) to access the relational data. The default authorization method is inherit from the API query , meaning if the client has access to the view, they have access to the relational data.","title":"Relations"},{"location":"02_Read_Domain/01_views/#business-key","text":"One of the fields may be appointed to serve as the business key (also called the primary key). The field that is used as the business key must be of the type String . Only when a business key is appointed, the view can be stored in the view-store as a root document. Only root documents may have queries exposed in the GraphQL API. If no business key is appointed, the view model can only serve as ObjectList relations in other views.","title":"Business Key"},{"location":"02_Read_Domain/01_views/#data-mapping","text":"Views with an appointed business key need to be filled with data published by aggregates. There are two types of event-handlers that are configured to listen for snapshot-events from a specific aggregate. A view may have multiple event handlers observing different aggregates or multiple handlers observing the same aggregate. This is useful when the aggregate instance has its business key updated, allowing one handler to create a new instance with the new key and another handler to remove the old view instance. A snapshot-event is streamed to the view-store every time an aggregate emits a domain-event.","title":"Data Mapping"},{"location":"02_Read_Domain/01_views/#mapper","text":"The first type of event-handler is a simple mapper. It enables you to map aggregate fields and collections to view-fields and ObjectList relations. Additionally, you must configure which aggregate field (String) serves as the business-key of the view-instance. Note that the business-key for the view may differ from the business-key from the aggregate. Furthermore, you can select a processing strategy, either item or dictionary . Item : The mapping is executed once for every aggregate snapshot. Dictionary : In this case, you select one of the aggregate's nested collections to be processed. This is useful when a nested collection in an aggregate must be autonomously queryable. Additionally, you can configure multiple delete-expressions, causing deletion of the view instance when any of the expressions evaluate to true.","title":"Mapper"},{"location":"02_Read_Domain/01_views/#custom-mapper","text":"Instead of relying on the standard mapping, you can use Python code to convert aggregate data into the view instance. This allows for advanced use cases like pre-filtering and enrichment beyond the capabilities of the standard mapper. For example, you could create multilevel nesting in the document (ObjectList relation) which the standard mapper only provides for one level. However, for maintainability, simpler is usually better.","title":"Custom Mapper"},{"location":"02_Read_Domain/01_views/#graphql-queries","text":"Queries associated with views define access patterns for retrieving data via GraphQL. The GraphQL namespace is configured at the view level, meaning all queries in a specific view are bundled in the same namespace. Each query has a unique name in this namespace, and the needed access control method can be configured: anonymous : Requires an API key that can be configured in the client. Because this key may become publicly available, the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. user : Requires a predefined query parameter to match the signed-in user, making the data accessible only to the current signed-in user. role-based : Requires the user to have a specific role to access the data. There are three types of queries: get : Returns one instance of the view based on the business key. This type can be configured once per view. filter : Returns a list of instances based on defined filter clauses, e.g., field x equals a query parameter . During modeling, the possible filter clauses are defined. Filters may also provide canonical search, making the key_begins_with query parameter mandatory. This type can be configured only once per view. named query : Similar to the filter query but with a configurable name during modeling, and the filter clauses are mandatory for the client.","title":"GraphQL Queries"},{"location":"02_Read_Domain/01_views/#data-retention-and-exclusion","text":"Views may specify: Data Retention : Period for which data within the view remains accessible. Defined in days, the default is -1, translating to indefinitely. Notification Exclusion : By default, the view store will publish a GraphQL notification to which clients can subscribe. The notification contains the view name and the primary key, allowing the client to refetch the data when an update occurs. Subscriptions to the notification may be anonymous. This option prevents the publication of this notification, e.g., because the business key contains identifiable data like a username or an email address.","title":"Data Retention and Exclusion"},{"location":"02_Read_Domain/01_views/#benefits","text":"Data Consistency : Views provide consistent and up-to-date representations of data entities. Customization : Custom handlers enable tailored logic and functionalities within views. Integration : Seamless integration with underlying aggregates ensures data accuracy and synchronization. Security : Authorization controls ensure data access is restricted as per defined policies.","title":"Benefits"},{"location":"02_Read_Domain/02_projections/","text":"Projections # The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results. API Configuration # To configure a projection, several key elements must be defined: Projection Name # A unique identifier for the projection, which helps in managing and referencing the projection. GraphQL Namespace # The namespace under which the projection's GraphQL queries will be bundled. This helps in organizing queries logically. GraphQL Method Name # The method name used to invoke the projection via GraphQL. This name should be unique within the specified namespace. Authorization Method # Specifies the access control for the projection. The following methods are supported: anonymous : Requires an API key that can be configured in the client. This key may be publicly available, and the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. role based : Requires the user to have a specific role to access the data. Return Object # References a view model that defines the structure of the data returned by the projection. Return Type # Specifies whether the projection returns a single item or a result set. The possible values are: item : Returns a single instance of the view. result_set : Returns a list of instances. Arguments # Projections can accept various query variables, which are specified as follows: Name # The name of the query variable. Type # The data type of the query variable. Supported types include: String Int Float Boolean StringList Required # Specifies whether the query variable is mandatory for the projection to execute. The possible values are: true false Data Preparation Logic # The core logic for preparing data in a projection is written in Python. This logic involves accessing materialized views, applying filters, and transforming data as needed. The following example demonstrates how to implement the data preparation logic: from draftsman.ViewStoreApi import Query def transform(arguments, username): # You have access to the username of the requestor and the arguments. print(f\"Handle graph request [{arguments}/{username}]\") # You have access to a fluent API to access materialized views # Here are some examples: # Access a specific object (most efficient method cost-wise) data = Query('ViewName').get_item(\"key\").run() # Get a list of all objects of a specific type data = Query('ViewName').get_items().run() # Filter a list of objects based on type and key, part of the canonical key concept data = Query('ViewName').get_items(\"key_starts_with_this_value\").run() # Filter on content for a specific type (performs a scan on all views of type 'ViewName') data = Query('ViewName').get_items().equals('key', 'value').between('key', 0, 100).run() # Combine the two filter methods to filter within a subset data = Query('ViewName').get_items(\"key_starts_with_this_value\").equals('key', 'value').between('key', 0, 100).run() # Switch to an \"or\" operator for filters (default is \"and\") data = Query('ViewName').get_items(key=\"key_starts_with_this_value\", filter_chain_method=\"or\").equals('key', 'value').between( 'key', 0, 100).run() # Program data transformations with Python # Ensure you add all fields that are defined in the return view object definition return {\"field_name\": \"value\"} Fluent API # The Fluent API, which allows you to query and filter data from the DynamoDB view store in a flexible and efficient manner. The Fluent API provides a convenient way to construct and execute queries against the DynamoDB view-store-table. It supports various filtering methods and allows for the retrieval of individual items or sets of items based on specified criteria. Initialization # To start using the Fluent API, you need to initialize the Query class with the type of view you want to query. from draftsman.ViewStoreApi import Query # Initialize a query for a specific view name query = Query(\"YourViewName\") Methods # Retrieving Items # get_item # Retrieve a single item based on its key. query.get_item(\"your_item_key\") get_items # Retrieve a set of items, optionally filtered by a key prefix. query.get_items(key=\"key_prefix\", filter_chain_method=\"and\") key (optional): A prefix to filter items by their keys. filter_chain_method (optional): Determines how multiple filters are combined. Can be \"and\" or \"or\". Default is \" and\". Adding Filters # Filters can be applied to the query to narrow down the results. The following filter methods are available: equals # Filter items where the attribute equals a specified value. query.equals(\"attribute_name\", \"value\") less_than # Filter items where the attribute is less than a specified value. query.less_than(\"attribute_name\", value) less_than_equals # Filter items where the attribute is less than or equal to a specified value. query.less_than_equals(\"attribute_name\", value) greater_than # Filter items where the attribute is greater than a specified value. query.greater_than(\"attribute_name\", value) greater_than_equals # Filter items where the attribute is greater than or equal to a specified value. query.greater_than_equals(\"attribute_name\", value) begins_with # Filter items where the attribute begins with a specified value. query.begins_with(\"attribute_name\", \"value_prefix\") between # Filter items where the attribute is between two specified values. query.between(\"attribute_name\", low_value, high_value) not_equals # Filter items where the attribute does not equal a specified value. query.not_equals(\"attribute_name\", \"value\") exists # Filter items where the attribute exists. query.exists(\"attribute_name\") not_exists # Filter items where the attribute does not exist. query.not_exists(\"attribute_name\") contains # Filter items where the attribute contains a specified value. query.contains(\"attribute_name\", \"value\") attribute_type # Filter items where the attribute is of a specified type. query.attribute_type(\"attribute_name\", \"type\") is_in # Filter items where the attribute is in a specified list of values. query.is_in(\"attribute_name\", [\"value1\", \"value2\", \"value3\"]) Running the Query # Once the query is constructed with the necessary filters, it can be executed using the run method. result = query.run() If get_item was used, run returns a single item. If get_items was used, run returns a list of items matching the query. Method chaining # The Fluent API supports method chaining, allowing you to construct complex queries in a readable and concise manner by chaining multiple method calls together. Method chaining enhances the clarity of query construction and reduces the need for intermediate variables. resultset = query.get_items(\"key_prefix\").equals(\"status\", \"active\").greater_than(\"priority\", 1).equals(\"user\",\"j.doe\").run()","title":"Projections"},{"location":"02_Read_Domain/02_projections/#projections","text":"The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results.","title":"Projections"},{"location":"02_Read_Domain/02_projections/#api-configuration","text":"To configure a projection, several key elements must be defined:","title":"API Configuration"},{"location":"02_Read_Domain/02_projections/#projection-name","text":"A unique identifier for the projection, which helps in managing and referencing the projection.","title":"Projection Name"},{"location":"02_Read_Domain/02_projections/#graphql-namespace","text":"The namespace under which the projection's GraphQL queries will be bundled. This helps in organizing queries logically.","title":"GraphQL Namespace"},{"location":"02_Read_Domain/02_projections/#graphql-method-name","text":"The method name used to invoke the projection via GraphQL. This name should be unique within the specified namespace.","title":"GraphQL Method Name"},{"location":"02_Read_Domain/02_projections/#authorization-method","text":"Specifies the access control for the projection. The following methods are supported: anonymous : Requires an API key that can be configured in the client. This key may be publicly available, and the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. role based : Requires the user to have a specific role to access the data.","title":"Authorization Method"},{"location":"02_Read_Domain/02_projections/#return-object","text":"References a view model that defines the structure of the data returned by the projection.","title":"Return Object"},{"location":"02_Read_Domain/02_projections/#return-type","text":"Specifies whether the projection returns a single item or a result set. The possible values are: item : Returns a single instance of the view. result_set : Returns a list of instances.","title":"Return Type"},{"location":"02_Read_Domain/02_projections/#arguments","text":"Projections can accept various query variables, which are specified as follows:","title":"Arguments"},{"location":"02_Read_Domain/02_projections/#name","text":"The name of the query variable.","title":"Name"},{"location":"02_Read_Domain/02_projections/#type","text":"The data type of the query variable. Supported types include: String Int Float Boolean StringList","title":"Type"},{"location":"02_Read_Domain/02_projections/#required","text":"Specifies whether the query variable is mandatory for the projection to execute. The possible values are: true false","title":"Required"},{"location":"02_Read_Domain/02_projections/#data-preparation-logic","text":"The core logic for preparing data in a projection is written in Python. This logic involves accessing materialized views, applying filters, and transforming data as needed. The following example demonstrates how to implement the data preparation logic: from draftsman.ViewStoreApi import Query def transform(arguments, username): # You have access to the username of the requestor and the arguments. print(f\"Handle graph request [{arguments}/{username}]\") # You have access to a fluent API to access materialized views # Here are some examples: # Access a specific object (most efficient method cost-wise) data = Query('ViewName').get_item(\"key\").run() # Get a list of all objects of a specific type data = Query('ViewName').get_items().run() # Filter a list of objects based on type and key, part of the canonical key concept data = Query('ViewName').get_items(\"key_starts_with_this_value\").run() # Filter on content for a specific type (performs a scan on all views of type 'ViewName') data = Query('ViewName').get_items().equals('key', 'value').between('key', 0, 100).run() # Combine the two filter methods to filter within a subset data = Query('ViewName').get_items(\"key_starts_with_this_value\").equals('key', 'value').between('key', 0, 100).run() # Switch to an \"or\" operator for filters (default is \"and\") data = Query('ViewName').get_items(key=\"key_starts_with_this_value\", filter_chain_method=\"or\").equals('key', 'value').between( 'key', 0, 100).run() # Program data transformations with Python # Ensure you add all fields that are defined in the return view object definition return {\"field_name\": \"value\"}","title":"Data Preparation Logic"},{"location":"02_Read_Domain/02_projections/#fluent-api","text":"The Fluent API, which allows you to query and filter data from the DynamoDB view store in a flexible and efficient manner. The Fluent API provides a convenient way to construct and execute queries against the DynamoDB view-store-table. It supports various filtering methods and allows for the retrieval of individual items or sets of items based on specified criteria.","title":"Fluent API"},{"location":"02_Read_Domain/02_projections/#initialization","text":"To start using the Fluent API, you need to initialize the Query class with the type of view you want to query. from draftsman.ViewStoreApi import Query # Initialize a query for a specific view name query = Query(\"YourViewName\")","title":"Initialization"},{"location":"02_Read_Domain/02_projections/#methods","text":"","title":"Methods"},{"location":"02_Read_Domain/02_projections/#retrieving-items","text":"","title":"Retrieving Items"},{"location":"02_Read_Domain/02_projections/#get_item","text":"Retrieve a single item based on its key. query.get_item(\"your_item_key\")","title":"get_item"},{"location":"02_Read_Domain/02_projections/#get_items","text":"Retrieve a set of items, optionally filtered by a key prefix. query.get_items(key=\"key_prefix\", filter_chain_method=\"and\") key (optional): A prefix to filter items by their keys. filter_chain_method (optional): Determines how multiple filters are combined. Can be \"and\" or \"or\". Default is \" and\".","title":"get_items"},{"location":"02_Read_Domain/02_projections/#adding-filters","text":"Filters can be applied to the query to narrow down the results. The following filter methods are available:","title":"Adding Filters"},{"location":"02_Read_Domain/02_projections/#equals","text":"Filter items where the attribute equals a specified value. query.equals(\"attribute_name\", \"value\")","title":"equals"},{"location":"02_Read_Domain/02_projections/#less_than","text":"Filter items where the attribute is less than a specified value. query.less_than(\"attribute_name\", value)","title":"less_than"},{"location":"02_Read_Domain/02_projections/#less_than_equals","text":"Filter items where the attribute is less than or equal to a specified value. query.less_than_equals(\"attribute_name\", value)","title":"less_than_equals"},{"location":"02_Read_Domain/02_projections/#greater_than","text":"Filter items where the attribute is greater than a specified value. query.greater_than(\"attribute_name\", value)","title":"greater_than"},{"location":"02_Read_Domain/02_projections/#greater_than_equals","text":"Filter items where the attribute is greater than or equal to a specified value. query.greater_than_equals(\"attribute_name\", value)","title":"greater_than_equals"},{"location":"02_Read_Domain/02_projections/#begins_with","text":"Filter items where the attribute begins with a specified value. query.begins_with(\"attribute_name\", \"value_prefix\")","title":"begins_with"},{"location":"02_Read_Domain/02_projections/#between","text":"Filter items where the attribute is between two specified values. query.between(\"attribute_name\", low_value, high_value)","title":"between"},{"location":"02_Read_Domain/02_projections/#not_equals","text":"Filter items where the attribute does not equal a specified value. query.not_equals(\"attribute_name\", \"value\")","title":"not_equals"},{"location":"02_Read_Domain/02_projections/#exists","text":"Filter items where the attribute exists. query.exists(\"attribute_name\")","title":"exists"},{"location":"02_Read_Domain/02_projections/#not_exists","text":"Filter items where the attribute does not exist. query.not_exists(\"attribute_name\")","title":"not_exists"},{"location":"02_Read_Domain/02_projections/#contains","text":"Filter items where the attribute contains a specified value. query.contains(\"attribute_name\", \"value\")","title":"contains"},{"location":"02_Read_Domain/02_projections/#attribute_type","text":"Filter items where the attribute is of a specified type. query.attribute_type(\"attribute_name\", \"type\")","title":"attribute_type"},{"location":"02_Read_Domain/02_projections/#is_in","text":"Filter items where the attribute is in a specified list of values. query.is_in(\"attribute_name\", [\"value1\", \"value2\", \"value3\"])","title":"is_in"},{"location":"02_Read_Domain/02_projections/#running-the-query","text":"Once the query is constructed with the necessary filters, it can be executed using the run method. result = query.run() If get_item was used, run returns a single item. If get_items was used, run returns a list of items matching the query.","title":"Running the Query"},{"location":"02_Read_Domain/02_projections/#method-chaining","text":"The Fluent API supports method chaining, allowing you to construct complex queries in a readable and concise manner by chaining multiple method calls together. Method chaining enhances the clarity of query construction and reduces the need for intermediate variables. resultset = query.get_items(\"key_prefix\").equals(\"status\", \"active\").greater_than(\"priority\", 1).equals(\"user\",\"j.doe\").run()","title":"Method chaining"},{"location":"03_Utils/","text":"Globaly available concepts # Utils are globally available concepts that are modeled once and used across the system. They include: Expressions # Expressions are reusable logical constructs used to transform data on-the-fly before executing logic. There are two types of expressions: API Authorization Expression # These expressions are used in models that define an access path in the GraphQL API. They convert query variables to specific roles, useful in multi-tenant systems where each tenant has a private set of roles. Trigger Key Expression # The key expression is usable in all identity-based triggers: Aggregate behavior flows & View data sources. It is used to convert an event attribute into a functional key. Dependencies # Allows you to add pip packages to you runtime, the package will be available for importing in all custom python code in your project. Patterns # Patterns are regular expressions that can be used while modelling commands to ensure a specific pattern for String fields e.g. LowercasedOnly # ^[a-z]+$ Date # ^(?:20)\\d{2}-\\d{2}-\\d{2}$ Extending patters # Patterns may reference each-other e.g. ^{{LowercasedOnly}}:arn:{{LowercasedOnly}}$ Roles # Simply a list of role names available in the modeler, at this moment these roles are not automatically loaded into the the IAM system by Draftsman. The application owner needs to define these roles manually, or model an automation (@afterDeployment) that creates these roles. Python modules # Python modules are usable scripts that define methods that are accessible from: Behavior flows Automations def behavior_or_notifier_function(flow): # May set a flow variable flow.myVariable = \"Hello World!\" # And has access to flow variables print(flow.myVariable) # And has also access to the aggregate document print(flow.entity) print(flow.entity.entityField) It is a simple function but it does provide you access to all variables available to the flow execution.","title":"Globaly available concepts"},{"location":"03_Utils/#globaly-available-concepts","text":"Utils are globally available concepts that are modeled once and used across the system. They include:","title":"Globaly available concepts"},{"location":"03_Utils/#expressions","text":"Expressions are reusable logical constructs used to transform data on-the-fly before executing logic. There are two types of expressions:","title":"Expressions"},{"location":"03_Utils/#api-authorization-expression","text":"These expressions are used in models that define an access path in the GraphQL API. They convert query variables to specific roles, useful in multi-tenant systems where each tenant has a private set of roles.","title":"API Authorization Expression"},{"location":"03_Utils/#trigger-key-expression","text":"The key expression is usable in all identity-based triggers: Aggregate behavior flows & View data sources. It is used to convert an event attribute into a functional key.","title":"Trigger Key Expression"},{"location":"03_Utils/#dependencies","text":"Allows you to add pip packages to you runtime, the package will be available for importing in all custom python code in your project.","title":"Dependencies"},{"location":"03_Utils/#patterns","text":"Patterns are regular expressions that can be used while modelling commands to ensure a specific pattern for String fields e.g.","title":"Patterns"},{"location":"03_Utils/#lowercasedonly","text":"^[a-z]+$","title":"LowercasedOnly"},{"location":"03_Utils/#date","text":"^(?:20)\\d{2}-\\d{2}-\\d{2}$","title":"Date"},{"location":"03_Utils/#extending-patters","text":"Patterns may reference each-other e.g. ^{{LowercasedOnly}}:arn:{{LowercasedOnly}}$","title":"Extending patters"},{"location":"03_Utils/#roles","text":"Simply a list of role names available in the modeler, at this moment these roles are not automatically loaded into the the IAM system by Draftsman. The application owner needs to define these roles manually, or model an automation (@afterDeployment) that creates these roles.","title":"Roles"},{"location":"03_Utils/#python-modules","text":"Python modules are usable scripts that define methods that are accessible from: Behavior flows Automations def behavior_or_notifier_function(flow): # May set a flow variable flow.myVariable = \"Hello World!\" # And has access to flow variables print(flow.myVariable) # And has also access to the aggregate document print(flow.entity) print(flow.entity.entityField) It is a simple function but it does provide you access to all variables available to the flow execution.","title":"Python modules"},{"location":"03_Utils/01_expressions/","text":"Expressions # API Authorization Expression # The authorization expression can be used in all components exposed in the API: Commands View queries It is used to convert an input parameter (command field or query filter field, e.g., key, key_begins_with, or a custom filter attribute) to a technical role that the API resolver will validate to determine if the requester has the specific role. This is useful for providing role-based access in a multi-tenant system. The expression has a name , e.g., extractRoleFromArn , which is used to access the expression from command models or view models. You model inputs for this function separated with a ; e.g., arn;role And then use Velocity Template Language (VTL) syntax with basic JavaScript to model the logic: ${arn.split(':')[0]}:${arn.split(':')[1]}:role In a command or view query, you can use this expression in the role field when you select role-based access: #global.extractRoleFromArn(key, 'viewer') When you execute, for example, a query where this is implemented, it will evaluate to: #foreach($group in $context.identity.claims.get(\"cognito:groups\")) #if($group == \"${ctx.args.key.split(':')[0]}:${ctx.args.key.split(':')[1]}:viewer\") #set($inCognitoGroup = true) #end #end #if($inCognitoGroup){ \"version\": \"2018-05-29\", \"operation\": \"GetItem\", \"key\": { \"type\": $util.dynamodb.toDynamoDBJson(\"ViewName\"), \"key\": $util.dynamodb.toDynamoDBJson($ctx.args.key) } } #else $utils.unauthorized() #end Trigger Key Expression # The key expression is usable in all identity-based triggers: Aggregate behavior flows View data sources It is used to convert an event attribute into a functional key. The name of the expression, e.g., truncateArn , is used to access this expression from a behavior or data-source model. You can model input for the expression, e.g., arn;length . The expression itself is written in pure Python, e.g., ':'.join(arn.split(':')[:int(length)]) You can access it by configuring a method call inside the key field of behavior or data-source. The input parameters will reference a trigger attribute. You can use literals, but they can only be strings. In our case, the expression will evaluate the string to an integer. #global.truncateArn(childArn, '2') Let's say that the trigger contains an attribute childArn=draftsmanid:workspace:project , then the resulting key for the flow will be draftsmanid:workspace .","title":"Expressions"},{"location":"03_Utils/01_expressions/#expressions","text":"","title":"Expressions"},{"location":"03_Utils/01_expressions/#api-authorization-expression","text":"The authorization expression can be used in all components exposed in the API: Commands View queries It is used to convert an input parameter (command field or query filter field, e.g., key, key_begins_with, or a custom filter attribute) to a technical role that the API resolver will validate to determine if the requester has the specific role. This is useful for providing role-based access in a multi-tenant system. The expression has a name , e.g., extractRoleFromArn , which is used to access the expression from command models or view models. You model inputs for this function separated with a ; e.g., arn;role And then use Velocity Template Language (VTL) syntax with basic JavaScript to model the logic: ${arn.split(':')[0]}:${arn.split(':')[1]}:role In a command or view query, you can use this expression in the role field when you select role-based access: #global.extractRoleFromArn(key, 'viewer') When you execute, for example, a query where this is implemented, it will evaluate to: #foreach($group in $context.identity.claims.get(\"cognito:groups\")) #if($group == \"${ctx.args.key.split(':')[0]}:${ctx.args.key.split(':')[1]}:viewer\") #set($inCognitoGroup = true) #end #end #if($inCognitoGroup){ \"version\": \"2018-05-29\", \"operation\": \"GetItem\", \"key\": { \"type\": $util.dynamodb.toDynamoDBJson(\"ViewName\"), \"key\": $util.dynamodb.toDynamoDBJson($ctx.args.key) } } #else $utils.unauthorized() #end","title":"API Authorization Expression"},{"location":"03_Utils/01_expressions/#trigger-key-expression","text":"The key expression is usable in all identity-based triggers: Aggregate behavior flows View data sources It is used to convert an event attribute into a functional key. The name of the expression, e.g., truncateArn , is used to access this expression from a behavior or data-source model. You can model input for the expression, e.g., arn;length . The expression itself is written in pure Python, e.g., ':'.join(arn.split(':')[:int(length)]) You can access it by configuring a method call inside the key field of behavior or data-source. The input parameters will reference a trigger attribute. You can use literals, but they can only be strings. In our case, the expression will evaluate the string to an integer. #global.truncateArn(childArn, '2') Let's say that the trigger contains an attribute childArn=draftsmanid:workspace:project , then the resulting key for the flow will be draftsmanid:workspace .","title":"Trigger Key Expression"},{"location":"04_Pipeline/","text":"Deployment Pipeline # This documentation outlines the steps involved in our deployment pipeline, explaining the purpose and necessity of each step to ensure a smooth and reliable process from code generation to production deployment. Generate: Prepare Code # Purpose # The generate step is crucial for converting high-level models into executable code and validating that code through unit testing. This ensures that the code is correctly generated and functions as intended before moving further along the pipeline. Process # Model Conversion : The high-level model, which defines the application's structure and behavior, is converted into Python code and CloudFormation templates. This step translates abstract models into concrete, executable code and infrastructure definitions. Unit Testing : After the conversion, unit tests are executed to validate the functionality of the generated code. Unit tests help identify and fix issues early in the development process, ensuring that individual components of the application work correctly in isolation. (These are the modeled test cases in the behavior-flow models). Why It's Needed # Accuracy : Ensures that the model is correctly translated into code. Early Issue Detection : Unit tests catch errors early, reducing the risk of bugs in later stages. Validation : Confirms that the generated code adheres to expected behaviors and standards. Package: Prepare Deployment # Purpose # The package step involves preparing the application artifacts for deployment. These artifacts include all the necessary files and configurations that CloudFormation requires to deploy the application infrastructure. Process # Artifact Creation : Collect and package the code, CloudFormation templates, and other necessary resources into deployable artifacts. Versioning : Assign versions to the artifacts to manage releases and ensure consistency across different environments. Why It's Needed # Consistency : Ensures that the same artifacts are deployed across different environments, reducing discrepancies. Preparation : Packages the application into a format that CloudFormation can process, streamlining the deployment process. Staging: Deploy Staging # Purpose # The staging step involves deploying the application to a staging environment, which closely mirrors the production environment. This allows for comprehensive testing and validation before the application is deployed to production. Process # Deployment : Use CloudFormation to deploy the application artifacts to the staging environment. Configuration : Configure the staging environment to match the production setup as closely as possible. (e.g. manually load API secrets into the staging secrets manager) Why It's Needed # Validation : Provides a realistic environment to test the application, ensuring it behaves as expected in a production-like setting. Risk Mitigation : Identifies and resolves issues in a controlled environment, reducing the risk of problems in production. Quality Gate: Testing # Purpose # The quality gate step involves executing API tests against the staging environment to verify that the application is stable, functional, and ready for production deployment. This step ensures that the code meets the required quality standards. Process # Cleanup : Optionally, clear the environments databases and IAM role assignments. API Testing : Execute functional and integration tests on the staging environment, focusing on the application's API endpoints. These tests are modeled using the Tracepaper tool. Validation : Validate that all tests executed successfully. Coverage : Validate that there is sufficient coverage regarding the published events in the domain and the view queries. Why It's Needed # Quality Assurance : Ensures that the application meets quality standards and is free of critical bugs. Verification : Validates that the application behaves correctly under various scenarios and edge cases. Readiness Check : Confirms that the application is ready for production deployment. Production: Deploy Production # Purpose # The production step is the final stage, where the application is deployed to the production environment. This step is executed only after the application has passed all previous stages, ensuring a smooth and reliable deployment. Process # Deployment : Use CloudFormation to deploy the validated application artifacts to the production environment. Adviced: Monitoring : Immediately start monitoring the production environment to detect and address any issues that may arise post-deployment. This is not provided by Draftsman, we ourselves use Lumigo.io to monitor our applications. Why It's Needed # Go-Live : Makes the application available to end-users.","title":"Deployment Pipeline"},{"location":"04_Pipeline/#deployment-pipeline","text":"This documentation outlines the steps involved in our deployment pipeline, explaining the purpose and necessity of each step to ensure a smooth and reliable process from code generation to production deployment.","title":"Deployment Pipeline"},{"location":"04_Pipeline/#generate-prepare-code","text":"","title":"Generate: Prepare Code"},{"location":"04_Pipeline/#purpose","text":"The generate step is crucial for converting high-level models into executable code and validating that code through unit testing. This ensures that the code is correctly generated and functions as intended before moving further along the pipeline.","title":"Purpose"},{"location":"04_Pipeline/#process","text":"Model Conversion : The high-level model, which defines the application's structure and behavior, is converted into Python code and CloudFormation templates. This step translates abstract models into concrete, executable code and infrastructure definitions. Unit Testing : After the conversion, unit tests are executed to validate the functionality of the generated code. Unit tests help identify and fix issues early in the development process, ensuring that individual components of the application work correctly in isolation. (These are the modeled test cases in the behavior-flow models).","title":"Process"},{"location":"04_Pipeline/#why-its-needed","text":"Accuracy : Ensures that the model is correctly translated into code. Early Issue Detection : Unit tests catch errors early, reducing the risk of bugs in later stages. Validation : Confirms that the generated code adheres to expected behaviors and standards.","title":"Why It's Needed"},{"location":"04_Pipeline/#package-prepare-deployment","text":"","title":"Package: Prepare Deployment"},{"location":"04_Pipeline/#purpose_1","text":"The package step involves preparing the application artifacts for deployment. These artifacts include all the necessary files and configurations that CloudFormation requires to deploy the application infrastructure.","title":"Purpose"},{"location":"04_Pipeline/#process_1","text":"Artifact Creation : Collect and package the code, CloudFormation templates, and other necessary resources into deployable artifacts. Versioning : Assign versions to the artifacts to manage releases and ensure consistency across different environments.","title":"Process"},{"location":"04_Pipeline/#why-its-needed_1","text":"Consistency : Ensures that the same artifacts are deployed across different environments, reducing discrepancies. Preparation : Packages the application into a format that CloudFormation can process, streamlining the deployment process.","title":"Why It's Needed"},{"location":"04_Pipeline/#staging-deploy-staging","text":"","title":"Staging: Deploy Staging"},{"location":"04_Pipeline/#purpose_2","text":"The staging step involves deploying the application to a staging environment, which closely mirrors the production environment. This allows for comprehensive testing and validation before the application is deployed to production.","title":"Purpose"},{"location":"04_Pipeline/#process_2","text":"Deployment : Use CloudFormation to deploy the application artifacts to the staging environment. Configuration : Configure the staging environment to match the production setup as closely as possible. (e.g. manually load API secrets into the staging secrets manager)","title":"Process"},{"location":"04_Pipeline/#why-its-needed_2","text":"Validation : Provides a realistic environment to test the application, ensuring it behaves as expected in a production-like setting. Risk Mitigation : Identifies and resolves issues in a controlled environment, reducing the risk of problems in production.","title":"Why It's Needed"},{"location":"04_Pipeline/#quality-gate-testing","text":"","title":"Quality Gate: Testing"},{"location":"04_Pipeline/#purpose_3","text":"The quality gate step involves executing API tests against the staging environment to verify that the application is stable, functional, and ready for production deployment. This step ensures that the code meets the required quality standards.","title":"Purpose"},{"location":"04_Pipeline/#process_3","text":"Cleanup : Optionally, clear the environments databases and IAM role assignments. API Testing : Execute functional and integration tests on the staging environment, focusing on the application's API endpoints. These tests are modeled using the Tracepaper tool. Validation : Validate that all tests executed successfully. Coverage : Validate that there is sufficient coverage regarding the published events in the domain and the view queries.","title":"Process"},{"location":"04_Pipeline/#why-its-needed_3","text":"Quality Assurance : Ensures that the application meets quality standards and is free of critical bugs. Verification : Validates that the application behaves correctly under various scenarios and edge cases. Readiness Check : Confirms that the application is ready for production deployment.","title":"Why It's Needed"},{"location":"04_Pipeline/#production-deploy-production","text":"","title":"Production: Deploy Production"},{"location":"04_Pipeline/#purpose_4","text":"The production step is the final stage, where the application is deployed to the production environment. This step is executed only after the application has passed all previous stages, ensuring a smooth and reliable deployment.","title":"Purpose"},{"location":"04_Pipeline/#process_4","text":"Deployment : Use CloudFormation to deploy the validated application artifacts to the production environment. Adviced: Monitoring : Immediately start monitoring the production environment to detect and address any issues that may arise post-deployment. This is not provided by Draftsman, we ourselves use Lumigo.io to monitor our applications.","title":"Process"},{"location":"04_Pipeline/#why-its-needed_4","text":"Go-Live : Makes the application available to end-users.","title":"Why It's Needed"},{"location":"04_Pipeline/QualityGate/","text":"Quality Gate # The testing concept in the modeling tool ensures that the application acts as expected by validating the modeled behavior, data access, and authorizations before deploying to production. This section covers how to define, structure, and execute functional scenarios using the provided DSL. Purpose # The primary purpose of the test concept is to ensure application functionality, data integrity, and proper authorization by running tests against the GraphQL API of a deployed version of the application. These tests help verify that the application's behavior aligns with the modeled specifications. Functional Scenario Definition # Functional scenarios are defined and describe a sequence of actions and validations to be executed against the GraphQL API. Each scenario consists of various activities that can include mutations, queries, and validation steps. Available Actions # The model supports the following actions: Grant Role to Test User : Assigns a specific role to the current test user. Execute Mutation : Performs a mutation (command) to change the state of the application. Validate Behavior and Automations : Checks that expected behavior flows and automations execute with a defined status (success/error). Validate View Queries : Ensures that queries return the expected data and allows extraction of data for use in later actions. Define Variables : Sets variables for use in other actions within the scenario. GraphQL Queries and Mutations # All modeled commands are accessible as mutations, and all modeled queries and projections are accessible for testing purposes. This ensures comprehensive coverage of the application's functionality. The test runner has access to Track & Trace data, meaning that you can instruct the test-runner to validate if a defined set of behavior-flows and automations are executed, and are finished in an success or error state. Handling Authentication and Authorization # For testing purposes, the tool creates test users in the staging environment. Users can insert actions to grant specific roles to the current test user or refresh the user's API token to ensure proper authorization during the test flow. Results and Outputs # Tests provide a pass or fail result based on: Unexpected Behavior : Deviation from the expected behavior. Data Results : Mismatch in expected data results. Coverage : Low event or view coverage, indicating insufficient test coverage. A JSON report is generated and stored in the database. If a test fails, the application will not be deployed to production. Validating Responses # Engineers can model response validations to ensure the application's responses meet the expected criteria. This includes checking the presence and values of specific fields in the response data. Managing Test Data # Before tests are executed, the database is cleared to ensure tests run in a consistent and repeatable manner, avoiding interference from residual data.","title":"Quality Gate"},{"location":"04_Pipeline/QualityGate/#quality-gate","text":"The testing concept in the modeling tool ensures that the application acts as expected by validating the modeled behavior, data access, and authorizations before deploying to production. This section covers how to define, structure, and execute functional scenarios using the provided DSL.","title":"Quality Gate"},{"location":"04_Pipeline/QualityGate/#purpose","text":"The primary purpose of the test concept is to ensure application functionality, data integrity, and proper authorization by running tests against the GraphQL API of a deployed version of the application. These tests help verify that the application's behavior aligns with the modeled specifications.","title":"Purpose"},{"location":"04_Pipeline/QualityGate/#functional-scenario-definition","text":"Functional scenarios are defined and describe a sequence of actions and validations to be executed against the GraphQL API. Each scenario consists of various activities that can include mutations, queries, and validation steps.","title":"Functional Scenario Definition"},{"location":"04_Pipeline/QualityGate/#available-actions","text":"The model supports the following actions: Grant Role to Test User : Assigns a specific role to the current test user. Execute Mutation : Performs a mutation (command) to change the state of the application. Validate Behavior and Automations : Checks that expected behavior flows and automations execute with a defined status (success/error). Validate View Queries : Ensures that queries return the expected data and allows extraction of data for use in later actions. Define Variables : Sets variables for use in other actions within the scenario.","title":"Available Actions"},{"location":"04_Pipeline/QualityGate/#graphql-queries-and-mutations","text":"All modeled commands are accessible as mutations, and all modeled queries and projections are accessible for testing purposes. This ensures comprehensive coverage of the application's functionality. The test runner has access to Track & Trace data, meaning that you can instruct the test-runner to validate if a defined set of behavior-flows and automations are executed, and are finished in an success or error state.","title":"GraphQL Queries and Mutations"},{"location":"04_Pipeline/QualityGate/#handling-authentication-and-authorization","text":"For testing purposes, the tool creates test users in the staging environment. Users can insert actions to grant specific roles to the current test user or refresh the user's API token to ensure proper authorization during the test flow.","title":"Handling Authentication and Authorization"},{"location":"04_Pipeline/QualityGate/#results-and-outputs","text":"Tests provide a pass or fail result based on: Unexpected Behavior : Deviation from the expected behavior. Data Results : Mismatch in expected data results. Coverage : Low event or view coverage, indicating insufficient test coverage. A JSON report is generated and stored in the database. If a test fails, the application will not be deployed to production.","title":"Results and Outputs"},{"location":"04_Pipeline/QualityGate/#validating-responses","text":"Engineers can model response validations to ensure the application's responses meet the expected criteria. This includes checking the presence and values of specific fields in the response data.","title":"Validating Responses"},{"location":"04_Pipeline/QualityGate/#managing-test-data","text":"Before tests are executed, the database is cleared to ensure tests run in a consistent and repeatable manner, avoiding interference from residual data.","title":"Managing Test Data"},{"location":"05_Technical_Concepts/01_Technical_Architecture/","text":"Technical Architecture # This diagram outlines the following components and interactions and visualizes the technical architecture to where the model is converted to: Secrets Manager : Manages sensitive information and can be accessed by the Notifier Lambda function. Cognito (IAM): Manages user authentication and access permissions. AppSync (API Gateway): Handles GraphQL API requests and invokes other Lambda functions. EventBridge : Acts as the event bus, orchestrating events between various Lambda functions. Lambda Functions : Notifier : Also known as Automations, they may interact with AWS resources, the AppSync API or other API's. T&T (Track & Trace) : Processes and stores the trace messages. Projection Engine : Handles data projections. View Updater : Updates the View Store with the latest data. Domain Behavior : Executes the business logic. Triggers : Activate Lambda functions based on specific ActorEvents or DomainEvents. DynamoDB Tables : Trace Table : Holds trace events. View Store : Contains data for display. IdempotencyStore : Ensures idempotent actions. EventStore : Stores all events. Transactional Outbox : Manages transactional messages that need to be published to EventBridge. Mutation Workflow # AppSync receives GraphQL requests and invokes publishes an ActorEvent on the EventBridge. T&T Lambda registers the request in track and trace. Domain Behavior Lambda IdempotencyStore is checked to ensure the request is not already processed. In case it is a duplicate request, a T&T success/already procesed event is published. EventStore : The state is rehydrated from the event store. business logic is executed The IdempotencyStore , EventStore & Transactional Outbox are updated. T&T Lambda : registers the behavior processing state in track and trace. EventFanout : The DomainEvent(s) are published on the EventBridge . Notifier : An automation may trigger on one of the domain events and take action accordingly. For example: querying the App via AppSync, construct a command, and trigger a new mutation via AppSync (using the same trace id). View Updater Lambda reads snapshots from the Transactional Outbox and updates the View Store . Query Workflow # AppSync receives GraphQL requests and resolves all statements to DynamoDB queries using VTL scripting. This may invoke numerous DynamoDB queries that will be aggregated into 1 response. This results in fast response times. Summary # This architecture is designed to be serverless, scalable, and cost-effective, leveraging AWS managed services to minimize operational overhead and focus on application logic and performance.","title":"Technical Architecture"},{"location":"05_Technical_Concepts/01_Technical_Architecture/#technical-architecture","text":"This diagram outlines the following components and interactions and visualizes the technical architecture to where the model is converted to: Secrets Manager : Manages sensitive information and can be accessed by the Notifier Lambda function. Cognito (IAM): Manages user authentication and access permissions. AppSync (API Gateway): Handles GraphQL API requests and invokes other Lambda functions. EventBridge : Acts as the event bus, orchestrating events between various Lambda functions. Lambda Functions : Notifier : Also known as Automations, they may interact with AWS resources, the AppSync API or other API's. T&T (Track & Trace) : Processes and stores the trace messages. Projection Engine : Handles data projections. View Updater : Updates the View Store with the latest data. Domain Behavior : Executes the business logic. Triggers : Activate Lambda functions based on specific ActorEvents or DomainEvents. DynamoDB Tables : Trace Table : Holds trace events. View Store : Contains data for display. IdempotencyStore : Ensures idempotent actions. EventStore : Stores all events. Transactional Outbox : Manages transactional messages that need to be published to EventBridge.","title":"Technical Architecture"},{"location":"05_Technical_Concepts/01_Technical_Architecture/#mutation-workflow","text":"AppSync receives GraphQL requests and invokes publishes an ActorEvent on the EventBridge. T&T Lambda registers the request in track and trace. Domain Behavior Lambda IdempotencyStore is checked to ensure the request is not already processed. In case it is a duplicate request, a T&T success/already procesed event is published. EventStore : The state is rehydrated from the event store. business logic is executed The IdempotencyStore , EventStore & Transactional Outbox are updated. T&T Lambda : registers the behavior processing state in track and trace. EventFanout : The DomainEvent(s) are published on the EventBridge . Notifier : An automation may trigger on one of the domain events and take action accordingly. For example: querying the App via AppSync, construct a command, and trigger a new mutation via AppSync (using the same trace id). View Updater Lambda reads snapshots from the Transactional Outbox and updates the View Store .","title":"Mutation Workflow"},{"location":"05_Technical_Concepts/01_Technical_Architecture/#query-workflow","text":"AppSync receives GraphQL requests and resolves all statements to DynamoDB queries using VTL scripting. This may invoke numerous DynamoDB queries that will be aggregated into 1 response. This results in fast response times.","title":"Query Workflow"},{"location":"05_Technical_Concepts/01_Technical_Architecture/#summary","text":"This architecture is designed to be serverless, scalable, and cost-effective, leveraging AWS managed services to minimize operational overhead and focus on application logic and performance.","title":"Summary"},{"location":"05_Technical_Concepts/02_AWS_services/","text":"AWS services # DynamoDB # Service Description : Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It supports both document and key-value data models, making it suitable for a wide range of applications. Selection Rationale : We chose DynamoDB for its seamless scalability and managed nature, which allows us to focus on application development rather than database management. Its integration with other AWS services and ability to handle large-scale workloads with low latency were key factors in our decision. Optimization Goals : AWS Native & Serverless : DynamoDB is a fully managed service provided by AWS, eliminating the need for server management. Minimizing Management and Cost : It offers pay-as-you-go pricing and automate tasks such as hardware provisioning, setup, and configuration, reducing operational overhead and potential costs. Documentation : AWS DynamoDB Documentation Service Limits : DynamoDB Service Limits Amazon Cognito # Service Description : Amazon Cognito provides authentication, authorization, and user management for web and mobile applications. It supports social identity providers, such as Facebook, Google, and Amazon, as well as enterprise identity providers via SAML 2.0. Selection Rationale : We selected Amazon Cognito for its ease of integration with other AWS services and its comprehensive identity management features. It simplifies user authentication and access control while providing a secure and scalable solution. Optimization Goals : AWS Native & Serverless : Amazon Cognito is a fully managed service by AWS, reducing operational complexity and server management. Minimizing Management and Cost : It offers pay-as-you-go pricing and automates user management tasks, helping to lower operational costs. Documentation : Amazon Cognito Documentation Service Limits : Amazon Cognito Limits AWS Secrets Manager # Service Description : AWS Secrets Manager helps you protect access to your applications, services, and IT resources without the upfront cost and complexity of managing your own infrastructure. It enables you to rotate, manage, and retrieve secrets throughout their lifecycle. Selection Rationale : We opted for AWS Secrets Manager to centrally manage and secure access to sensitive information, such as database credentials and API keys. Its integration with AWS services were pivotal in our decision. Optimization Goals : AWS Native & Serverless : AWS Secrets Manager is a fully managed service, eliminating the need for infrastructure management. Documentation : AWS Secrets Manager Documentation Service Limits : AWS Secrets Manager Limits Amazon EventBridge # Service Description : Amazon EventBridge is a serverless event bus service that makes it easy to connect applications using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. Selection Rationale : We selected Amazon EventBridge for its seamless integration capabilities across AWS services. It simplifies event-driven architectures by decoupling event producers from consumers and supporting event filtering and transformation. Especially the content based routing is pivotal in our architecture. Optimization Goals : AWS Native & Serverless : Amazon EventBridge is serverless, handling all infrastructure provisioning and scaling automatically. Minimizing Management and Cost : It reduces operational overhead by managing event routing and processing, optimizing costs through pay-as-you-go pricing. Documentation : Amazon EventBridge Documentation Service Limits : Amazon EventBridge Limits AWS Lambda # Service Description : AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It automatically scales your application by running code in response to triggers and manages resources on your behalf. Selection Rationale : We chose AWS Lambda for its serverless architecture, which eliminates the need for server management and optimizes resource utilization based on workload demands. Its seamless integration with other AWS services facilitates event-driven computing models. Optimization Goals : AWS Native & Serverless : AWS Lambda is fully managed by AWS, reducing operational complexity and administrative overhead. Minimizing Management and Cost : It optimizes cost by charging only for compute time consumed and automatically scaling resources as needed. Documentation : AWS Lambda Documentation Service Limits : AWS Lambda Limits AWS AppSync # Service Description : AWS AppSync simplifies application development by letting you create scalable APIs that securely access data from multiple sources. It supports real-time and offline capabilities, making it suitable for mobile and web applications. Selection Rationale : We selected AWS AppSync for its managed GraphQL service, which reduces the complexity of API development and data synchronization. Its ability to handle real-time updates aligns with our application\u2019s requirements. Optimization Goals : AWS Native & Serverless : AWS AppSync is fully managed by AWS, minimizing operational overhead and infrastructure management. Minimizing Management and Cost : It optimizes cost by providing built-in scaling and pay-as-you-go pricing, ensuring efficient resource utilization. Documentation : AWS AppSync Documentation Service Limits : AWS AppSync Limits","title":"AWS services"},{"location":"05_Technical_Concepts/02_AWS_services/#aws-services","text":"","title":"AWS services"},{"location":"05_Technical_Concepts/02_AWS_services/#dynamodb","text":"Service Description : Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It supports both document and key-value data models, making it suitable for a wide range of applications. Selection Rationale : We chose DynamoDB for its seamless scalability and managed nature, which allows us to focus on application development rather than database management. Its integration with other AWS services and ability to handle large-scale workloads with low latency were key factors in our decision. Optimization Goals : AWS Native & Serverless : DynamoDB is a fully managed service provided by AWS, eliminating the need for server management. Minimizing Management and Cost : It offers pay-as-you-go pricing and automate tasks such as hardware provisioning, setup, and configuration, reducing operational overhead and potential costs. Documentation : AWS DynamoDB Documentation Service Limits : DynamoDB Service Limits","title":"DynamoDB"},{"location":"05_Technical_Concepts/02_AWS_services/#amazon-cognito","text":"Service Description : Amazon Cognito provides authentication, authorization, and user management for web and mobile applications. It supports social identity providers, such as Facebook, Google, and Amazon, as well as enterprise identity providers via SAML 2.0. Selection Rationale : We selected Amazon Cognito for its ease of integration with other AWS services and its comprehensive identity management features. It simplifies user authentication and access control while providing a secure and scalable solution. Optimization Goals : AWS Native & Serverless : Amazon Cognito is a fully managed service by AWS, reducing operational complexity and server management. Minimizing Management and Cost : It offers pay-as-you-go pricing and automates user management tasks, helping to lower operational costs. Documentation : Amazon Cognito Documentation Service Limits : Amazon Cognito Limits","title":"Amazon Cognito"},{"location":"05_Technical_Concepts/02_AWS_services/#aws-secrets-manager","text":"Service Description : AWS Secrets Manager helps you protect access to your applications, services, and IT resources without the upfront cost and complexity of managing your own infrastructure. It enables you to rotate, manage, and retrieve secrets throughout their lifecycle. Selection Rationale : We opted for AWS Secrets Manager to centrally manage and secure access to sensitive information, such as database credentials and API keys. Its integration with AWS services were pivotal in our decision. Optimization Goals : AWS Native & Serverless : AWS Secrets Manager is a fully managed service, eliminating the need for infrastructure management. Documentation : AWS Secrets Manager Documentation Service Limits : AWS Secrets Manager Limits","title":"AWS Secrets Manager"},{"location":"05_Technical_Concepts/02_AWS_services/#amazon-eventbridge","text":"Service Description : Amazon EventBridge is a serverless event bus service that makes it easy to connect applications using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. Selection Rationale : We selected Amazon EventBridge for its seamless integration capabilities across AWS services. It simplifies event-driven architectures by decoupling event producers from consumers and supporting event filtering and transformation. Especially the content based routing is pivotal in our architecture. Optimization Goals : AWS Native & Serverless : Amazon EventBridge is serverless, handling all infrastructure provisioning and scaling automatically. Minimizing Management and Cost : It reduces operational overhead by managing event routing and processing, optimizing costs through pay-as-you-go pricing. Documentation : Amazon EventBridge Documentation Service Limits : Amazon EventBridge Limits","title":"Amazon EventBridge"},{"location":"05_Technical_Concepts/02_AWS_services/#aws-lambda","text":"Service Description : AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It automatically scales your application by running code in response to triggers and manages resources on your behalf. Selection Rationale : We chose AWS Lambda for its serverless architecture, which eliminates the need for server management and optimizes resource utilization based on workload demands. Its seamless integration with other AWS services facilitates event-driven computing models. Optimization Goals : AWS Native & Serverless : AWS Lambda is fully managed by AWS, reducing operational complexity and administrative overhead. Minimizing Management and Cost : It optimizes cost by charging only for compute time consumed and automatically scaling resources as needed. Documentation : AWS Lambda Documentation Service Limits : AWS Lambda Limits","title":"AWS Lambda"},{"location":"05_Technical_Concepts/02_AWS_services/#aws-appsync","text":"Service Description : AWS AppSync simplifies application development by letting you create scalable APIs that securely access data from multiple sources. It supports real-time and offline capabilities, making it suitable for mobile and web applications. Selection Rationale : We selected AWS AppSync for its managed GraphQL service, which reduces the complexity of API development and data synchronization. Its ability to handle real-time updates aligns with our application\u2019s requirements. Optimization Goals : AWS Native & Serverless : AWS AppSync is fully managed by AWS, minimizing operational overhead and infrastructure management. Minimizing Management and Cost : It optimizes cost by providing built-in scaling and pay-as-you-go pricing, ensuring efficient resource utilization. Documentation : AWS AppSync Documentation Service Limits : AWS AppSync Limits","title":"AWS AppSync"},{"location":"05_Technical_Concepts/03_asynchronisity/","text":"Asynchronous Command Handling # Overview # Our application utilizes an asynchronous command handling system through AWS AppSync. When a client sends a command (via GraphQL mutation), it is accepted and dispatched asynchronously, providing a trace ID for monitoring purposes. This approach ensures robust and responsive user interactions, accommodating the inherent challenges of distributed systems. Process Flow # Command Dispatch : When a command is issued, it is accepted into the domain and dispatched asynchronously. AWS AppSync abstracts this process, handling the details of asynchronous communication. Trace ID : Upon accepting a command, the system generates and returns a trace ID. This ID is crucial for clients to monitor the status of their commands. Monitoring with Trace ID # Clients can use the trace ID to monitor the status of their commands through GraphQL queries or subscriptions. The information retrievable includes: Component name Timestamp Status (accepted, success, error) Query vs. Subscription # Query : Used when client technology does not support subscriptions, or when viewing information after the command has been published. Queries are useful for retrospective checks and scenarios requiring fast processing. Subscription : Ideal for real-time monitoring, providing immediate updates on the command\u2019s status. Error Handling # Retries : For technical errors (e.g., network issues), the system retries the command up to three times. Error Events : If retries fail, an error event is thrown, and the context is stored in the deadletter table for further investigation and handling. Benefits of Asynchronous Handling # Performance : Fast initial responses to clients improve user experience. Robustness : Handles the fallacies of distributed systems, ensuring reliable operation despite potential issues. Real-time Updates : Clients receive immediate feedback on the status of their commands, aiding in timely decision-making and interactions. This asynchronous approach leverages the strengths of AWS AppSync to provide a responsive, robust, and user-friendly system for handling commands and ensuring that clients are well-informed about the status and outcomes of their requests.","title":"Asynchronous Command Handling"},{"location":"05_Technical_Concepts/03_asynchronisity/#asynchronous-command-handling","text":"","title":"Asynchronous Command Handling"},{"location":"05_Technical_Concepts/03_asynchronisity/#overview","text":"Our application utilizes an asynchronous command handling system through AWS AppSync. When a client sends a command (via GraphQL mutation), it is accepted and dispatched asynchronously, providing a trace ID for monitoring purposes. This approach ensures robust and responsive user interactions, accommodating the inherent challenges of distributed systems.","title":"Overview"},{"location":"05_Technical_Concepts/03_asynchronisity/#process-flow","text":"Command Dispatch : When a command is issued, it is accepted into the domain and dispatched asynchronously. AWS AppSync abstracts this process, handling the details of asynchronous communication. Trace ID : Upon accepting a command, the system generates and returns a trace ID. This ID is crucial for clients to monitor the status of their commands.","title":"Process Flow"},{"location":"05_Technical_Concepts/03_asynchronisity/#monitoring-with-trace-id","text":"Clients can use the trace ID to monitor the status of their commands through GraphQL queries or subscriptions. The information retrievable includes: Component name Timestamp Status (accepted, success, error)","title":"Monitoring with Trace ID"},{"location":"05_Technical_Concepts/03_asynchronisity/#query-vs-subscription","text":"Query : Used when client technology does not support subscriptions, or when viewing information after the command has been published. Queries are useful for retrospective checks and scenarios requiring fast processing. Subscription : Ideal for real-time monitoring, providing immediate updates on the command\u2019s status.","title":"Query vs. Subscription"},{"location":"05_Technical_Concepts/03_asynchronisity/#error-handling","text":"Retries : For technical errors (e.g., network issues), the system retries the command up to three times. Error Events : If retries fail, an error event is thrown, and the context is stored in the deadletter table for further investigation and handling.","title":"Error Handling"},{"location":"05_Technical_Concepts/03_asynchronisity/#benefits-of-asynchronous-handling","text":"Performance : Fast initial responses to clients improve user experience. Robustness : Handles the fallacies of distributed systems, ensuring reliable operation despite potential issues. Real-time Updates : Clients receive immediate feedback on the status of their commands, aiding in timely decision-making and interactions. This asynchronous approach leverages the strengths of AWS AppSync to provide a responsive, robust, and user-friendly system for handling commands and ensuring that clients are well-informed about the status and outcomes of their requests.","title":"Benefits of Asynchronous Handling"},{"location":"05_Technical_Concepts/04_keychain/","text":"Understanding Keys and the Keychain Concept # In Draftsman-generated applications, the key concepts are crucial for efficient data management and access. The important thing to know, is that during modelling you only have to think about the meanigfull business-keys. We automate the management of the technical keys for you in our architecture. Here's a detailed explanation of the concepts behind key management: Business Key # The business key is a meaningful identifier used in the view-store of the application. This key has semantic significance, allowing clients to request data based on recognizable identities. For instance, in a system tracking projects, the business key might encode project identifiers and related hierarchies, making it intuitive for users to query specific projects. This key helps structure the data in a way that aligns with real-world relationships, ensuring efficient querying and filtering in the read model. Technical Key # The technical key, unlike the business key, is an internal identifier used primarily in the write model and event stores. It is typically a UUID (Universally Unique Identifier), which provides a unique, immutable reference for entities regardless of their semantic meaning. This key is essential for ensuring the integrity and consistency of the event log, as it remains unchanged even when the business context (and thus the business key) evolves. The technical key supports robust state determination by replaying events, essential for behavior-driven data models. Keychain # The keychain is a mechanism introduced to map business keys to technical keys seamlessly. It abstracts the complexity of translating user-friendly, meaningful identifiers into system-specific, immutable ones. This translation is critical because while business keys might change (e.g., due to renaming or restructuring), the underlying technical keys must remain consistent to maintain data integrity. The keychain ensures that users can interact with the system using business keys while the system internally manages and references the stable technical keys. The Need for a Keychain # A keychain is necessary for several reasons: Data Integrity and Consistency : By separating business and technical keys, the system maintains consistent references in the event log, even when business keys change. User-Friendly Interactions : Users can work with meaningful identifiers (business keys) without worrying about internal key management complexities. Flexibility and Evolution : As business requirements change, entities can be renamed or restructured without affecting the underlying system's stability. Efficient Data Access and Management : The keychain enables efficient querying by leveraging meaningful keys for data access while ensuring the robustness of technical keys for internal operations. In summary, the keychain concept in Draftsman applications bridges the gap between user-friendly, meaningful identifiers and the system's need for consistent, immutable references, ensuring both usability and reliability. For more details, refer to the original blog post on Draftsman.io .","title":"Understanding Keys and the Keychain Concept"},{"location":"05_Technical_Concepts/04_keychain/#understanding-keys-and-the-keychain-concept","text":"In Draftsman-generated applications, the key concepts are crucial for efficient data management and access. The important thing to know, is that during modelling you only have to think about the meanigfull business-keys. We automate the management of the technical keys for you in our architecture. Here's a detailed explanation of the concepts behind key management:","title":"Understanding Keys and the Keychain Concept"},{"location":"05_Technical_Concepts/04_keychain/#business-key","text":"The business key is a meaningful identifier used in the view-store of the application. This key has semantic significance, allowing clients to request data based on recognizable identities. For instance, in a system tracking projects, the business key might encode project identifiers and related hierarchies, making it intuitive for users to query specific projects. This key helps structure the data in a way that aligns with real-world relationships, ensuring efficient querying and filtering in the read model.","title":"Business Key"},{"location":"05_Technical_Concepts/04_keychain/#technical-key","text":"The technical key, unlike the business key, is an internal identifier used primarily in the write model and event stores. It is typically a UUID (Universally Unique Identifier), which provides a unique, immutable reference for entities regardless of their semantic meaning. This key is essential for ensuring the integrity and consistency of the event log, as it remains unchanged even when the business context (and thus the business key) evolves. The technical key supports robust state determination by replaying events, essential for behavior-driven data models.","title":"Technical Key"},{"location":"05_Technical_Concepts/04_keychain/#keychain","text":"The keychain is a mechanism introduced to map business keys to technical keys seamlessly. It abstracts the complexity of translating user-friendly, meaningful identifiers into system-specific, immutable ones. This translation is critical because while business keys might change (e.g., due to renaming or restructuring), the underlying technical keys must remain consistent to maintain data integrity. The keychain ensures that users can interact with the system using business keys while the system internally manages and references the stable technical keys.","title":"Keychain"},{"location":"05_Technical_Concepts/04_keychain/#the-need-for-a-keychain","text":"A keychain is necessary for several reasons: Data Integrity and Consistency : By separating business and technical keys, the system maintains consistent references in the event log, even when business keys change. User-Friendly Interactions : Users can work with meaningful identifiers (business keys) without worrying about internal key management complexities. Flexibility and Evolution : As business requirements change, entities can be renamed or restructured without affecting the underlying system's stability. Efficient Data Access and Management : The keychain enables efficient querying by leveraging meaningful keys for data access while ensuring the robustness of technical keys for internal operations. In summary, the keychain concept in Draftsman applications bridges the gap between user-friendly, meaningful identifiers and the system's need for consistent, immutable references, ensuring both usability and reliability. For more details, refer to the original blog post on Draftsman.io .","title":"The Need for a Keychain"},{"location":"05_Technical_Concepts/05_workflow_and_ownership/","text":"Workflow and Ownership # The provided diagram illustrates the workflow and components of the Tracepaper system, detailing the integration between modeling, code generation, and deployment processes, along with the ownership of each part of the infrastructure. Workflow Description # Tracepaper: Web Modeler Users model their applications in the Tracepaper Web Modeler, which is used for project management and to set build triggers. Ownership: Tracepaper Tracepaper Backend Manages project configurations and triggers builds based on the models created in the Web Modeler. Ownership: Tracepaper Draftsman Build Agent The build agent pulls the model from the repository, converts it to Python and CloudFormation code, and executes unit tests. This generated code is then pushed to respective repositories. Ownership: Tracepaper Model Repo Contains the domain-specific language (XML) and any custom code. This repository is synced with the Tracepaper backend and the Draftsman Build Agent. Ownership: Client Backend Repo Stores the generated domain logic in Python and the infrastructure-as-code scripts (CloudFormation). These artifacts are prepared for deployment. Ownership: Client GUI Repo Holds the GUI assist framework and custom code, ensuring the user interface aligns with the backend logic. Ownership: Client AWS Pipeline This pipeline handles the entire deployment process: Package: Prepare deployment - Packages the application artifacts. Staging: Deploy Staging - Deploys the application to a staging environment. Quality Gate: Testing - Runs API tests against the staging environment to ensure the application is functioning correctly. Production: Deploy Production - Finally, deploys the application to the production environment if all tests pass. Ownership: Client This architecture ensures a seamless and robust development-to-deployment workflow, emphasizing testing, error handling, and real-time client feedback.","title":"Workflow and Ownership"},{"location":"05_Technical_Concepts/05_workflow_and_ownership/#workflow-and-ownership","text":"The provided diagram illustrates the workflow and components of the Tracepaper system, detailing the integration between modeling, code generation, and deployment processes, along with the ownership of each part of the infrastructure.","title":"Workflow and Ownership"},{"location":"05_Technical_Concepts/05_workflow_and_ownership/#workflow-description","text":"Tracepaper: Web Modeler Users model their applications in the Tracepaper Web Modeler, which is used for project management and to set build triggers. Ownership: Tracepaper Tracepaper Backend Manages project configurations and triggers builds based on the models created in the Web Modeler. Ownership: Tracepaper Draftsman Build Agent The build agent pulls the model from the repository, converts it to Python and CloudFormation code, and executes unit tests. This generated code is then pushed to respective repositories. Ownership: Tracepaper Model Repo Contains the domain-specific language (XML) and any custom code. This repository is synced with the Tracepaper backend and the Draftsman Build Agent. Ownership: Client Backend Repo Stores the generated domain logic in Python and the infrastructure-as-code scripts (CloudFormation). These artifacts are prepared for deployment. Ownership: Client GUI Repo Holds the GUI assist framework and custom code, ensuring the user interface aligns with the backend logic. Ownership: Client AWS Pipeline This pipeline handles the entire deployment process: Package: Prepare deployment - Packages the application artifacts. Staging: Deploy Staging - Deploys the application to a staging environment. Quality Gate: Testing - Runs API tests against the staging environment to ensure the application is functioning correctly. Production: Deploy Production - Finally, deploys the application to the production environment if all tests pass. Ownership: Client This architecture ensures a seamless and robust development-to-deployment workflow, emphasizing testing, error handling, and real-time client feedback.","title":"Workflow Description"},{"location":"06_Why/01_python/","text":"Why Python # Our goal is to make system design accessible to domain experts, and Python is central to achieving this. Here\u2019s why we chose Python, especially in the context of our AWS-based infrastructure: Benefits of Python # Readability and Simplicity : Python\u2019s clean and easy-to-understand syntax is perfect for domain experts with limited programming experience. Emphasis on readability reduces the learning curve and helps in quickly grasping complex business-logic. Rich Ecosystem and Libraries : Python has a vast ecosystem of libraries . These libraries expedite development and provide powerful tools for solving domain-specific problems. Integration Capabilities : Python integrates seamlessly with AWS with the aid of the boto 3 library . Community Support : A large and active community provides extensive documentation, tutorials, and support. Community-driven development keeps Python up-to-date with the latest technological advancements and best practices. AWS Lambda : Python\u2019s lightweight and efficient nature is ideal for serverless functions, enabling scalable and cost-effective execution. Python and Domain-Driven Design (DDD) # Python\u2019s simplicity and readability align well with Domain-Driven Design (DDD) principles: Expressive Code : Python\u2019s syntax mirrors domain-specific language, facilitating understanding and contribution from domain experts. Rapid Prototyping : Python\u2019s dynamic nature and extensive library support enable quick prototyping, allowing domain experts to see and refine system designs swiftly. Conclusion # Choosing Python helps us bridge the gap between complex system design and domain expertise. Its readability, versatility, rich ecosystem, integration capabilities, and strong community support make it the ideal language for empowering domain experts to contribute effectively to system design, especially within our AWS-based infrastructure. For more detailed insights into why we love Python, please refer to this document .","title":"Why Python"},{"location":"06_Why/01_python/#why-python","text":"Our goal is to make system design accessible to domain experts, and Python is central to achieving this. Here\u2019s why we chose Python, especially in the context of our AWS-based infrastructure:","title":"Why Python"},{"location":"06_Why/01_python/#benefits-of-python","text":"Readability and Simplicity : Python\u2019s clean and easy-to-understand syntax is perfect for domain experts with limited programming experience. Emphasis on readability reduces the learning curve and helps in quickly grasping complex business-logic. Rich Ecosystem and Libraries : Python has a vast ecosystem of libraries . These libraries expedite development and provide powerful tools for solving domain-specific problems. Integration Capabilities : Python integrates seamlessly with AWS with the aid of the boto 3 library . Community Support : A large and active community provides extensive documentation, tutorials, and support. Community-driven development keeps Python up-to-date with the latest technological advancements and best practices. AWS Lambda : Python\u2019s lightweight and efficient nature is ideal for serverless functions, enabling scalable and cost-effective execution.","title":"Benefits of Python"},{"location":"06_Why/01_python/#python-and-domain-driven-design-ddd","text":"Python\u2019s simplicity and readability align well with Domain-Driven Design (DDD) principles: Expressive Code : Python\u2019s syntax mirrors domain-specific language, facilitating understanding and contribution from domain experts. Rapid Prototyping : Python\u2019s dynamic nature and extensive library support enable quick prototyping, allowing domain experts to see and refine system designs swiftly.","title":"Python and Domain-Driven Design (DDD)"},{"location":"06_Why/01_python/#conclusion","text":"Choosing Python helps us bridge the gap between complex system design and domain expertise. Its readability, versatility, rich ecosystem, integration capabilities, and strong community support make it the ideal language for empowering domain experts to contribute effectively to system design, especially within our AWS-based infrastructure. For more detailed insights into why we love Python, please refer to this document .","title":"Conclusion"},{"location":"06_Why/02_cqrs/","text":"Why CQRS # What is CQRS? # Command Query Responsibility Segregation (CQRS) is a design pattern that separates read (query) and write (command) operations into distinct models. This segregation allows for optimized and scalable handling of commands and queries. Key Principles of CQRS: Commands : Modify the state of the application. Each command represents an action or a change. Queries : Retrieve information without modifying the state. Queries are optimized for read operations. Benefits of CQRS # Scalability : Separating reads and writes allows each to be independently scaled, which is particularly useful in high-load scenarios. For example, read operations can be scaled out with replicas, while write operations can be handled with dedicated resources. Performance Optimization : Queries can be optimized for fast read performance, while commands can be optimized for write efficiency. This leads to better overall system performance. Flexibility and Extensibility : CQRS allows for the use of different data models for reading and writing. This means you can use a highly normalized model for writes and a denormalized model for reads, optimizing each for their specific use cases. Simplified Complexity : By separating commands and queries, each part of the system becomes simpler and more focused. This separation reduces the cognitive load on developers, making the system easier to understand and maintain. Enhanced Security : The separation allows for more granular security controls. Commands can have strict validation and authentication, while queries can be more permissive. Why CQRS Fits Our Architecture # We draw inspiration from Domain-Driven Design (DDD) and event storming in our modeling tool. Adopting CQRS fits well with those concepts and aiding the domain narative. Additionally, we can leverage the benefits of CQRS and the good fit with event-driven architectures. Conclusion # Choosing CQRS allows us to build a robust, scalable, and maintainable system that leverages the strengths of both Event Sourcing and modern cloud infrastructure. This decision aligns with our goal of simplifying complex system design for domain experts, enabling them to focus on delivering business value.","title":"Why CQRS"},{"location":"06_Why/02_cqrs/#why-cqrs","text":"","title":"Why CQRS"},{"location":"06_Why/02_cqrs/#what-is-cqrs","text":"Command Query Responsibility Segregation (CQRS) is a design pattern that separates read (query) and write (command) operations into distinct models. This segregation allows for optimized and scalable handling of commands and queries. Key Principles of CQRS: Commands : Modify the state of the application. Each command represents an action or a change. Queries : Retrieve information without modifying the state. Queries are optimized for read operations.","title":"What is CQRS?"},{"location":"06_Why/02_cqrs/#benefits-of-cqrs","text":"Scalability : Separating reads and writes allows each to be independently scaled, which is particularly useful in high-load scenarios. For example, read operations can be scaled out with replicas, while write operations can be handled with dedicated resources. Performance Optimization : Queries can be optimized for fast read performance, while commands can be optimized for write efficiency. This leads to better overall system performance. Flexibility and Extensibility : CQRS allows for the use of different data models for reading and writing. This means you can use a highly normalized model for writes and a denormalized model for reads, optimizing each for their specific use cases. Simplified Complexity : By separating commands and queries, each part of the system becomes simpler and more focused. This separation reduces the cognitive load on developers, making the system easier to understand and maintain. Enhanced Security : The separation allows for more granular security controls. Commands can have strict validation and authentication, while queries can be more permissive.","title":"Benefits of CQRS"},{"location":"06_Why/02_cqrs/#why-cqrs-fits-our-architecture","text":"We draw inspiration from Domain-Driven Design (DDD) and event storming in our modeling tool. Adopting CQRS fits well with those concepts and aiding the domain narative. Additionally, we can leverage the benefits of CQRS and the good fit with event-driven architectures.","title":"Why CQRS Fits Our Architecture"},{"location":"06_Why/02_cqrs/#conclusion","text":"Choosing CQRS allows us to build a robust, scalable, and maintainable system that leverages the strengths of both Event Sourcing and modern cloud infrastructure. This decision aligns with our goal of simplifying complex system design for domain experts, enabling them to focus on delivering business value.","title":"Conclusion"},{"location":"06_Why/03_event_sourcing/","text":"Why Event Sourcing # Event Sourcing is a powerful architectural pattern that has become an integral part of our system design. Here\u2019s why we chose Event Sourcing: Benefits of Event Sourcing # Historical Accuracy and Auditing : Complete History : Every change to the application state is stored as an event, preserving a complete history of actions. Audit Trail : This provides an accurate and immutable audit trail, crucial for compliance and debugging. Improved Consistency and Reliability : Consistency : Event Sourcing ensures that state changes are consistently recorded and applied in the order they occur. Reliability : By replaying events, systems can recover from failures and ensure reliable state reconstruction. Enhanced Scalability and Performance : Scalability : Events can be processed asynchronously and in parallel, enhancing system scalability. Performance : Read models can be optimized for query performance without affecting the write models, thanks to the separation of concerns. Flexibility and Extensibility : Flexibility : Event Sourcing supports evolving requirements by allowing new views or projections to be created from the event log. Extensibility : New functionality can be added without changing existing events, enabling seamless system evolution. Compatibility with CQRS : CQRS Alignment : Event Sourcing naturally fits with the Command Query Responsibility Segregation (CQRS) pattern, enhancing our ability to separate read and write concerns. Behavior Modeling : Commands capture intent and behavior, while events capture the results, aligning perfectly with our DDD approach. What is Event Sourcing? # Event Sourcing is an architectural pattern where all changes to an application's state are stored as a sequence of events. Instead of just storing the current state, every state-changing action (event) is saved, providing a complete history. This allows for the reconstruction of any past state by replaying these events. Key aspects of Event Sourcing include: Immutable Events : Each event is immutable and represents a single change in the state. Event Log : A central log where all events are stored in the order they occurred. State Reconstruction : The current state can be rebuilt by replaying the stored events. Example: # Consider a bank account: Event : \"Deposit of $100\" and \"Withdrawal of $50\" are recorded as events. Reconstruction : To get the current balance, the system replays these events starting from an initial balance. initial: $0 replay deposit -> $0 + $100 replay withdrawal -> $100 - $50 balance: $50 Event Sourcing is especially powerful when combined with Command Query Responsibility Segregation (CQRS) and Domain-Driven Design (DDD), providing a robust foundation for complex applications. Conclusion # Event Sourcing provides a robust framework for building reliable, scalable, and maintainable systems. Its alignment with CQRS and DDD principles makes it an ideal choice for our architecture. This pattern ensures that our systems are not only capable of handling current demands but are also well-prepared for future growth and changes.","title":"Why Event Sourcing"},{"location":"06_Why/03_event_sourcing/#why-event-sourcing","text":"Event Sourcing is a powerful architectural pattern that has become an integral part of our system design. Here\u2019s why we chose Event Sourcing:","title":"Why Event Sourcing"},{"location":"06_Why/03_event_sourcing/#benefits-of-event-sourcing","text":"Historical Accuracy and Auditing : Complete History : Every change to the application state is stored as an event, preserving a complete history of actions. Audit Trail : This provides an accurate and immutable audit trail, crucial for compliance and debugging. Improved Consistency and Reliability : Consistency : Event Sourcing ensures that state changes are consistently recorded and applied in the order they occur. Reliability : By replaying events, systems can recover from failures and ensure reliable state reconstruction. Enhanced Scalability and Performance : Scalability : Events can be processed asynchronously and in parallel, enhancing system scalability. Performance : Read models can be optimized for query performance without affecting the write models, thanks to the separation of concerns. Flexibility and Extensibility : Flexibility : Event Sourcing supports evolving requirements by allowing new views or projections to be created from the event log. Extensibility : New functionality can be added without changing existing events, enabling seamless system evolution. Compatibility with CQRS : CQRS Alignment : Event Sourcing naturally fits with the Command Query Responsibility Segregation (CQRS) pattern, enhancing our ability to separate read and write concerns. Behavior Modeling : Commands capture intent and behavior, while events capture the results, aligning perfectly with our DDD approach.","title":"Benefits of Event Sourcing"},{"location":"06_Why/03_event_sourcing/#what-is-event-sourcing","text":"Event Sourcing is an architectural pattern where all changes to an application's state are stored as a sequence of events. Instead of just storing the current state, every state-changing action (event) is saved, providing a complete history. This allows for the reconstruction of any past state by replaying these events. Key aspects of Event Sourcing include: Immutable Events : Each event is immutable and represents a single change in the state. Event Log : A central log where all events are stored in the order they occurred. State Reconstruction : The current state can be rebuilt by replaying the stored events.","title":"What is Event Sourcing?"},{"location":"06_Why/03_event_sourcing/#example","text":"Consider a bank account: Event : \"Deposit of $100\" and \"Withdrawal of $50\" are recorded as events. Reconstruction : To get the current balance, the system replays these events starting from an initial balance. initial: $0 replay deposit -> $0 + $100 replay withdrawal -> $100 - $50 balance: $50 Event Sourcing is especially powerful when combined with Command Query Responsibility Segregation (CQRS) and Domain-Driven Design (DDD), providing a robust foundation for complex applications.","title":"Example:"},{"location":"06_Why/03_event_sourcing/#conclusion","text":"Event Sourcing provides a robust framework for building reliable, scalable, and maintainable systems. Its alignment with CQRS and DDD principles makes it an ideal choice for our architecture. This pattern ensures that our systems are not only capable of handling current demands but are also well-prepared for future growth and changes.","title":"Conclusion"},{"location":"06_Why/04_graphql/","text":"Why GraphQL # Feature/Aspect GraphQL REST Alignment with CQRS Perfect fit with CQRS; commands (mutations) and queries are distinct, reducing cognitive load because no additional level of abstraction is needed. Less aligned; REST has a good fit with CRUD, while we deal with commands (CUD) and queries (R), merging those concepts together increases complexity and cognitive load (adds just another thing that you should think about). Modeling Simplifies modeling by mirroring existing structures; focus on behavior, not data. Requires explicit API modeling, adding a layer of abstraction and complexity. Client Interaction Commands modeled as GraphQL mutations; async processing monitored via subscriptions. CRUD operations on resources; requires inventing virtual resources for modeling, less intuitive. Learning Curve Familiarity with GraphQL required, but benefits in aligning with CQRS and reducing overall complexity. REST is widely known but might not fit well with our implementation of CQRS, requiring additional abstractions. Benefits of Using GraphQL for Modeling: # Conceptual Fit with CQRS : Commands and Mutations : In our modeling tool, commands represent client interactions or requests for changes, aligning perfectly with GraphQL mutations. The API accepts these commands, returning a trace ID that can be monitored via GraphQL subscriptions, allowing clients to track the asynchronous processing of these mutations. Materialized Views and Queries : Our tool uses materialized views and projections to handle data viewing, aligning seamlessly with GraphQL queries. Reduced Cognitive Load : Natural API Structure : With GraphQL, the API mirrors the existing structure of the model. This means developers only need to decide what to expose and configure authentication/authorization methods, significantly reducing the cognitive load. Behavior Over Data : Our approach focuses on behavior rather than just data manipulation. REST, being more resource-centric, would require us to invent virtual resources and simulate CRUD operations, which adds unnecessary complexity. Potential Challenges : Less Common Usage : While GraphQL is powerful, it may be less commonly used compared to REST. However, the benefits in reducing cognitive load and fitting naturally with our CQRS architecture make it a superior choice for our needs. By choosing GraphQL, we ensure that our modeling tool aligns perfectly with our CQRS architecture, providing a more intuitive, efficient, and effective way to design APIs, reducing the complexity and cognitive load on developers. This alignment allows for a more straightforward configuration of commands, queries, and access patterns, leading to a smoother and more productive development experience.","title":"Why GraphQL"},{"location":"06_Why/04_graphql/#why-graphql","text":"Feature/Aspect GraphQL REST Alignment with CQRS Perfect fit with CQRS; commands (mutations) and queries are distinct, reducing cognitive load because no additional level of abstraction is needed. Less aligned; REST has a good fit with CRUD, while we deal with commands (CUD) and queries (R), merging those concepts together increases complexity and cognitive load (adds just another thing that you should think about). Modeling Simplifies modeling by mirroring existing structures; focus on behavior, not data. Requires explicit API modeling, adding a layer of abstraction and complexity. Client Interaction Commands modeled as GraphQL mutations; async processing monitored via subscriptions. CRUD operations on resources; requires inventing virtual resources for modeling, less intuitive. Learning Curve Familiarity with GraphQL required, but benefits in aligning with CQRS and reducing overall complexity. REST is widely known but might not fit well with our implementation of CQRS, requiring additional abstractions.","title":"Why GraphQL"},{"location":"06_Why/04_graphql/#benefits-of-using-graphql-for-modeling","text":"Conceptual Fit with CQRS : Commands and Mutations : In our modeling tool, commands represent client interactions or requests for changes, aligning perfectly with GraphQL mutations. The API accepts these commands, returning a trace ID that can be monitored via GraphQL subscriptions, allowing clients to track the asynchronous processing of these mutations. Materialized Views and Queries : Our tool uses materialized views and projections to handle data viewing, aligning seamlessly with GraphQL queries. Reduced Cognitive Load : Natural API Structure : With GraphQL, the API mirrors the existing structure of the model. This means developers only need to decide what to expose and configure authentication/authorization methods, significantly reducing the cognitive load. Behavior Over Data : Our approach focuses on behavior rather than just data manipulation. REST, being more resource-centric, would require us to invent virtual resources and simulate CRUD operations, which adds unnecessary complexity. Potential Challenges : Less Common Usage : While GraphQL is powerful, it may be less commonly used compared to REST. However, the benefits in reducing cognitive load and fitting naturally with our CQRS architecture make it a superior choice for our needs. By choosing GraphQL, we ensure that our modeling tool aligns perfectly with our CQRS architecture, providing a more intuitive, efficient, and effective way to design APIs, reducing the complexity and cognitive load on developers. This alignment allows for a more straightforward configuration of commands, queries, and access patterns, leading to a smoother and more productive development experience.","title":"Benefits of Using GraphQL for Modeling:"},{"location":"06_Why/05_serverless/","text":"Why Serverless # What is Serverless? # Serverless computing is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers. AWS offers a range of serverless services such as AWS Lambda, AWS AppSync, and Amazon EventBridge, which allow developers to build and run applications without having to manage infrastructure. Benefits of Serverless on AWS # Scalability : Automatic Scaling : AWS serverless services automatically scale to handle the demands of your application. AWS Lambda, for instance, scales in response to incoming traffic, ensuring consistent performance during peak loads without manual intervention. Cost Efficiency : Pay-as-You-Go : With serverless, you only pay for the compute time you consume. AWS charges you based on the number of requests and the duration your code runs, which can significantly reduce costs compared to traditional server-based architectures. Reduced Operational Overhead : No Server Management : AWS handles the infrastructure management, including server maintenance, patching, and scaling. This allows your development team to focus on writing code and delivering features rather than managing servers. Enhanced Agility : Faster Deployment : Serverless architecture enables rapid development and deployment cycles. AWS services such as Lambda and AppSync integrate seamlessly with CI/CD pipelines, allowing for continuous integration and delivery. Event-Driven Design : Integration with AWS Services : AWS serverless services are designed to work well together. For example, you can use Amazon EventBridge to trigger AWS Lambda functions, enabling an event-driven architecture that is highly responsive to changes and real-time data. High Availability and Reliability : Built-in Fault Tolerance : AWS serverless services offer built-in fault tolerance and availability. AWS Lambda functions are replicated across multiple availability zones, ensuring that your application remains resilient and highly available. Why Serverless Fits Our Architecture # Alignment with CQRS and Event Sourcing : Our choice of CQRS and event sourcing patterns fits perfectly with serverless architecture. AWS Lambda functions handle commands (writes) and queries (reads) efficiently, while services like EventBridge facilitate event-driven communication. Seamless API Management : Using AWS AppSync for our GraphQL API management aligns with our need for flexible and scalable API solutions. AppSync simplifies the creation and management of GraphQL APIs, allowing us to focus on delivering business value. Optimized for Domain Experts : Serverless architecture abstracts away infrastructure complexities, allowing domain experts to focus on system design and business logic. This aligns with our goal of enabling domain experts to contribute directly to the system without needing deep technical expertise in infrastructure management. While serverless icreases configuration complexity, this is largly abstracted away by the Tracepaper modeling concepts. Conclusion # Choosing a serverless architecture on AWS provides us with scalability, cost efficiency, reduced operational overhead, and enhanced agility. These benefits align perfectly with our design patterns (CQRS and event sourcing) and our goal of empowering domain experts to focus on delivering business value. AWS's comprehensive suite of serverless services ensures that our applications are robust, scalable, and maintainable, supporting our long-term objectives.","title":"Why Serverless"},{"location":"06_Why/05_serverless/#why-serverless","text":"","title":"Why Serverless"},{"location":"06_Why/05_serverless/#what-is-serverless","text":"Serverless computing is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers. AWS offers a range of serverless services such as AWS Lambda, AWS AppSync, and Amazon EventBridge, which allow developers to build and run applications without having to manage infrastructure.","title":"What is Serverless?"},{"location":"06_Why/05_serverless/#benefits-of-serverless-on-aws","text":"Scalability : Automatic Scaling : AWS serverless services automatically scale to handle the demands of your application. AWS Lambda, for instance, scales in response to incoming traffic, ensuring consistent performance during peak loads without manual intervention. Cost Efficiency : Pay-as-You-Go : With serverless, you only pay for the compute time you consume. AWS charges you based on the number of requests and the duration your code runs, which can significantly reduce costs compared to traditional server-based architectures. Reduced Operational Overhead : No Server Management : AWS handles the infrastructure management, including server maintenance, patching, and scaling. This allows your development team to focus on writing code and delivering features rather than managing servers. Enhanced Agility : Faster Deployment : Serverless architecture enables rapid development and deployment cycles. AWS services such as Lambda and AppSync integrate seamlessly with CI/CD pipelines, allowing for continuous integration and delivery. Event-Driven Design : Integration with AWS Services : AWS serverless services are designed to work well together. For example, you can use Amazon EventBridge to trigger AWS Lambda functions, enabling an event-driven architecture that is highly responsive to changes and real-time data. High Availability and Reliability : Built-in Fault Tolerance : AWS serverless services offer built-in fault tolerance and availability. AWS Lambda functions are replicated across multiple availability zones, ensuring that your application remains resilient and highly available.","title":"Benefits of Serverless on AWS"},{"location":"06_Why/05_serverless/#why-serverless-fits-our-architecture","text":"Alignment with CQRS and Event Sourcing : Our choice of CQRS and event sourcing patterns fits perfectly with serverless architecture. AWS Lambda functions handle commands (writes) and queries (reads) efficiently, while services like EventBridge facilitate event-driven communication. Seamless API Management : Using AWS AppSync for our GraphQL API management aligns with our need for flexible and scalable API solutions. AppSync simplifies the creation and management of GraphQL APIs, allowing us to focus on delivering business value. Optimized for Domain Experts : Serverless architecture abstracts away infrastructure complexities, allowing domain experts to focus on system design and business logic. This aligns with our goal of enabling domain experts to contribute directly to the system without needing deep technical expertise in infrastructure management. While serverless icreases configuration complexity, this is largly abstracted away by the Tracepaper modeling concepts.","title":"Why Serverless Fits Our Architecture"},{"location":"06_Why/05_serverless/#conclusion","text":"Choosing a serverless architecture on AWS provides us with scalability, cost efficiency, reduced operational overhead, and enhanced agility. These benefits align perfectly with our design patterns (CQRS and event sourcing) and our goal of empowering domain experts to focus on delivering business value. AWS's comprehensive suite of serverless services ensures that our applications are robust, scalable, and maintainable, supporting our long-term objectives.","title":"Conclusion"},{"location":"06_Why/06_aws/","text":"Why AWS # Overview # AWS (Amazon Web Services) is the leading cloud service provider, offering a comprehensive and mature suite of cloud services that cater to various business needs. Our decision to use AWS for our serverless architecture was driven by several key factors that align with our technical and business requirements. Key Benefits of AWS # Comprehensive Service Offering : AWS provides a vast array of services that cover all aspects of modern cloud computing, including computing power, storage, databases, machine learning, and analytics. This comprehensive suite allows us to build, deploy, and scale our applications efficiently. Mature Serverless Ecosystem : AWS has a robust and mature serverless ecosystem, including AWS Lambda for compute, Amazon EventBridge for event-driven integrations, and AWS AppSync for GraphQL APIs. These services enable us to implement a fully serverless architecture that is scalable, cost-effective, and easy to manage. Scalability and Performance : AWS is designed to handle workloads of any size and complexity. It offers automatic scaling and high availability across multiple regions, ensuring our applications can meet demand spikes and maintain performance. Security and Compliance : AWS provides a secure and compliant cloud infrastructure with numerous certifications and compliance programs. This ensures that our applications meet industry standards and regulatory requirements, giving us and our customers confidence in our security posture. Cost Management : AWS's pay-as-you-go pricing model allows us to optimize costs by only paying for the resources we use. Additionally, tools like AWS Cost Explorer and AWS Budgets help us monitor and manage our spending effectively. Global Reach : With a global network of data centers, AWS allows us to deploy applications closer to our users, reducing latency and improving the user experience. This global reach supports our expansion into new markets and regions. Why AWS Fits Our Architecture # Serverless Capabilities : Our architecture leverages AWS Lambda for executing code in response to events, Amazon EventBridge for integrating services, and AWS AppSync for managing GraphQL APIs. This serverless approach reduces operational overhead and allows us to focus on developing business logic. Support for CQRS and Event Sourcing : AWS services align well with our CQRS and event sourcing patterns. AWS Lambda functions handle commands and queries efficiently, while EventBridge enables event-driven communication. This integration supports our architectural principles and enhances system scalability and performance. Integration with AWS Ecosystem : AWS offers seamless integration with other AWS services, such as S3 for storage, DynamoDB for NoSQL databases, and IAM for access management. This integration streamlines our development process and ensures a cohesive and efficient infrastructure. Developer Tools and Ecosystem : AWS provides a rich set of developer tools, including AWS CodePipeline, AWS CodeBuild, and AWS CloudFormation, which support our CI/CD workflows and infrastructure as code practices. These tools enhance our development efficiency and deployment automation. Why We Don't Support Other Cloud Vendors # We have chosen AWS as our exclusive cloud provider due to its unmatched combination of services, scalability, and integration capabilities, which perfectly align with our technical and business needs. While other cloud vendors offer competitive services, none match the maturity and breadth of AWS's offerings in the serverless ecosystem. Transitioning to or supporting multiple cloud providers would introduce unnecessary complexity, increase operational overhead, and dilute our focus on optimizing our AWS-based architecture. Consequently, we do not foresee supporting other cloud vendors in the short term. Conclusion # Choosing AWS as our cloud provider offers numerous benefits, including a comprehensive service offering, a mature serverless ecosystem, scalability, security, and global reach. These advantages align with our architectural principles and business objectives, enabling us to build robust, scalable, and secure applications while focusing on delivering value to our customers. AWS's extensive suite of services and tools supports our goal of simplifying system design for domain experts and achieving operational excellence.","title":"Why AWS"},{"location":"06_Why/06_aws/#why-aws","text":"","title":"Why AWS"},{"location":"06_Why/06_aws/#overview","text":"AWS (Amazon Web Services) is the leading cloud service provider, offering a comprehensive and mature suite of cloud services that cater to various business needs. Our decision to use AWS for our serverless architecture was driven by several key factors that align with our technical and business requirements.","title":"Overview"},{"location":"06_Why/06_aws/#key-benefits-of-aws","text":"Comprehensive Service Offering : AWS provides a vast array of services that cover all aspects of modern cloud computing, including computing power, storage, databases, machine learning, and analytics. This comprehensive suite allows us to build, deploy, and scale our applications efficiently. Mature Serverless Ecosystem : AWS has a robust and mature serverless ecosystem, including AWS Lambda for compute, Amazon EventBridge for event-driven integrations, and AWS AppSync for GraphQL APIs. These services enable us to implement a fully serverless architecture that is scalable, cost-effective, and easy to manage. Scalability and Performance : AWS is designed to handle workloads of any size and complexity. It offers automatic scaling and high availability across multiple regions, ensuring our applications can meet demand spikes and maintain performance. Security and Compliance : AWS provides a secure and compliant cloud infrastructure with numerous certifications and compliance programs. This ensures that our applications meet industry standards and regulatory requirements, giving us and our customers confidence in our security posture. Cost Management : AWS's pay-as-you-go pricing model allows us to optimize costs by only paying for the resources we use. Additionally, tools like AWS Cost Explorer and AWS Budgets help us monitor and manage our spending effectively. Global Reach : With a global network of data centers, AWS allows us to deploy applications closer to our users, reducing latency and improving the user experience. This global reach supports our expansion into new markets and regions.","title":"Key Benefits of AWS"},{"location":"06_Why/06_aws/#why-aws-fits-our-architecture","text":"Serverless Capabilities : Our architecture leverages AWS Lambda for executing code in response to events, Amazon EventBridge for integrating services, and AWS AppSync for managing GraphQL APIs. This serverless approach reduces operational overhead and allows us to focus on developing business logic. Support for CQRS and Event Sourcing : AWS services align well with our CQRS and event sourcing patterns. AWS Lambda functions handle commands and queries efficiently, while EventBridge enables event-driven communication. This integration supports our architectural principles and enhances system scalability and performance. Integration with AWS Ecosystem : AWS offers seamless integration with other AWS services, such as S3 for storage, DynamoDB for NoSQL databases, and IAM for access management. This integration streamlines our development process and ensures a cohesive and efficient infrastructure. Developer Tools and Ecosystem : AWS provides a rich set of developer tools, including AWS CodePipeline, AWS CodeBuild, and AWS CloudFormation, which support our CI/CD workflows and infrastructure as code practices. These tools enhance our development efficiency and deployment automation.","title":"Why AWS Fits Our Architecture"},{"location":"06_Why/06_aws/#why-we-dont-support-other-cloud-vendors","text":"We have chosen AWS as our exclusive cloud provider due to its unmatched combination of services, scalability, and integration capabilities, which perfectly align with our technical and business needs. While other cloud vendors offer competitive services, none match the maturity and breadth of AWS's offerings in the serverless ecosystem. Transitioning to or supporting multiple cloud providers would introduce unnecessary complexity, increase operational overhead, and dilute our focus on optimizing our AWS-based architecture. Consequently, we do not foresee supporting other cloud vendors in the short term.","title":"Why We Don't Support Other Cloud Vendors"},{"location":"06_Why/06_aws/#conclusion","text":"Choosing AWS as our cloud provider offers numerous benefits, including a comprehensive service offering, a mature serverless ecosystem, scalability, security, and global reach. These advantages align with our architectural principles and business objectives, enabling us to build robust, scalable, and secure applications while focusing on delivering value to our customers. AWS's extensive suite of services and tools supports our goal of simplifying system design for domain experts and achieving operational excellence.","title":"Conclusion"}]}