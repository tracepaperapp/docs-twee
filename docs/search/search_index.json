{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Tracepaper # Introducing Tracepaper by Draftsman, a revolutionary tool designed to simplify the complexities of building business applications. While the landscape is rich with low-code tools to aid development, the challenges of managing these tools in production can be daunting. From maintaining performance to ensuring granular access control and backups, the operational demands can strain businesses lacking a robust engineering department. Enter serverless services offered by major cloud providers like AWS, promising to alleviate this operational burden. \"Serverless is how the cloud wants you to build applications,\" as Gregor Hohpe puts it. But is this approach practical? In practice, serverless architecture introduces a high level of runtime granularity, where each line in your domain model translates into network calls fraught with access control, latency, and security considerations. It requires a shift from operational competence to distributed system design proficiency. However, if your expertise lies in your domain rather than distributed systems intricacies, you likely prefer to reason about your domain holistically\u2014a \"Monolith,\" in industry terms. This is where Tracepaper shines, drawing inspiration from Domain Driven Design principles. We empower you to model your domain using distinct boxes connected by lines, representing various elements such as API access points, domain behaviors, and materialized views. Our modeling tool prioritizes data structures and the connections between these boxes, allowing you to focus on the essence of your domain. While we provide modeling concepts for the business logic within these boxes, the true strength lies in the ability to inject custom Python code. Transitioning from pseudo-code to Python is seamless, freeing you from concerns about the connections between your logic. Let us handle the lines, so you can concentrate on the boxes\u2014the core of your domain expertise. The model seamlessly converts into a Python project defined with CloudFormation, AWS's Infrastructure as Code specification. This ensures deployability within your AWS account, granting you full control over runtime parameters such as access and cost. Additionally, both the model and the generated project are stored in your GitHub account, offering complete oversight and ownership. With Tracepaper, we streamline the process of modeling your domain as a distributed system. Let us handle the intricacies, while you focus on what truly matters\u2014your domain.","title":"Welcome to Tracepaper"},{"location":"#welcome-to-tracepaper","text":"Introducing Tracepaper by Draftsman, a revolutionary tool designed to simplify the complexities of building business applications. While the landscape is rich with low-code tools to aid development, the challenges of managing these tools in production can be daunting. From maintaining performance to ensuring granular access control and backups, the operational demands can strain businesses lacking a robust engineering department. Enter serverless services offered by major cloud providers like AWS, promising to alleviate this operational burden. \"Serverless is how the cloud wants you to build applications,\" as Gregor Hohpe puts it. But is this approach practical? In practice, serverless architecture introduces a high level of runtime granularity, where each line in your domain model translates into network calls fraught with access control, latency, and security considerations. It requires a shift from operational competence to distributed system design proficiency. However, if your expertise lies in your domain rather than distributed systems intricacies, you likely prefer to reason about your domain holistically\u2014a \"Monolith,\" in industry terms. This is where Tracepaper shines, drawing inspiration from Domain Driven Design principles. We empower you to model your domain using distinct boxes connected by lines, representing various elements such as API access points, domain behaviors, and materialized views. Our modeling tool prioritizes data structures and the connections between these boxes, allowing you to focus on the essence of your domain. While we provide modeling concepts for the business logic within these boxes, the true strength lies in the ability to inject custom Python code. Transitioning from pseudo-code to Python is seamless, freeing you from concerns about the connections between your logic. Let us handle the lines, so you can concentrate on the boxes\u2014the core of your domain expertise. The model seamlessly converts into a Python project defined with CloudFormation, AWS's Infrastructure as Code specification. This ensures deployability within your AWS account, granting you full control over runtime parameters such as access and cost. Additionally, both the model and the generated project are stored in your GitHub account, offering complete oversight and ownership. With Tracepaper, we streamline the process of modeling your domain as a distributed system. Let us handle the intricacies, while you focus on what truly matters\u2014your domain.","title":"Welcome to Tracepaper"},{"location":"overview/","text":"Concept overview # Certainly! Here's a textual explanation of the concepts depicted in the architecture diagram: In the architecture diagram: Commands Commands represent actions triggered by users or external systems. They encapsulate requests to perform specific operations within the system. Aggregates and Behavior Event Sourcing : This approach ensures that domain events, which capture immutable truths about past actions, remain unchanged over time. It separates the factual recording (event sourcing) from how we interpret and reason about these events (projection). Technical Key : This key is immutable and crucial for maintaining consistency in the system. Views and Queries Projection of Truth : While projecting the truth to execute domain logic is essential, querying involves a different perspective. It allows deviations in the data model and supports querying based on functional keys that can change over time. Materialized Views : These are separate read models optimized for queries. They are decoupled from the write model and offer benefits like converting technical keys to functional ones, optimizing data aggregations, and establishing relationships between entities that are otherwise decoupled. Notifiers Notifiers orchestrate commands in response to domain events without maintaining state. They have the capability to invoke a wide range of web APIs, including AWS, REST, GraphQL, and the platform's own API. Projections Projections enhance queries by combining data from APIs, materialized views, and custom Python logic. They facilitate transformations such as calculations, advanced filtering, or generating time-sensitive attributes (tokens). This structured approach ensures clarity and flexibility in handling actions, data querying, and system interactions within the architectural context provided.","title":"Concept overview"},{"location":"overview/#concept-overview","text":"Certainly! Here's a textual explanation of the concepts depicted in the architecture diagram: In the architecture diagram: Commands Commands represent actions triggered by users or external systems. They encapsulate requests to perform specific operations within the system. Aggregates and Behavior Event Sourcing : This approach ensures that domain events, which capture immutable truths about past actions, remain unchanged over time. It separates the factual recording (event sourcing) from how we interpret and reason about these events (projection). Technical Key : This key is immutable and crucial for maintaining consistency in the system. Views and Queries Projection of Truth : While projecting the truth to execute domain logic is essential, querying involves a different perspective. It allows deviations in the data model and supports querying based on functional keys that can change over time. Materialized Views : These are separate read models optimized for queries. They are decoupled from the write model and offer benefits like converting technical keys to functional ones, optimizing data aggregations, and establishing relationships between entities that are otherwise decoupled. Notifiers Notifiers orchestrate commands in response to domain events without maintaining state. They have the capability to invoke a wide range of web APIs, including AWS, REST, GraphQL, and the platform's own API. Projections Projections enhance queries by combining data from APIs, materialized views, and custom Python logic. They facilitate transformations such as calculations, advanced filtering, or generating time-sensitive attributes (tokens). This structured approach ensures clarity and flexibility in handling actions, data querying, and system interactions within the architectural context provided.","title":"Concept overview"},{"location":"01_Write_Domain/","text":"About # Introduction # The Write Domain is a fundamental part of our model-driven development environment. It focuses on processing and recording data within a system. This domain encompasses all components responsible for invoking and executing behaviors that change the state of the application. The Write Domain is essential for maintaining the integrity and consistency of data. Components of the Write Domain # The Write Domain consists of several key components: Commands, Subdomains, Aggregates, Behavior Flows, Domain Events, and Automations. Each of these components plays a specific role in capturing and processing events and actions that affect the state of the application. Commands # Commands represent interactions from users or systems with the application. These events are triggered by actions of actors (such as users or external systems) and initiate changes in the application's state. Subdomains # Subdomains help to organize and structure the Write Domain by grouping related aggregates. This modular approach allows for better separation of concerns and clearer boundaries between different parts of the system. Each subdomain focuses on a specific aspect of the business logic, making it easier to manage and maintain. Aggregates # Aggregates are a fundamental concept within the Write Domain that represent a cluster of related objects treated as a single unit for data changes. Each aggregate has a document model that represents state with a defined boundary that ensures the consistency of its data changes. Aggregates encapsulate both data and behavior, providing a clear structure for managing complex business logic. Behavior Flows # Behavior Flows are the core of the Write Domain. They model a set of actions that take place after an event is received. These actions include data-validation rules or business rules, and they may alter the state of the application by publishing a domain event. Domain Events # Domain events represent significant occurrences within the system that reflect a change in state. They capture the essential information about what happened, providing a way to track and react to changes in the application. Event Handling : Domain events are used to update the state of aggregates. Event handlers process these events to apply the necessary changes and maintain consistency within the system. A domain event may also trigger other behavior flows or automations. Automations # Automations are responsible for performing specific actions when certain conditions or events occur. They are often used for system activities that need to take place in response to specific events within the Write Domain. Automations can fail silently if necessary. Purpose and Benefits of the Write Domain # The Write Domain plays a crucial role in ensuring the integrity and consistency of data within an application. By strictly separating data and behavior logic, the Write Domain offers the following benefits: Consistency : By encapsulating data and behavior within behavior flows and aggregates, it ensures that changes are applied consistently and atomically. Traceability : Commands provide a clear and auditable trail of all actions and events that lead to changes in the application's state. Maintainability : By separating different responsibilities (commands, behavior flows, aggregates, subdomains, domain events, automations), complexity is reduced, making the code more maintainable and extendable. Modularity : Subdomains and aggregates provide a structured way to organize the system, promoting modularity and reducing the risk of interdependencies that can lead to errors. Reactivity : Domain events enable the system to react to significant changes in state, allowing for more responsive and dynamic behavior. Conclusion # The Write Domain is an essential concept within our model-driven development environment. It provides a clear structure and maintains the integrity of the application through well-defined components such as Commands, Behavior Flows, Aggregates, Subdomains, Domain Events, and Automations. By effectively using these components, we can build robust, consistent, and well-maintained applications.","title":"About"},{"location":"01_Write_Domain/#about","text":"","title":"About"},{"location":"01_Write_Domain/#introduction","text":"The Write Domain is a fundamental part of our model-driven development environment. It focuses on processing and recording data within a system. This domain encompasses all components responsible for invoking and executing behaviors that change the state of the application. The Write Domain is essential for maintaining the integrity and consistency of data.","title":"Introduction"},{"location":"01_Write_Domain/#components-of-the-write-domain","text":"The Write Domain consists of several key components: Commands, Subdomains, Aggregates, Behavior Flows, Domain Events, and Automations. Each of these components plays a specific role in capturing and processing events and actions that affect the state of the application.","title":"Components of the Write Domain"},{"location":"01_Write_Domain/#commands","text":"Commands represent interactions from users or systems with the application. These events are triggered by actions of actors (such as users or external systems) and initiate changes in the application's state.","title":"Commands"},{"location":"01_Write_Domain/#subdomains","text":"Subdomains help to organize and structure the Write Domain by grouping related aggregates. This modular approach allows for better separation of concerns and clearer boundaries between different parts of the system. Each subdomain focuses on a specific aspect of the business logic, making it easier to manage and maintain.","title":"Subdomains"},{"location":"01_Write_Domain/#aggregates","text":"Aggregates are a fundamental concept within the Write Domain that represent a cluster of related objects treated as a single unit for data changes. Each aggregate has a document model that represents state with a defined boundary that ensures the consistency of its data changes. Aggregates encapsulate both data and behavior, providing a clear structure for managing complex business logic.","title":"Aggregates"},{"location":"01_Write_Domain/#behavior-flows","text":"Behavior Flows are the core of the Write Domain. They model a set of actions that take place after an event is received. These actions include data-validation rules or business rules, and they may alter the state of the application by publishing a domain event.","title":"Behavior Flows"},{"location":"01_Write_Domain/#domain-events","text":"Domain events represent significant occurrences within the system that reflect a change in state. They capture the essential information about what happened, providing a way to track and react to changes in the application. Event Handling : Domain events are used to update the state of aggregates. Event handlers process these events to apply the necessary changes and maintain consistency within the system. A domain event may also trigger other behavior flows or automations.","title":"Domain Events"},{"location":"01_Write_Domain/#automations","text":"Automations are responsible for performing specific actions when certain conditions or events occur. They are often used for system activities that need to take place in response to specific events within the Write Domain. Automations can fail silently if necessary.","title":"Automations"},{"location":"01_Write_Domain/#purpose-and-benefits-of-the-write-domain","text":"The Write Domain plays a crucial role in ensuring the integrity and consistency of data within an application. By strictly separating data and behavior logic, the Write Domain offers the following benefits: Consistency : By encapsulating data and behavior within behavior flows and aggregates, it ensures that changes are applied consistently and atomically. Traceability : Commands provide a clear and auditable trail of all actions and events that lead to changes in the application's state. Maintainability : By separating different responsibilities (commands, behavior flows, aggregates, subdomains, domain events, automations), complexity is reduced, making the code more maintainable and extendable. Modularity : Subdomains and aggregates provide a structured way to organize the system, promoting modularity and reducing the risk of interdependencies that can lead to errors. Reactivity : Domain events enable the system to react to significant changes in state, allowing for more responsive and dynamic behavior.","title":"Purpose and Benefits of the Write Domain"},{"location":"01_Write_Domain/#conclusion","text":"The Write Domain is an essential concept within our model-driven development environment. It provides a clear structure and maintains the integrity of the application through well-defined components such as Commands, Behavior Flows, Aggregates, Subdomains, Domain Events, and Automations. By effectively using these components, we can build robust, consistent, and well-maintained applications.","title":"Conclusion"},{"location":"01_Write_Domain/01_commands/","text":"Commands # Overview # Commands are actions triggered by users or external systems to request the system to perform a specific operation. Commands encapsulate all the necessary information required to carry out the action, including authorization details and data fields. They are the primary means through which external entities interact with the system. Command Definition # A command is defined by specifying its name, authorization requirements, and associated data fields. Commands are triggered by actors (users or external systems) and may lead to state changes within the system. Attributes of a Command # Name : A unique identifier for the command. Authorization : Defines who can trigger the command. This includes: Authorization Type : Indicates if the command requires a specific role, can be triggered by any user ( anonymous ), or requires the user to be authenticated ( authenticated ). Role : In case role-based access is selected, specifies the user role required (e.g., administrator ). GraphQL Configuration # Commands are exposed via a GraphQL API, allowing clients to interact with the system using standard GraphQL operations. The configuration for GraphQL exposure includes: GraphQL Namespace : Organizes related commands within a logical grouping. GraphQL Name : Defines the specific mutation name for the command. This configuration enables seamless integration and interaction with the system, providing a clear and structured API for external clients. Fields # Fields represent the data required to execute the command. Each field is defined with the following attributes: Name : The name of the field. Type : The data type of the field. Possible types include: String : Textual data. Int : Integer values. Boolean : True/false values. Float : Floating-point numbers. Pattern : (Optional) A regular expression pattern that the field value must match (e.g., DateTime input in a String field). Default : (Optional) Default value for the field if none is provided. When a default is provided, the field will become optional in the GraphQL schema. Auto-fill : (Optional) Instructions for automatically generating the field value (e.g., uuid for unique identifiers or username ). Using auto-fill will remove this field from the GraphQL schema. Nested Collections # Commands can have complex data structures, which can be represented using collections of nested objects. Nested objects allow the inclusion of sub-structures within a command, providing a way to encapsulate related data. A nested collection has a name and a set of fields. Summary # Commands in an event-driven architecture encapsulate actions triggered by actors, specifying the necessary data and authorization requirements. By defining commands with fields, nested collections, and GraphQL configurations, systems can ensure a robust, secure, and structured interaction model for handling user and system interactions.","title":"Commands"},{"location":"01_Write_Domain/01_commands/#commands","text":"","title":"Commands"},{"location":"01_Write_Domain/01_commands/#overview","text":"Commands are actions triggered by users or external systems to request the system to perform a specific operation. Commands encapsulate all the necessary information required to carry out the action, including authorization details and data fields. They are the primary means through which external entities interact with the system.","title":"Overview"},{"location":"01_Write_Domain/01_commands/#command-definition","text":"A command is defined by specifying its name, authorization requirements, and associated data fields. Commands are triggered by actors (users or external systems) and may lead to state changes within the system.","title":"Command Definition"},{"location":"01_Write_Domain/01_commands/#attributes-of-a-command","text":"Name : A unique identifier for the command. Authorization : Defines who can trigger the command. This includes: Authorization Type : Indicates if the command requires a specific role, can be triggered by any user ( anonymous ), or requires the user to be authenticated ( authenticated ). Role : In case role-based access is selected, specifies the user role required (e.g., administrator ).","title":"Attributes of a Command"},{"location":"01_Write_Domain/01_commands/#graphql-configuration","text":"Commands are exposed via a GraphQL API, allowing clients to interact with the system using standard GraphQL operations. The configuration for GraphQL exposure includes: GraphQL Namespace : Organizes related commands within a logical grouping. GraphQL Name : Defines the specific mutation name for the command. This configuration enables seamless integration and interaction with the system, providing a clear and structured API for external clients.","title":"GraphQL Configuration"},{"location":"01_Write_Domain/01_commands/#fields","text":"Fields represent the data required to execute the command. Each field is defined with the following attributes: Name : The name of the field. Type : The data type of the field. Possible types include: String : Textual data. Int : Integer values. Boolean : True/false values. Float : Floating-point numbers. Pattern : (Optional) A regular expression pattern that the field value must match (e.g., DateTime input in a String field). Default : (Optional) Default value for the field if none is provided. When a default is provided, the field will become optional in the GraphQL schema. Auto-fill : (Optional) Instructions for automatically generating the field value (e.g., uuid for unique identifiers or username ). Using auto-fill will remove this field from the GraphQL schema.","title":"Fields"},{"location":"01_Write_Domain/01_commands/#nested-collections","text":"Commands can have complex data structures, which can be represented using collections of nested objects. Nested objects allow the inclusion of sub-structures within a command, providing a way to encapsulate related data. A nested collection has a name and a set of fields.","title":"Nested Collections"},{"location":"01_Write_Domain/01_commands/#summary","text":"Commands in an event-driven architecture encapsulate actions triggered by actors, specifying the necessary data and authorization requirements. By defining commands with fields, nested collections, and GraphQL configurations, systems can ensure a robust, secure, and structured interaction model for handling user and system interactions.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/","text":"Aggregates # An aggregate reflects a domain concept, a collection of data (state) and behavior to manipulate the data. The data is internally encapsulated, so the only way to induce a state change is by invoking behavior. Modeling an aggregate involves two activities: Modeling the data model. Modeling behavior on the model. A concept from DDD opinionated to optimize cognitive load, focusing on the concepts rather than the underlying technology. It abstracts the Command/Compute part of our CQRS architecture. Aggregate Data Model # The data model consists of two parts, the actual part: the events , and the projection: the document . The Document # The document is a projection of the data, a mental model of the state so that we can implement validations in the behavior to ensure data integrity. Because it is a projection, this model can evolve over time without manipulating the underlying data. Events from the past are immutable by definition. Thus, the document serves both as a mental model for ourselves on how we want to think about state and as a way to model how we present data to the viewstore. It is the data contract between the aggregate and the viewstore. One of the document fields (String) will serve as the business key of the aggregate instance The Events # The domain events are the recording of facts. They reflect a bundle of data that encompasses the delta between two states. Therefore, the state is not stored in the database, merely a log of deltas that exist alongside each other on a timeline. This is called the event log. Event Handling # To populate our mental model, the projection, we need event handlers. These model the mapping between the event log and the document. Side Notes on Views # Views will be discussed later, but regarding the contract, the viewstore essentially models the external contract, the GraphQL API. So while the document is a data contract, this contract remains within the domain. The viewstore can do various things with the data: Store As Is # Essentially caching a snapshot of the aggregate state. In this case, the internal model becomes publicly accessible, being queryable and read-optimized. Enrich and Store # Enrichment can take various forms, such as combining data from different aggregates into one document or modifying data for storage, i.e., determining derived data and caching it in the viewstore. Enrich During Data Reading # Modifying the API response before it is sent to the client. This involves executing logic on the combination of request data and cached data. Essentially, this is an on-the-fly projection where the view model is virtual. The logic has access to the request data and a fluent API to the viewstore, allowing the creation of a response object using Python scripting from this combination. Behavior Flows # Overview # Behavior flows define the lifecycle and interactions of aggregates within a system. They encapsulate the business logic required to handle events and commands, ensuring consistent state transitions and proper handling of complex workflows. By defining how an aggregate responds to various inputs, behavior flows help maintain system integrity and align with business rules. General # Behavior is a response to an event emited within the domain this may be an ActorEvent (Command) or a DomainEvent (From an other entity), which can lead to changes in the state of an aggregate. Name : A unique identifier for the command. Create Command : (Optional) Indicates if this should be the initial behavior of a new instance of the aggregate. If the instance with the specific business key is already present in the database the execution will fail. Trigger # A trigger specifies the conditions under which a command is activated. It listens for specific events and uses the data from these events to execute the corresponding behavior flow. Source : The event that activates the command. Key Field : The primary field used to correlate the incoming event with the correct aggregate instance. Mapping : Transfers data from the event to the flow variable fields. Optionally event-fields can be marked as part of the idempotency key giving you the ability to model functional idempotency. Processors # Processors are essential components within behavior flows that perform specific operations. They manage validations, data transformations, event emissions, and other logical actions to ensure that the behavior executes correctly and that aggregates maintain consistent state transitions. Each processor type serves a unique function and has specific attributes to support its operations. Processor Types # Emit Event # The emit-event processor is responsible for generating and emitting domain events, which signal that a significant change or action has occurred within the system. This processor type includes a reference specifying the event to be emitted, and a mapping mechanism that transfers flow-variables to the event fields. Code # The code processor allows for custom logic to be executed within the behavior flow. This processor is used when predefined processor types do not cover the required operation. It includes the code (inline) or script (global) to be executed, and optionally, inputs or parameters required by the script. Validator # The validator processor checks specific conditions and ensures they are met before proceeding. If the condition fails, an exception or error is triggered, preventing the behavior from executing further. This processor type includes the condition to be checked, typically expressed as a logical expression, and the message or exception to be raised if the condition is not met. If a functional exception is raised, no events will be persisted to the event store. So a validator may also validate on the instance state after emit-event processors are already applied, it will still prevent the state transition. Set Variable # The set-variable processor assigns values to variables (in memory) within the behavior flow. This can be used to store intermediate results, configuration values, or other data needed for subsequent processing steps. This processor type includes the name of the variable to be set, and the expression or value used to determine the variable's value. Update Key # The update-key processor updates the business key of an aggregate instance. This is useful for operations that require changing the identifier or key used to access an aggregate instance. This processor type includes the field in the aggregate to be updated, and the new value to be assigned to the key field. Test Case # Test cases validate the behavior of a command by defining inputs, expected domain events, and resulting state changes. They ensure that the command performs as intended and that the aggregate's state transitions correctly. Name : A unique identifier for the test case. Trigger Event : The event that initiates the test case. Input : Defines the input data for the command. Name : The name of the input field. Value : The value assigned to the input field. Type : The data type of the input field. Expected Domain Event : Specifies the expected event to be emitted by the command. Field : Defines the expected values for fields in the emitted event. State : (Optional) The initial state of the aggregate before executing the command. Expected State : The expected state of the aggregate after executing the command. Primary Key (pk) : The key identifying the aggregate instance. State Data : The expected state data of the aggregate. Summary # Behavior flows are essential for managing the lifecycle and interactions of aggregates. By defining commands, triggers, mappings, processors, and test cases, behavior flows ensure that business logic is consistently applied, state transitions are correctly managed, and the system's behavior aligns with the intended domain model. Understanding and modeling behavior flows are crucial for implementing robust, maintainable, and scalable systems. Advanced Features of an Aggregate # Event Store Time-to-Live (TTL) # The Event Store Time-to-Live (TTL) is an advanced configuration setting for aggregates that specifies the duration, in seconds, for which events associated with the aggregate should be retained in the event store. This feature is particularly useful in scenarios where certain events become irrelevant or obsolete after a specified period. The default TTL is -1 seconds which translates to indefinitely. The TTL setting allows system architects and developers to manage the lifecycle of events within the event store effectively. By setting an expiration time for events related to an aggregate, the system can automatically purge older events beyond the specified TTL. This helps in maintaining a lean and efficient event store by removing outdated data that no longer contributes to the current state or history of the aggregate. Benefits # Optimized Event Storage : Helps in managing storage space by automatically removing events that are no longer relevant. Compliance and Governance : Supports compliance requirements by ensuring that sensitive or outdated data is removed from the system in a timely manner. Performance : Improves performance by reducing the volume of data that needs to be processed and queried over time. Considerations # Event Retention Policies : Define appropriate TTL values based on business needs and regulatory requirements. Data Archival : Consider integrating TTL with data archival strategies to maintain historical data beyond the event store. Summary # The Event Store Time-to-Live (TTL) configuration for aggregates enhances the management and efficiency of event-driven architectures. By setting TTL values, systems can automatically manage event data retention, ensuring that only relevant and current information is maintained in the event store while optimizing storage resources and improving overall system performance. Snapshot Interval # The Snapshot Interval is an advanced configuration setting for aggregates that determines how frequently snapshots of the aggregate's state should be taken. Snapshots capture the current state of an aggregate at a specific point in time, providing a checkpoint that can optimize event sourcing and reconstruction processes. In event sourcing architectures, aggregates can accumulate a large number of events over time, which can impact performance during state reconstruction. Snapshots serve as efficient checkpoints by storing the aggregate's current state periodically. Instead of replaying all events from the beginning to reconstruct the aggregate's state, the system can load the latest snapshot and replay only the events that occurred after the snapshot was taken. Usage # When configuring an aggregate with Snapshot Interval: Snapshot Interval : Specifies the number of events after which a new snapshot of the aggregate's state should be taken. For example, a snapshot interval of 100 means that every 100 events, a snapshot of the aggregate's state will be captured. Benefits # Performance Optimization : Reduces the time and computational resources required for state reconstruction by loading snapshots and applying only subsequent events. Efficient Event Handling : Improves overall system performance by minimizing the number of events that need to be processed during state recovery. Scalability : Facilitates scalability by reducing the processing overhead associated with large aggregates and long event histories. Considerations # Snapshot Size : Evaluate the size and complexity of aggregate states to determine an appropriate snapshot interval. Event Volume : Adjust the snapshot interval based on the frequency and volume of events generated by aggregates. Default Value # The default Snapshot Interval is typically set to 100 events, providing a balance between capturing frequent snapshots and minimizing overhead. This default value can be adjusted based on specific application requirements and performance considerations. Summary # The Snapshot Interval configuration enhances the efficiency and performance of event-sourced aggregates by periodically capturing snapshots of their states. By reducing the computational effort required for state reconstruction, snapshots optimize event handling and support scalability in event-driven architectures. Adjusting the snapshot interval allows systems to achieve optimal performance while effectively managing aggregate states and event histories. Backup to Blob Storage (S3) # The Backup to Blob Storage feature allows event-sourced aggregates to automatically back up their state to cloud storage, specifically Amazon S3. This capability ensures that critical data remains secure and accessible, providing resilience against data loss and supporting disaster recovery strategies. In event sourcing architectures, ensuring data durability and recoverability is paramount. Backup to Blob Storage leverages cloud infrastructure, such as Amazon S3, to store snapshots of aggregate states at regular intervals. Additionally, it manages the retention of these backups based on specified policies, providing flexibility in data management and compliance with retention requirements. The event store (DynamoDB) has point-in-time recovery enabled, therefore, this feature is disabled by default to reduce the area where data needs to be protected. If this feature should be enabled for a specific aggregate is a business consideration. Configuration # Interval : Specifies the frequency, specified in days, at which snapshots of aggregate states should be backed up to S3. . Retention Period : Defines the duration for which backup snapshots should be retained in S3 storage, measured in days. After the retention period expires, older snapshots are automatically deleted. Usage # The Backup to Blob Storage feature offers several advantages: Data Resilience : Enhances data durability by securely storing aggregate state snapshots in Amazon S3, which is designed for high availability and redundancy. Disaster Recovery : Facilitates rapid recovery of aggregate states in the event of data corruption, system failures, or disasters by maintaining up-to-date backups. Although the required script to recover from cold storage are not (yet) provided by Draftsman. Compliance : Supports compliance with regulatory requirements and data retention policies by managing backup retention periods effectively. Implementation Considerations # Security : Ensure proper access controls and encryption mechanisms are in place to protect sensitive data stored in S3. Cost Management : Monitor storage costs associated with S3 backups and optimize usage based on storage needs and budget constraints. Integration : Integrate with existing backup and recovery processes to streamline data management and operational workflows. Summary # Backup to Blob Storage (S3) is a critical feature in event-driven architectures that ensures the resilience and recoverability of event-sourced aggregates. By automatically backing up aggregate states to Amazon S3 at defined intervals and managing retention periods, this feature enhances data durability, supports disaster recovery strategies, and enables compliance with data retention policies. Configuring backup intervals and retention periods optimally balances data protection with operational efficiency, thereby safeguarding business-critical information in event sourcing environments.","title":"Aggregates"},{"location":"01_Write_Domain/02_aggregate/#aggregates","text":"An aggregate reflects a domain concept, a collection of data (state) and behavior to manipulate the data. The data is internally encapsulated, so the only way to induce a state change is by invoking behavior. Modeling an aggregate involves two activities: Modeling the data model. Modeling behavior on the model. A concept from DDD opinionated to optimize cognitive load, focusing on the concepts rather than the underlying technology. It abstracts the Command/Compute part of our CQRS architecture.","title":"Aggregates"},{"location":"01_Write_Domain/02_aggregate/#aggregate-data-model","text":"The data model consists of two parts, the actual part: the events , and the projection: the document .","title":"Aggregate Data Model"},{"location":"01_Write_Domain/02_aggregate/#the-document","text":"The document is a projection of the data, a mental model of the state so that we can implement validations in the behavior to ensure data integrity. Because it is a projection, this model can evolve over time without manipulating the underlying data. Events from the past are immutable by definition. Thus, the document serves both as a mental model for ourselves on how we want to think about state and as a way to model how we present data to the viewstore. It is the data contract between the aggregate and the viewstore. One of the document fields (String) will serve as the business key of the aggregate instance","title":"The Document"},{"location":"01_Write_Domain/02_aggregate/#the-events","text":"The domain events are the recording of facts. They reflect a bundle of data that encompasses the delta between two states. Therefore, the state is not stored in the database, merely a log of deltas that exist alongside each other on a timeline. This is called the event log.","title":"The Events"},{"location":"01_Write_Domain/02_aggregate/#event-handling","text":"To populate our mental model, the projection, we need event handlers. These model the mapping between the event log and the document.","title":"Event Handling"},{"location":"01_Write_Domain/02_aggregate/#side-notes-on-views","text":"Views will be discussed later, but regarding the contract, the viewstore essentially models the external contract, the GraphQL API. So while the document is a data contract, this contract remains within the domain. The viewstore can do various things with the data:","title":"Side Notes on Views"},{"location":"01_Write_Domain/02_aggregate/#store-as-is","text":"Essentially caching a snapshot of the aggregate state. In this case, the internal model becomes publicly accessible, being queryable and read-optimized.","title":"Store As Is"},{"location":"01_Write_Domain/02_aggregate/#enrich-and-store","text":"Enrichment can take various forms, such as combining data from different aggregates into one document or modifying data for storage, i.e., determining derived data and caching it in the viewstore.","title":"Enrich and Store"},{"location":"01_Write_Domain/02_aggregate/#enrich-during-data-reading","text":"Modifying the API response before it is sent to the client. This involves executing logic on the combination of request data and cached data. Essentially, this is an on-the-fly projection where the view model is virtual. The logic has access to the request data and a fluent API to the viewstore, allowing the creation of a response object using Python scripting from this combination.","title":"Enrich During Data Reading"},{"location":"01_Write_Domain/02_aggregate/#behavior-flows","text":"","title":"Behavior Flows"},{"location":"01_Write_Domain/02_aggregate/#overview","text":"Behavior flows define the lifecycle and interactions of aggregates within a system. They encapsulate the business logic required to handle events and commands, ensuring consistent state transitions and proper handling of complex workflows. By defining how an aggregate responds to various inputs, behavior flows help maintain system integrity and align with business rules.","title":"Overview"},{"location":"01_Write_Domain/02_aggregate/#general","text":"Behavior is a response to an event emited within the domain this may be an ActorEvent (Command) or a DomainEvent (From an other entity), which can lead to changes in the state of an aggregate. Name : A unique identifier for the command. Create Command : (Optional) Indicates if this should be the initial behavior of a new instance of the aggregate. If the instance with the specific business key is already present in the database the execution will fail.","title":"General"},{"location":"01_Write_Domain/02_aggregate/#trigger","text":"A trigger specifies the conditions under which a command is activated. It listens for specific events and uses the data from these events to execute the corresponding behavior flow. Source : The event that activates the command. Key Field : The primary field used to correlate the incoming event with the correct aggregate instance. Mapping : Transfers data from the event to the flow variable fields. Optionally event-fields can be marked as part of the idempotency key giving you the ability to model functional idempotency.","title":"Trigger"},{"location":"01_Write_Domain/02_aggregate/#processors","text":"Processors are essential components within behavior flows that perform specific operations. They manage validations, data transformations, event emissions, and other logical actions to ensure that the behavior executes correctly and that aggregates maintain consistent state transitions. Each processor type serves a unique function and has specific attributes to support its operations.","title":"Processors"},{"location":"01_Write_Domain/02_aggregate/#processor-types","text":"","title":"Processor Types"},{"location":"01_Write_Domain/02_aggregate/#emit-event","text":"The emit-event processor is responsible for generating and emitting domain events, which signal that a significant change or action has occurred within the system. This processor type includes a reference specifying the event to be emitted, and a mapping mechanism that transfers flow-variables to the event fields.","title":"Emit Event"},{"location":"01_Write_Domain/02_aggregate/#code","text":"The code processor allows for custom logic to be executed within the behavior flow. This processor is used when predefined processor types do not cover the required operation. It includes the code (inline) or script (global) to be executed, and optionally, inputs or parameters required by the script.","title":"Code"},{"location":"01_Write_Domain/02_aggregate/#validator","text":"The validator processor checks specific conditions and ensures they are met before proceeding. If the condition fails, an exception or error is triggered, preventing the behavior from executing further. This processor type includes the condition to be checked, typically expressed as a logical expression, and the message or exception to be raised if the condition is not met. If a functional exception is raised, no events will be persisted to the event store. So a validator may also validate on the instance state after emit-event processors are already applied, it will still prevent the state transition.","title":"Validator"},{"location":"01_Write_Domain/02_aggregate/#set-variable","text":"The set-variable processor assigns values to variables (in memory) within the behavior flow. This can be used to store intermediate results, configuration values, or other data needed for subsequent processing steps. This processor type includes the name of the variable to be set, and the expression or value used to determine the variable's value.","title":"Set Variable"},{"location":"01_Write_Domain/02_aggregate/#update-key","text":"The update-key processor updates the business key of an aggregate instance. This is useful for operations that require changing the identifier or key used to access an aggregate instance. This processor type includes the field in the aggregate to be updated, and the new value to be assigned to the key field.","title":"Update Key"},{"location":"01_Write_Domain/02_aggregate/#test-case","text":"Test cases validate the behavior of a command by defining inputs, expected domain events, and resulting state changes. They ensure that the command performs as intended and that the aggregate's state transitions correctly. Name : A unique identifier for the test case. Trigger Event : The event that initiates the test case. Input : Defines the input data for the command. Name : The name of the input field. Value : The value assigned to the input field. Type : The data type of the input field. Expected Domain Event : Specifies the expected event to be emitted by the command. Field : Defines the expected values for fields in the emitted event. State : (Optional) The initial state of the aggregate before executing the command. Expected State : The expected state of the aggregate after executing the command. Primary Key (pk) : The key identifying the aggregate instance. State Data : The expected state data of the aggregate.","title":"Test Case"},{"location":"01_Write_Domain/02_aggregate/#summary","text":"Behavior flows are essential for managing the lifecycle and interactions of aggregates. By defining commands, triggers, mappings, processors, and test cases, behavior flows ensure that business logic is consistently applied, state transitions are correctly managed, and the system's behavior aligns with the intended domain model. Understanding and modeling behavior flows are crucial for implementing robust, maintainable, and scalable systems.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#advanced-features-of-an-aggregate","text":"","title":"Advanced Features of an Aggregate"},{"location":"01_Write_Domain/02_aggregate/#event-store-time-to-live-ttl","text":"The Event Store Time-to-Live (TTL) is an advanced configuration setting for aggregates that specifies the duration, in seconds, for which events associated with the aggregate should be retained in the event store. This feature is particularly useful in scenarios where certain events become irrelevant or obsolete after a specified period. The default TTL is -1 seconds which translates to indefinitely. The TTL setting allows system architects and developers to manage the lifecycle of events within the event store effectively. By setting an expiration time for events related to an aggregate, the system can automatically purge older events beyond the specified TTL. This helps in maintaining a lean and efficient event store by removing outdated data that no longer contributes to the current state or history of the aggregate.","title":"Event Store Time-to-Live (TTL)"},{"location":"01_Write_Domain/02_aggregate/#benefits","text":"Optimized Event Storage : Helps in managing storage space by automatically removing events that are no longer relevant. Compliance and Governance : Supports compliance requirements by ensuring that sensitive or outdated data is removed from the system in a timely manner. Performance : Improves performance by reducing the volume of data that needs to be processed and queried over time.","title":"Benefits"},{"location":"01_Write_Domain/02_aggregate/#considerations","text":"Event Retention Policies : Define appropriate TTL values based on business needs and regulatory requirements. Data Archival : Consider integrating TTL with data archival strategies to maintain historical data beyond the event store.","title":"Considerations"},{"location":"01_Write_Domain/02_aggregate/#summary_1","text":"The Event Store Time-to-Live (TTL) configuration for aggregates enhances the management and efficiency of event-driven architectures. By setting TTL values, systems can automatically manage event data retention, ensuring that only relevant and current information is maintained in the event store while optimizing storage resources and improving overall system performance.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#snapshot-interval","text":"The Snapshot Interval is an advanced configuration setting for aggregates that determines how frequently snapshots of the aggregate's state should be taken. Snapshots capture the current state of an aggregate at a specific point in time, providing a checkpoint that can optimize event sourcing and reconstruction processes. In event sourcing architectures, aggregates can accumulate a large number of events over time, which can impact performance during state reconstruction. Snapshots serve as efficient checkpoints by storing the aggregate's current state periodically. Instead of replaying all events from the beginning to reconstruct the aggregate's state, the system can load the latest snapshot and replay only the events that occurred after the snapshot was taken.","title":"Snapshot Interval"},{"location":"01_Write_Domain/02_aggregate/#usage","text":"When configuring an aggregate with Snapshot Interval: Snapshot Interval : Specifies the number of events after which a new snapshot of the aggregate's state should be taken. For example, a snapshot interval of 100 means that every 100 events, a snapshot of the aggregate's state will be captured.","title":"Usage"},{"location":"01_Write_Domain/02_aggregate/#benefits_1","text":"Performance Optimization : Reduces the time and computational resources required for state reconstruction by loading snapshots and applying only subsequent events. Efficient Event Handling : Improves overall system performance by minimizing the number of events that need to be processed during state recovery. Scalability : Facilitates scalability by reducing the processing overhead associated with large aggregates and long event histories.","title":"Benefits"},{"location":"01_Write_Domain/02_aggregate/#considerations_1","text":"Snapshot Size : Evaluate the size and complexity of aggregate states to determine an appropriate snapshot interval. Event Volume : Adjust the snapshot interval based on the frequency and volume of events generated by aggregates.","title":"Considerations"},{"location":"01_Write_Domain/02_aggregate/#default-value","text":"The default Snapshot Interval is typically set to 100 events, providing a balance between capturing frequent snapshots and minimizing overhead. This default value can be adjusted based on specific application requirements and performance considerations.","title":"Default Value"},{"location":"01_Write_Domain/02_aggregate/#summary_2","text":"The Snapshot Interval configuration enhances the efficiency and performance of event-sourced aggregates by periodically capturing snapshots of their states. By reducing the computational effort required for state reconstruction, snapshots optimize event handling and support scalability in event-driven architectures. Adjusting the snapshot interval allows systems to achieve optimal performance while effectively managing aggregate states and event histories.","title":"Summary"},{"location":"01_Write_Domain/02_aggregate/#backup-to-blob-storage-s3","text":"The Backup to Blob Storage feature allows event-sourced aggregates to automatically back up their state to cloud storage, specifically Amazon S3. This capability ensures that critical data remains secure and accessible, providing resilience against data loss and supporting disaster recovery strategies. In event sourcing architectures, ensuring data durability and recoverability is paramount. Backup to Blob Storage leverages cloud infrastructure, such as Amazon S3, to store snapshots of aggregate states at regular intervals. Additionally, it manages the retention of these backups based on specified policies, providing flexibility in data management and compliance with retention requirements. The event store (DynamoDB) has point-in-time recovery enabled, therefore, this feature is disabled by default to reduce the area where data needs to be protected. If this feature should be enabled for a specific aggregate is a business consideration.","title":"Backup to Blob Storage (S3)"},{"location":"01_Write_Domain/02_aggregate/#configuration","text":"Interval : Specifies the frequency, specified in days, at which snapshots of aggregate states should be backed up to S3. . Retention Period : Defines the duration for which backup snapshots should be retained in S3 storage, measured in days. After the retention period expires, older snapshots are automatically deleted.","title":"Configuration"},{"location":"01_Write_Domain/02_aggregate/#usage_1","text":"The Backup to Blob Storage feature offers several advantages: Data Resilience : Enhances data durability by securely storing aggregate state snapshots in Amazon S3, which is designed for high availability and redundancy. Disaster Recovery : Facilitates rapid recovery of aggregate states in the event of data corruption, system failures, or disasters by maintaining up-to-date backups. Although the required script to recover from cold storage are not (yet) provided by Draftsman. Compliance : Supports compliance with regulatory requirements and data retention policies by managing backup retention periods effectively.","title":"Usage"},{"location":"01_Write_Domain/02_aggregate/#implementation-considerations","text":"Security : Ensure proper access controls and encryption mechanisms are in place to protect sensitive data stored in S3. Cost Management : Monitor storage costs associated with S3 backups and optimize usage based on storage needs and budget constraints. Integration : Integrate with existing backup and recovery processes to streamline data management and operational workflows.","title":"Implementation Considerations"},{"location":"01_Write_Domain/02_aggregate/#summary_3","text":"Backup to Blob Storage (S3) is a critical feature in event-driven architectures that ensures the resilience and recoverability of event-sourced aggregates. By automatically backing up aggregate states to Amazon S3 at defined intervals and managing retention periods, this feature enhances data durability, supports disaster recovery strategies, and enables compliance with data retention policies. Configuring backup intervals and retention periods optimally balances data protection with operational efficiency, thereby safeguarding business-critical information in event sourcing environments.","title":"Summary"},{"location":"01_Write_Domain/03_automations/","text":"Automations # Overview # Automations , also referred to as notifiers , are predefined processes that are triggered in response to specific events or conditions within a system. They are designed to perform actions automatically without direct user intervention, often to notify users or other systems about important events or changes. Automations enhance system responsiveness and improve user experience by providing timely and relevant information. Automation Definition # An automation is defined by specifying its name, trigger conditions, and actions to be performed. Automations react to events occurring within the system, executing predefined actions when certain conditions are met. Attributes of an Automation # Name # A unique identifier for the automation. Trigger Conditions # Conditions or events that initiate the automation. These triggers can be system events, changes in data, or specific time-based conditions. Examples of trigger conditions include: Event-based : Automations can be triggered by specific events such as Commands (ActorEvents) originating from the GraphQL API. Or domain events published by aggregates. Time-based : Automations can be set to trigger at specific times or after certain intervals. After-deployment : Automations can trigger after the application is deployed. Actions # Automation Activity Types # Automations in a system can perform a variety of tasks, known as activities. Each activity type represents a specific action that the automation can execute when triggered. Below is a documentation of the possible activity types that can be used within automations (notifiers). Overview # Automations consist of a series of activities that define the specific actions to be performed. These activities are triggered by events or conditions within the system and can interact with various system components or external services. Activity Types # Identity and Access Management (IAM - AWS Cognito) Activities # create-iam-group Description : Creates a new IAM group within the system. Use Case : Used when a new group of users with specific permissions needs to be created. delete-iam-group Description : Deletes an existing IAM group. Use Case : Used when an IAM group is no longer needed and should be removed. add-user-to-iam-group Description : Adds a user to an IAM group. Use Case : Used to grant a user the permissions associated with the IAM group. remove-user-from-iam-group Description : Removes a user from an IAM group. Use Case : Used to revoke a user's permissions associated with the IAM group. retrieve-email-from-iam Description : Retrieves a user's email address from the IAM system. Use Case : Used to get the email address of a user for communication purposes. iam-create-systemuser Description : Creates a system user in the IAM system. Use Case : Used to create a non-human user that interacts with the system programmatically. iam-create-user Description : Creates a new user in the IAM system. Use Case : Used for onboarding new users. iam-delete-user Description : Deletes a user from the IAM system. Use Case : Used when a user account needs to be permanently removed. Communication and Notification Activities # render-template Description : Renders a template with specified data. Use Case : Used to create personalized messages or documents. send-email Description : Sends an email to specified recipients. (AWS SES) Use Case : Used for sending notifications, alerts, or other communications. send-graphql-notification Description : Sends a notification via the GraphQL endpoint. Use Case : Used for notifying clients or systems through the GraphQL API. File and Data Operations # write-file Description : Writes data to a file. Use Case : Used to save data in a remote file system (AWS S3). fetch-property Description : Fetches a specific property or data point from the system. Use Case : Used to retrieve configuration settings or other data. Token Management Activities # get-token Description : Retrieves a token for authentication or authorization purposes. Use Case : Used to get tokens required for accessing secured resources. get-systemuser-token Description : Retrieves a token for a system user. Use Case : Used for system-to-system authentication. Variable and State Management # set-variable Description : Sets a flow variable to a specified value. Use Case : Used to store temporary data or state within an automation. API and External Service Interactions # call-internal-api Description : Calls an internal GraphQL API endpoint. Use Case : Used to invoke internal system functions (Commands/Queries/Projections). HTTP Description : Makes an HTTP request to an external service. Use Case : Used to interact with external APIs or services. Miscellaneous Activities # code Description : Executes custom code. Use Case : Used for complex logic that cannot be handled by predefined activities. invalidate-cdn Description : Invalidates a CDN cache. Use Case : Used to ensure that updated content is served from the AWS CloudFront CDN. loop Description : Repeats a set of activities a specified number of times. Use Case : Used for iterating over a collection of items or repeating an action multiple times. Summary # Automations leverage a diverse set of activity types to perform various tasks automatically. By combining these activities, systems can create sophisticated workflows that enhance functionality, improve efficiency, and provide timely responses to events and conditions. Understanding these activity types is essential for designing effective automations that meet specific business needs.","title":"Automations"},{"location":"01_Write_Domain/03_automations/#automations","text":"","title":"Automations"},{"location":"01_Write_Domain/03_automations/#overview","text":"Automations , also referred to as notifiers , are predefined processes that are triggered in response to specific events or conditions within a system. They are designed to perform actions automatically without direct user intervention, often to notify users or other systems about important events or changes. Automations enhance system responsiveness and improve user experience by providing timely and relevant information.","title":"Overview"},{"location":"01_Write_Domain/03_automations/#automation-definition","text":"An automation is defined by specifying its name, trigger conditions, and actions to be performed. Automations react to events occurring within the system, executing predefined actions when certain conditions are met.","title":"Automation Definition"},{"location":"01_Write_Domain/03_automations/#attributes-of-an-automation","text":"","title":"Attributes of an Automation"},{"location":"01_Write_Domain/03_automations/#name","text":"A unique identifier for the automation.","title":"Name"},{"location":"01_Write_Domain/03_automations/#trigger-conditions","text":"Conditions or events that initiate the automation. These triggers can be system events, changes in data, or specific time-based conditions. Examples of trigger conditions include: Event-based : Automations can be triggered by specific events such as Commands (ActorEvents) originating from the GraphQL API. Or domain events published by aggregates. Time-based : Automations can be set to trigger at specific times or after certain intervals. After-deployment : Automations can trigger after the application is deployed.","title":"Trigger Conditions"},{"location":"01_Write_Domain/03_automations/#actions","text":"","title":"Actions"},{"location":"01_Write_Domain/03_automations/#automation-activity-types","text":"Automations in a system can perform a variety of tasks, known as activities. Each activity type represents a specific action that the automation can execute when triggered. Below is a documentation of the possible activity types that can be used within automations (notifiers).","title":"Automation Activity Types"},{"location":"01_Write_Domain/03_automations/#overview_1","text":"Automations consist of a series of activities that define the specific actions to be performed. These activities are triggered by events or conditions within the system and can interact with various system components or external services.","title":"Overview"},{"location":"01_Write_Domain/03_automations/#activity-types","text":"","title":"Activity Types"},{"location":"01_Write_Domain/03_automations/#identity-and-access-management-iam-aws-cognito-activities","text":"create-iam-group Description : Creates a new IAM group within the system. Use Case : Used when a new group of users with specific permissions needs to be created. delete-iam-group Description : Deletes an existing IAM group. Use Case : Used when an IAM group is no longer needed and should be removed. add-user-to-iam-group Description : Adds a user to an IAM group. Use Case : Used to grant a user the permissions associated with the IAM group. remove-user-from-iam-group Description : Removes a user from an IAM group. Use Case : Used to revoke a user's permissions associated with the IAM group. retrieve-email-from-iam Description : Retrieves a user's email address from the IAM system. Use Case : Used to get the email address of a user for communication purposes. iam-create-systemuser Description : Creates a system user in the IAM system. Use Case : Used to create a non-human user that interacts with the system programmatically. iam-create-user Description : Creates a new user in the IAM system. Use Case : Used for onboarding new users. iam-delete-user Description : Deletes a user from the IAM system. Use Case : Used when a user account needs to be permanently removed.","title":"Identity and Access Management (IAM - AWS Cognito) Activities"},{"location":"01_Write_Domain/03_automations/#communication-and-notification-activities","text":"render-template Description : Renders a template with specified data. Use Case : Used to create personalized messages or documents. send-email Description : Sends an email to specified recipients. (AWS SES) Use Case : Used for sending notifications, alerts, or other communications. send-graphql-notification Description : Sends a notification via the GraphQL endpoint. Use Case : Used for notifying clients or systems through the GraphQL API.","title":"Communication and Notification Activities"},{"location":"01_Write_Domain/03_automations/#file-and-data-operations","text":"write-file Description : Writes data to a file. Use Case : Used to save data in a remote file system (AWS S3). fetch-property Description : Fetches a specific property or data point from the system. Use Case : Used to retrieve configuration settings or other data.","title":"File and Data Operations"},{"location":"01_Write_Domain/03_automations/#token-management-activities","text":"get-token Description : Retrieves a token for authentication or authorization purposes. Use Case : Used to get tokens required for accessing secured resources. get-systemuser-token Description : Retrieves a token for a system user. Use Case : Used for system-to-system authentication.","title":"Token Management Activities"},{"location":"01_Write_Domain/03_automations/#variable-and-state-management","text":"set-variable Description : Sets a flow variable to a specified value. Use Case : Used to store temporary data or state within an automation.","title":"Variable and State Management"},{"location":"01_Write_Domain/03_automations/#api-and-external-service-interactions","text":"call-internal-api Description : Calls an internal GraphQL API endpoint. Use Case : Used to invoke internal system functions (Commands/Queries/Projections). HTTP Description : Makes an HTTP request to an external service. Use Case : Used to interact with external APIs or services.","title":"API and External Service Interactions"},{"location":"01_Write_Domain/03_automations/#miscellaneous-activities","text":"code Description : Executes custom code. Use Case : Used for complex logic that cannot be handled by predefined activities. invalidate-cdn Description : Invalidates a CDN cache. Use Case : Used to ensure that updated content is served from the AWS CloudFront CDN. loop Description : Repeats a set of activities a specified number of times. Use Case : Used for iterating over a collection of items or repeating an action multiple times.","title":"Miscellaneous Activities"},{"location":"01_Write_Domain/03_automations/#summary","text":"Automations leverage a diverse set of activity types to perform various tasks automatically. By combining these activities, systems can create sophisticated workflows that enhance functionality, improve efficiency, and provide timely responses to events and conditions. Understanding these activity types is essential for designing effective automations that meet specific business needs.","title":"Summary"},{"location":"02_View_Domain/","text":"About # Overview # The View domain within the architecture serves the primary function of providing optimized and tailored data access for querying purposes. It complements the write operations handled by the Command domain by offering a structured approach to retrieving and presenting data from the system. This separation ensures that querying operations do not impact the integrity or performance of the write operations, facilitating efficient data retrieval and presentation. Key Concepts # Projection of Truth : Views in the architecture are designed to project the truth of the system's state into optimized read models. While the command side focuses on recording domain events and maintaining the system's state, the view side is concerned with interpreting this state for query purposes. Materialized Views : These are pre-computed views optimized for specific querying needs. Materialized views store aggregated or transformed data in a way that enhances query performance, enabling faster retrieval compared to recalculating data on the fly. Decoupling of Models : The View domain decouples the read model from the write model. This separation allows flexibility in how data is structured and queried, accommodating different perspectives and needs without affecting the core data integrity. Functional Aspects # Query Execution : Views facilitate efficient querying by providing access to materialized views and optimized data structures. This capability supports complex querying scenarios, including aggregations, filtering, and combining data from different sources. Functional Keys : Unlike technical keys used in the write model, views often employ functional keys that can evolve over time to meet changing business requirements. This flexibility allows the system to adapt to new data access patterns without disrupting existing operations. Just in time Projection : The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results. Benefits # Performance Optimization : By pre-computing and optimizing data for specific queries, views enhance overall system performance by reducing query latency and resource consumption. Flexibility and Adaptability : The decoupling of read and write models allows for independent evolution of data access patterns and query optimizations, ensuring that the system can adapt to new requirements without extensive refactoring. Improved Scalability : Separating read operations from write operations improves scalability by distributing the workload more efficiently across different components or services. Our implementation relies on AWS AppSync to Conclusion # In conclusion, the View domain plays a crucial role in the architecture by providing efficient, optimized, and flexible data access for querying purposes. By leveraging materialized views, decoupled models, and optimized query execution, the View domain ensures that querying operations are performant, scalable, and adaptable to evolving business needs.","title":"About"},{"location":"02_View_Domain/#about","text":"","title":"About"},{"location":"02_View_Domain/#overview","text":"The View domain within the architecture serves the primary function of providing optimized and tailored data access for querying purposes. It complements the write operations handled by the Command domain by offering a structured approach to retrieving and presenting data from the system. This separation ensures that querying operations do not impact the integrity or performance of the write operations, facilitating efficient data retrieval and presentation.","title":"Overview"},{"location":"02_View_Domain/#key-concepts","text":"Projection of Truth : Views in the architecture are designed to project the truth of the system's state into optimized read models. While the command side focuses on recording domain events and maintaining the system's state, the view side is concerned with interpreting this state for query purposes. Materialized Views : These are pre-computed views optimized for specific querying needs. Materialized views store aggregated or transformed data in a way that enhances query performance, enabling faster retrieval compared to recalculating data on the fly. Decoupling of Models : The View domain decouples the read model from the write model. This separation allows flexibility in how data is structured and queried, accommodating different perspectives and needs without affecting the core data integrity.","title":"Key Concepts"},{"location":"02_View_Domain/#functional-aspects","text":"Query Execution : Views facilitate efficient querying by providing access to materialized views and optimized data structures. This capability supports complex querying scenarios, including aggregations, filtering, and combining data from different sources. Functional Keys : Unlike technical keys used in the write model, views often employ functional keys that can evolve over time to meet changing business requirements. This flexibility allows the system to adapt to new data access patterns without disrupting existing operations. Just in time Projection : The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results.","title":"Functional Aspects"},{"location":"02_View_Domain/#benefits","text":"Performance Optimization : By pre-computing and optimizing data for specific queries, views enhance overall system performance by reducing query latency and resource consumption. Flexibility and Adaptability : The decoupling of read and write models allows for independent evolution of data access patterns and query optimizations, ensuring that the system can adapt to new requirements without extensive refactoring. Improved Scalability : Separating read operations from write operations improves scalability by distributing the workload more efficiently across different components or services. Our implementation relies on AWS AppSync to","title":"Benefits"},{"location":"02_View_Domain/#conclusion","text":"In conclusion, the View domain plays a crucial role in the architecture by providing efficient, optimized, and flexible data access for querying purposes. By leveraging materialized views, decoupled models, and optimized query execution, the View domain ensures that querying operations are performant, scalable, and adaptable to evolving business needs.","title":"Conclusion"},{"location":"02_View_Domain/01_views/","text":"Views # Views are defined entities that represent a set of attributes and may contain transformation of data from underlying aggregates (data sources). Each view is named and configured with specific parameters. Fields # Views represent collections of documents, fields are part of the document schema. Fields are configured with a name and a type (String, Int, Float, Boolean, StringList). Relations # The document schema of a view may have relations to other views. These may be OneToOne, OneToMany, ManyToOne, and ManyToMany. A special type of relation is the ObjectList. While the other relations point to other documents in the view store, ObjectList represents a nested object in the document. Relations are part of the document schema of a view and have a name, relation type, and referenced view-model as minimal configuration. Depending on the relation type, it may be mandatory to configure the foreign key (this may be a canonical key, an advanced feature explained later). Optionally, it is possible to require a specific role (authorization) to access the relational data. The default authorization method is inherit from the API query , meaning if the client has access to the view, they have access to the relational data. Business Key # One of the fields may be appointed to serve as the business key (also called the primary key). The field that is used as the business key must be of the type String . Only when a business key is appointed, the view can be stored in the view-store as a root document. Only root documents may have queries exposed in the GraphQL API. If no business key is appointed, the view model can only serve as ObjectList relations in other views. Data Mapping # Views with an appointed business key need to be filled with data published by aggregates. There are two types of event-handlers that are configured to listen for snapshot-events from a specific aggregate. A view may have multiple event handlers observing different aggregates or multiple handlers observing the same aggregate. This is useful when the aggregate instance has its business key updated, allowing one handler to create a new instance with the new key and another handler to remove the old view instance. A snapshot-event is streamed to the view-store every time an aggregate emits a domain-event. Mapper # The first type of event-handler is a simple mapper. It enables you to map aggregate fields and collections to view-fields and ObjectList relations. Additionally, you must configure which aggregate field (String) serves as the business-key of the view-instance. Note that the business-key for the view may differ from the business-key from the aggregate. Furthermore, you can select a processing strategy, either item or dictionary . Item : The mapping is executed once for every aggregate snapshot. Dictionary : In this case, you select one of the aggregate's nested collections to be processed. This is useful when a nested collection in an aggregate must be autonomously queryable. Additionally, you can configure multiple delete-expressions, causing deletion of the view instance when any of the expressions evaluate to true. Custom Mapper # Instead of relying on the standard mapping, you can use Python code to convert aggregate data into the view instance. This allows for advanced use cases like pre-filtering and enrichment beyond the capabilities of the standard mapper. For example, you could create multilevel nesting in the document (ObjectList relation) which the standard mapper only provides for one level. However, for maintainability, simpler is usually better. GraphQL Queries # Queries associated with views define access patterns for retrieving data via GraphQL. The GraphQL namespace is configured at the view level, meaning all queries in a specific view are bundled in the same namespace. Each query has a unique name in this namespace, and the needed access control method can be configured: anonymous : Requires an API key that can be configured in the client. Because this key may become publicly available, the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. user : Requires a predefined query parameter to match the signed-in user, making the data accessible only to the current signed-in user. role-based : Requires the user to have a specific role to access the data. There are three types of queries: get : Returns one instance of the view based on the business key. This type can be configured once per view. filter : Returns a list of instances based on defined filter clauses, e.g., field x equals a query parameter . During modeling, the possible filter clauses are defined. Filters may also provide canonical search, making the key_begins_with query parameter mandatory. This type can be configured only once per view. named query : Similar to the filter query but with a configurable name during modeling, and the filter clauses are mandatory for the client. Data Retention and Exclusion # Views may specify: Data Retention : Period for which data within the view remains accessible. Defined in days, the default is -1, translating to indefinitely. Notification Exclusion : By default, the view store will publish a GraphQL notification to which clients can subscribe. The notification contains the view name and the primary key, allowing the client to refetch the data when an update occurs. Subscriptions to the notification may be anonymous. This option prevents the publication of this notification, e.g., because the business key contains identifiable data like a username or an email address. Benefits # Data Consistency : Views provide consistent and up-to-date representations of data entities. Customization : Custom handlers enable tailored logic and functionalities within views. Integration : Seamless integration with underlying aggregates ensures data accuracy and synchronization. Security : Authorization controls ensure data access is restricted as per defined policies.","title":"Views"},{"location":"02_View_Domain/01_views/#views","text":"Views are defined entities that represent a set of attributes and may contain transformation of data from underlying aggregates (data sources). Each view is named and configured with specific parameters.","title":"Views"},{"location":"02_View_Domain/01_views/#fields","text":"Views represent collections of documents, fields are part of the document schema. Fields are configured with a name and a type (String, Int, Float, Boolean, StringList).","title":"Fields"},{"location":"02_View_Domain/01_views/#relations","text":"The document schema of a view may have relations to other views. These may be OneToOne, OneToMany, ManyToOne, and ManyToMany. A special type of relation is the ObjectList. While the other relations point to other documents in the view store, ObjectList represents a nested object in the document. Relations are part of the document schema of a view and have a name, relation type, and referenced view-model as minimal configuration. Depending on the relation type, it may be mandatory to configure the foreign key (this may be a canonical key, an advanced feature explained later). Optionally, it is possible to require a specific role (authorization) to access the relational data. The default authorization method is inherit from the API query , meaning if the client has access to the view, they have access to the relational data.","title":"Relations"},{"location":"02_View_Domain/01_views/#business-key","text":"One of the fields may be appointed to serve as the business key (also called the primary key). The field that is used as the business key must be of the type String . Only when a business key is appointed, the view can be stored in the view-store as a root document. Only root documents may have queries exposed in the GraphQL API. If no business key is appointed, the view model can only serve as ObjectList relations in other views.","title":"Business Key"},{"location":"02_View_Domain/01_views/#data-mapping","text":"Views with an appointed business key need to be filled with data published by aggregates. There are two types of event-handlers that are configured to listen for snapshot-events from a specific aggregate. A view may have multiple event handlers observing different aggregates or multiple handlers observing the same aggregate. This is useful when the aggregate instance has its business key updated, allowing one handler to create a new instance with the new key and another handler to remove the old view instance. A snapshot-event is streamed to the view-store every time an aggregate emits a domain-event.","title":"Data Mapping"},{"location":"02_View_Domain/01_views/#mapper","text":"The first type of event-handler is a simple mapper. It enables you to map aggregate fields and collections to view-fields and ObjectList relations. Additionally, you must configure which aggregate field (String) serves as the business-key of the view-instance. Note that the business-key for the view may differ from the business-key from the aggregate. Furthermore, you can select a processing strategy, either item or dictionary . Item : The mapping is executed once for every aggregate snapshot. Dictionary : In this case, you select one of the aggregate's nested collections to be processed. This is useful when a nested collection in an aggregate must be autonomously queryable. Additionally, you can configure multiple delete-expressions, causing deletion of the view instance when any of the expressions evaluate to true.","title":"Mapper"},{"location":"02_View_Domain/01_views/#custom-mapper","text":"Instead of relying on the standard mapping, you can use Python code to convert aggregate data into the view instance. This allows for advanced use cases like pre-filtering and enrichment beyond the capabilities of the standard mapper. For example, you could create multilevel nesting in the document (ObjectList relation) which the standard mapper only provides for one level. However, for maintainability, simpler is usually better.","title":"Custom Mapper"},{"location":"02_View_Domain/01_views/#graphql-queries","text":"Queries associated with views define access patterns for retrieving data via GraphQL. The GraphQL namespace is configured at the view level, meaning all queries in a specific view are bundled in the same namespace. Each query has a unique name in this namespace, and the needed access control method can be configured: anonymous : Requires an API key that can be configured in the client. Because this key may become publicly available, the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. user : Requires a predefined query parameter to match the signed-in user, making the data accessible only to the current signed-in user. role-based : Requires the user to have a specific role to access the data. There are three types of queries: get : Returns one instance of the view based on the business key. This type can be configured once per view. filter : Returns a list of instances based on defined filter clauses, e.g., field x equals a query parameter . During modeling, the possible filter clauses are defined. Filters may also provide canonical search, making the key_begins_with query parameter mandatory. This type can be configured only once per view. named query : Similar to the filter query but with a configurable name during modeling, and the filter clauses are mandatory for the client.","title":"GraphQL Queries"},{"location":"02_View_Domain/01_views/#data-retention-and-exclusion","text":"Views may specify: Data Retention : Period for which data within the view remains accessible. Defined in days, the default is -1, translating to indefinitely. Notification Exclusion : By default, the view store will publish a GraphQL notification to which clients can subscribe. The notification contains the view name and the primary key, allowing the client to refetch the data when an update occurs. Subscriptions to the notification may be anonymous. This option prevents the publication of this notification, e.g., because the business key contains identifiable data like a username or an email address.","title":"Data Retention and Exclusion"},{"location":"02_View_Domain/01_views/#benefits","text":"Data Consistency : Views provide consistent and up-to-date representations of data entities. Customization : Custom handlers enable tailored logic and functionalities within views. Integration : Seamless integration with underlying aggregates ensures data accuracy and synchronization. Security : Authorization controls ensure data access is restricted as per defined policies.","title":"Benefits"},{"location":"02_View_Domain/02_projections/","text":"Projections # The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results. API Configuration # To configure a projection, several key elements must be defined: Projection Name # A unique identifier for the projection, which helps in managing and referencing the projection. GraphQL Namespace # The namespace under which the projection's GraphQL queries will be bundled. This helps in organizing queries logically. GraphQL Method Name # The method name used to invoke the projection via GraphQL. This name should be unique within the specified namespace. Authorization Method # Specifies the access control for the projection. The following methods are supported: anonymous : Requires an API key that can be configured in the client. This key may be publicly available, and the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. role based : Requires the user to have a specific role to access the data. Return Object # References a view model that defines the structure of the data returned by the projection. Return Type # Specifies whether the projection returns a single item or a result set. The possible values are: item : Returns a single instance of the view. result_set : Returns a list of instances. Arguments # Projections can accept various query variables, which are specified as follows: Name # The name of the query variable. Type # The data type of the query variable. Supported types include: String Int Float Boolean StringList Required # Specifies whether the query variable is mandatory for the projection to execute. The possible values are: true false Data Preparation Logic # The core logic for preparing data in a projection is written in Python. This logic involves accessing materialized views, applying filters, and transforming data as needed. The following example demonstrates how to implement the data preparation logic: from draftsman.ViewStoreApi import Query def transform(arguments, username): # You have access to the username of the requestor and the arguments. print(f\"Handle graph request [{arguments}/{username}]\") # You have access to a fluent API to access materialized views # Here are some examples: # Access a specific object (most efficient method cost-wise) data = Query('ViewName').get_item(\"key\").run() # Get a list of all objects of a specific type data = Query('ViewName').get_items().run() # Filter a list of objects based on type and key, part of the canonical key concept data = Query('ViewName').get_items(\"key_starts_with_this_value\").run() # Filter on content for a specific type (performs a scan on all views of type 'ViewName') data = Query('ViewName').get_items().equals('key', 'value').between('key', 0, 100).run() # Combine the two filter methods to filter within a subset data = Query('ViewName').get_items(\"key_starts_with_this_value\").equals('key', 'value').between('key', 0, 100).run() # Switch to an \"or\" operator for filters (default is \"and\") data = Query('ViewName').get_items(key=\"key_starts_with_this_value\", filter_chain_method=\"or\").equals('key', 'value').between( 'key', 0, 100).run() # Program data transformations with Python # Ensure you add all fields that are defined in the return view object definition return {\"field_name\": \"value\"} Fluent API # The Fluent API, which allows you to query and filter data from the DynamoDB view store in a flexible and efficient manner. The Fluent API provides a convenient way to construct and execute queries against the DynamoDB view-store-table. It supports various filtering methods and allows for the retrieval of individual items or sets of items based on specified criteria. Initialization # To start using the Fluent API, you need to initialize the Query class with the type of view you want to query. from draftsman.ViewStoreApi import Query # Initialize a query for a specific view name query = Query(\"YourViewName\") Methods # Retrieving Items # get_item # Retrieve a single item based on its key. query.get_item(\"your_item_key\") get_items # Retrieve a set of items, optionally filtered by a key prefix. query.get_items(key=\"key_prefix\", filter_chain_method=\"and\") key (optional): A prefix to filter items by their keys. filter_chain_method (optional): Determines how multiple filters are combined. Can be \"and\" or \"or\". Default is \" and\". Adding Filters # Filters can be applied to the query to narrow down the results. The following filter methods are available: equals # Filter items where the attribute equals a specified value. query.equals(\"attribute_name\", \"value\") less_than # Filter items where the attribute is less than a specified value. query.less_than(\"attribute_name\", value) less_than_equals # Filter items where the attribute is less than or equal to a specified value. query.less_than_equals(\"attribute_name\", value) greater_than # Filter items where the attribute is greater than a specified value. query.greater_than(\"attribute_name\", value) greater_than_equals # Filter items where the attribute is greater than or equal to a specified value. query.greater_than_equals(\"attribute_name\", value) begins_with # Filter items where the attribute begins with a specified value. query.begins_with(\"attribute_name\", \"value_prefix\") between # Filter items where the attribute is between two specified values. query.between(\"attribute_name\", low_value, high_value) not_equals # Filter items where the attribute does not equal a specified value. query.not_equals(\"attribute_name\", \"value\") exists # Filter items where the attribute exists. query.exists(\"attribute_name\") not_exists # Filter items where the attribute does not exist. query.not_exists(\"attribute_name\") contains # Filter items where the attribute contains a specified value. query.contains(\"attribute_name\", \"value\") attribute_type # Filter items where the attribute is of a specified type. query.attribute_type(\"attribute_name\", \"type\") is_in # Filter items where the attribute is in a specified list of values. query.is_in(\"attribute_name\", [\"value1\", \"value2\", \"value3\"]) Running the Query # Once the query is constructed with the necessary filters, it can be executed using the run method. result = query.run() If get_item was used, run returns a single item. If get_items was used, run returns a list of items matching the query. Method chaining # The Fluent API supports method chaining, allowing you to construct complex queries in a readable and concise manner by chaining multiple method calls together. Method chaining enhances the clarity of query construction and reduces the need for intermediate variables. resultset = query.get_items(\"key_prefix\").equals(\"status\", \"active\").greater_than(\"priority\", 1).equals(\"user\",\"j.doe\").run()","title":"Projections"},{"location":"02_View_Domain/02_projections/#projections","text":"The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results.","title":"Projections"},{"location":"02_View_Domain/02_projections/#api-configuration","text":"To configure a projection, several key elements must be defined:","title":"API Configuration"},{"location":"02_View_Domain/02_projections/#projection-name","text":"A unique identifier for the projection, which helps in managing and referencing the projection.","title":"Projection Name"},{"location":"02_View_Domain/02_projections/#graphql-namespace","text":"The namespace under which the projection's GraphQL queries will be bundled. This helps in organizing queries logically.","title":"GraphQL Namespace"},{"location":"02_View_Domain/02_projections/#graphql-method-name","text":"The method name used to invoke the projection via GraphQL. This name should be unique within the specified namespace.","title":"GraphQL Method Name"},{"location":"02_View_Domain/02_projections/#authorization-method","text":"Specifies the access control for the projection. The following methods are supported: anonymous : Requires an API key that can be configured in the client. This key may be publicly available, and the API may rate limit these requests to prevent DDOS attacks. authenticated : Requires a user token to access the data. role based : Requires the user to have a specific role to access the data.","title":"Authorization Method"},{"location":"02_View_Domain/02_projections/#return-object","text":"References a view model that defines the structure of the data returned by the projection.","title":"Return Object"},{"location":"02_View_Domain/02_projections/#return-type","text":"Specifies whether the projection returns a single item or a result set. The possible values are: item : Returns a single instance of the view. result_set : Returns a list of instances.","title":"Return Type"},{"location":"02_View_Domain/02_projections/#arguments","text":"Projections can accept various query variables, which are specified as follows:","title":"Arguments"},{"location":"02_View_Domain/02_projections/#name","text":"The name of the query variable.","title":"Name"},{"location":"02_View_Domain/02_projections/#type","text":"The data type of the query variable. Supported types include: String Int Float Boolean StringList","title":"Type"},{"location":"02_View_Domain/02_projections/#required","text":"Specifies whether the query variable is mandatory for the projection to execute. The possible values are: true false","title":"Required"},{"location":"02_View_Domain/02_projections/#data-preparation-logic","text":"The core logic for preparing data in a projection is written in Python. This logic involves accessing materialized views, applying filters, and transforming data as needed. The following example demonstrates how to implement the data preparation logic: from draftsman.ViewStoreApi import Query def transform(arguments, username): # You have access to the username of the requestor and the arguments. print(f\"Handle graph request [{arguments}/{username}]\") # You have access to a fluent API to access materialized views # Here are some examples: # Access a specific object (most efficient method cost-wise) data = Query('ViewName').get_item(\"key\").run() # Get a list of all objects of a specific type data = Query('ViewName').get_items().run() # Filter a list of objects based on type and key, part of the canonical key concept data = Query('ViewName').get_items(\"key_starts_with_this_value\").run() # Filter on content for a specific type (performs a scan on all views of type 'ViewName') data = Query('ViewName').get_items().equals('key', 'value').between('key', 0, 100).run() # Combine the two filter methods to filter within a subset data = Query('ViewName').get_items(\"key_starts_with_this_value\").equals('key', 'value').between('key', 0, 100).run() # Switch to an \"or\" operator for filters (default is \"and\") data = Query('ViewName').get_items(key=\"key_starts_with_this_value\", filter_chain_method=\"or\").equals('key', 'value').between( 'key', 0, 100).run() # Program data transformations with Python # Ensure you add all fields that are defined in the return view object definition return {\"field_name\": \"value\"}","title":"Data Preparation Logic"},{"location":"02_View_Domain/02_projections/#fluent-api","text":"The Fluent API, which allows you to query and filter data from the DynamoDB view store in a flexible and efficient manner. The Fluent API provides a convenient way to construct and execute queries against the DynamoDB view-store-table. It supports various filtering methods and allows for the retrieval of individual items or sets of items based on specified criteria.","title":"Fluent API"},{"location":"02_View_Domain/02_projections/#initialization","text":"To start using the Fluent API, you need to initialize the Query class with the type of view you want to query. from draftsman.ViewStoreApi import Query # Initialize a query for a specific view name query = Query(\"YourViewName\")","title":"Initialization"},{"location":"02_View_Domain/02_projections/#methods","text":"","title":"Methods"},{"location":"02_View_Domain/02_projections/#retrieving-items","text":"","title":"Retrieving Items"},{"location":"02_View_Domain/02_projections/#get_item","text":"Retrieve a single item based on its key. query.get_item(\"your_item_key\")","title":"get_item"},{"location":"02_View_Domain/02_projections/#get_items","text":"Retrieve a set of items, optionally filtered by a key prefix. query.get_items(key=\"key_prefix\", filter_chain_method=\"and\") key (optional): A prefix to filter items by their keys. filter_chain_method (optional): Determines how multiple filters are combined. Can be \"and\" or \"or\". Default is \" and\".","title":"get_items"},{"location":"02_View_Domain/02_projections/#adding-filters","text":"Filters can be applied to the query to narrow down the results. The following filter methods are available:","title":"Adding Filters"},{"location":"02_View_Domain/02_projections/#equals","text":"Filter items where the attribute equals a specified value. query.equals(\"attribute_name\", \"value\")","title":"equals"},{"location":"02_View_Domain/02_projections/#less_than","text":"Filter items where the attribute is less than a specified value. query.less_than(\"attribute_name\", value)","title":"less_than"},{"location":"02_View_Domain/02_projections/#less_than_equals","text":"Filter items where the attribute is less than or equal to a specified value. query.less_than_equals(\"attribute_name\", value)","title":"less_than_equals"},{"location":"02_View_Domain/02_projections/#greater_than","text":"Filter items where the attribute is greater than a specified value. query.greater_than(\"attribute_name\", value)","title":"greater_than"},{"location":"02_View_Domain/02_projections/#greater_than_equals","text":"Filter items where the attribute is greater than or equal to a specified value. query.greater_than_equals(\"attribute_name\", value)","title":"greater_than_equals"},{"location":"02_View_Domain/02_projections/#begins_with","text":"Filter items where the attribute begins with a specified value. query.begins_with(\"attribute_name\", \"value_prefix\")","title":"begins_with"},{"location":"02_View_Domain/02_projections/#between","text":"Filter items where the attribute is between two specified values. query.between(\"attribute_name\", low_value, high_value)","title":"between"},{"location":"02_View_Domain/02_projections/#not_equals","text":"Filter items where the attribute does not equal a specified value. query.not_equals(\"attribute_name\", \"value\")","title":"not_equals"},{"location":"02_View_Domain/02_projections/#exists","text":"Filter items where the attribute exists. query.exists(\"attribute_name\")","title":"exists"},{"location":"02_View_Domain/02_projections/#not_exists","text":"Filter items where the attribute does not exist. query.not_exists(\"attribute_name\")","title":"not_exists"},{"location":"02_View_Domain/02_projections/#contains","text":"Filter items where the attribute contains a specified value. query.contains(\"attribute_name\", \"value\")","title":"contains"},{"location":"02_View_Domain/02_projections/#attribute_type","text":"Filter items where the attribute is of a specified type. query.attribute_type(\"attribute_name\", \"type\")","title":"attribute_type"},{"location":"02_View_Domain/02_projections/#is_in","text":"Filter items where the attribute is in a specified list of values. query.is_in(\"attribute_name\", [\"value1\", \"value2\", \"value3\"])","title":"is_in"},{"location":"02_View_Domain/02_projections/#running-the-query","text":"Once the query is constructed with the necessary filters, it can be executed using the run method. result = query.run() If get_item was used, run returns a single item. If get_items was used, run returns a list of items matching the query.","title":"Running the Query"},{"location":"02_View_Domain/02_projections/#method-chaining","text":"The Fluent API supports method chaining, allowing you to construct complex queries in a readable and concise manner by chaining multiple method calls together. Method chaining enhances the clarity of query construction and reduces the need for intermediate variables. resultset = query.get_items(\"key_prefix\").equals(\"status\", \"active\").greater_than(\"priority\", 1).equals(\"user\",\"j.doe\").run()","title":"Method chaining"},{"location":"03_Utils/","text":"Globaly available concepts # Utils are globally available concepts that are modeled once and used across the system. They include: Expressions # Expressions are reusable logical constructs used to transform data on-the-fly before executing logic. There are two types of expressions: API Authorization Expression # These expressions are used in models that define an access path in the GraphQL API. They convert query variables to specific roles, useful in multi-tenant systems where each tenant has a private set of roles. Trigger Key Expression # The key expression is usable in all identity-based triggers: Aggregate behavior flows & View data sources. It is used to convert an event attribute into a functional key. Dependencies # Allows you to add pip packages to you runtime, the package will be available for importing in all custom python code in your project. Patterns # Patterns are regular expressions that can be used while modelling commands to ensure a specific pattern for String fields e.g. LowercasedOnly # ^[a-z]+$ Date # ^(?:20)\\d{2}-\\d{2}-\\d{2}$ Extending patters # Patterns may reference each-other e.g. ^{{LowercasedOnly}}:arn:{{LowercasedOnly}}$ Roles # Simply a list of role names available in the modeler, at this moment these roles are not automatically loaded into the the IAM system by Draftsman. The application owner needs to define these roles manually, or model an automation (@afterDeployment) that creates these roles. Python modules # Python modules are usable scripts that define methods that are accessible from: Behavior flows Automations def behavior_or_notifier_function(flow): # May set a flow variable flow.myVariable = \"Hello World!\" # And has access to flow variables print(flow.myVariable) # And has also access to the aggregate document print(flow.entity) print(flow.entity.entityField) It is a simple function but it does provide you access to all variables available to the flow execution.","title":"Globaly available concepts"},{"location":"03_Utils/#globaly-available-concepts","text":"Utils are globally available concepts that are modeled once and used across the system. They include:","title":"Globaly available concepts"},{"location":"03_Utils/#expressions","text":"Expressions are reusable logical constructs used to transform data on-the-fly before executing logic. There are two types of expressions:","title":"Expressions"},{"location":"03_Utils/#api-authorization-expression","text":"These expressions are used in models that define an access path in the GraphQL API. They convert query variables to specific roles, useful in multi-tenant systems where each tenant has a private set of roles.","title":"API Authorization Expression"},{"location":"03_Utils/#trigger-key-expression","text":"The key expression is usable in all identity-based triggers: Aggregate behavior flows & View data sources. It is used to convert an event attribute into a functional key.","title":"Trigger Key Expression"},{"location":"03_Utils/#dependencies","text":"Allows you to add pip packages to you runtime, the package will be available for importing in all custom python code in your project.","title":"Dependencies"},{"location":"03_Utils/#patterns","text":"Patterns are regular expressions that can be used while modelling commands to ensure a specific pattern for String fields e.g.","title":"Patterns"},{"location":"03_Utils/#lowercasedonly","text":"^[a-z]+$","title":"LowercasedOnly"},{"location":"03_Utils/#date","text":"^(?:20)\\d{2}-\\d{2}-\\d{2}$","title":"Date"},{"location":"03_Utils/#extending-patters","text":"Patterns may reference each-other e.g. ^{{LowercasedOnly}}:arn:{{LowercasedOnly}}$","title":"Extending patters"},{"location":"03_Utils/#roles","text":"Simply a list of role names available in the modeler, at this moment these roles are not automatically loaded into the the IAM system by Draftsman. The application owner needs to define these roles manually, or model an automation (@afterDeployment) that creates these roles.","title":"Roles"},{"location":"03_Utils/#python-modules","text":"Python modules are usable scripts that define methods that are accessible from: Behavior flows Automations def behavior_or_notifier_function(flow): # May set a flow variable flow.myVariable = \"Hello World!\" # And has access to flow variables print(flow.myVariable) # And has also access to the aggregate document print(flow.entity) print(flow.entity.entityField) It is a simple function but it does provide you access to all variables available to the flow execution.","title":"Python modules"},{"location":"03_Utils/01_expressions/","text":"Expressions # API Authorization Expression # The authorization expression can be used in all components exposed in the API: Commands View queries It is used to convert an input parameter (command field or query filter field, e.g., key, key_begins_with, or a custom filter attribute) to a technical role that the API resolver will validate to determine if the requester has the specific role. This is useful for providing role-based access in a multi-tenant system. The expression has a name , e.g., extractRoleFromArn , which is used to access the expression from command models or view models. You model inputs for this function separated with a ; e.g., arn;role And then use Velocity Template Language (VTL) syntax with basic JavaScript to model the logic: ${arn.split(':')[0]}:${arn.split(':')[1]}:role In a command or view query, you can use this expression in the role field when you select role-based access: #global.extractRoleFromArn(key, 'viewer') When you execute, for example, a query where this is implemented, it will evaluate to: #foreach($group in $context.identity.claims.get(\"cognito:groups\")) #if($group == \"${ctx.args.key.split(':')[0]}:${ctx.args.key.split(':')[1]}:viewer\") #set($inCognitoGroup = true) #end #end #if($inCognitoGroup){ \"version\": \"2018-05-29\", \"operation\": \"GetItem\", \"key\": { \"type\": $util.dynamodb.toDynamoDBJson(\"ViewName\"), \"key\": $util.dynamodb.toDynamoDBJson($ctx.args.key) } } #else $utils.unauthorized() #end Trigger Key Expression # The key expression is usable in all identity-based triggers: Aggregate behavior flows View data sources It is used to convert an event attribute into a functional key. The name of the expression, e.g., truncateArn , is used to access this expression from a behavior or data-source model. You can model input for the expression, e.g., arn;length . The expression itself is written in pure Python, e.g., ':'.join(arn.split(':')[:int(length)]) You can access it by configuring a method call inside the key field of behavior or data-source. The input parameters will reference a trigger attribute. You can use literals, but they can only be strings. In our case, the expression will evaluate the string to an integer. #global.truncateArn(childArn, '2') Let's say that the trigger contains an attribute childArn=draftsmanid:workspace:project , then the resulting key for the flow will be draftsmanid:workspace .","title":"Expressions"},{"location":"03_Utils/01_expressions/#expressions","text":"","title":"Expressions"},{"location":"03_Utils/01_expressions/#api-authorization-expression","text":"The authorization expression can be used in all components exposed in the API: Commands View queries It is used to convert an input parameter (command field or query filter field, e.g., key, key_begins_with, or a custom filter attribute) to a technical role that the API resolver will validate to determine if the requester has the specific role. This is useful for providing role-based access in a multi-tenant system. The expression has a name , e.g., extractRoleFromArn , which is used to access the expression from command models or view models. You model inputs for this function separated with a ; e.g., arn;role And then use Velocity Template Language (VTL) syntax with basic JavaScript to model the logic: ${arn.split(':')[0]}:${arn.split(':')[1]}:role In a command or view query, you can use this expression in the role field when you select role-based access: #global.extractRoleFromArn(key, 'viewer') When you execute, for example, a query where this is implemented, it will evaluate to: #foreach($group in $context.identity.claims.get(\"cognito:groups\")) #if($group == \"${ctx.args.key.split(':')[0]}:${ctx.args.key.split(':')[1]}:viewer\") #set($inCognitoGroup = true) #end #end #if($inCognitoGroup){ \"version\": \"2018-05-29\", \"operation\": \"GetItem\", \"key\": { \"type\": $util.dynamodb.toDynamoDBJson(\"ViewName\"), \"key\": $util.dynamodb.toDynamoDBJson($ctx.args.key) } } #else $utils.unauthorized() #end","title":"API Authorization Expression"},{"location":"03_Utils/01_expressions/#trigger-key-expression","text":"The key expression is usable in all identity-based triggers: Aggregate behavior flows View data sources It is used to convert an event attribute into a functional key. The name of the expression, e.g., truncateArn , is used to access this expression from a behavior or data-source model. You can model input for the expression, e.g., arn;length . The expression itself is written in pure Python, e.g., ':'.join(arn.split(':')[:int(length)]) You can access it by configuring a method call inside the key field of behavior or data-source. The input parameters will reference a trigger attribute. You can use literals, but they can only be strings. In our case, the expression will evaluate the string to an integer. #global.truncateArn(childArn, '2') Let's say that the trigger contains an attribute childArn=draftsmanid:workspace:project , then the resulting key for the flow will be draftsmanid:workspace .","title":"Trigger Key Expression"}]}