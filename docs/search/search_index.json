{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Introducing Tracepaper by Draftsman, a revolutionary tool designed to simplify the complexities of building business applications. While the landscape is rich with low-code tools to aid development, the challenges of managing these tools in production can be daunting. From maintaining performance to ensuring granular access control and backups, the operational demands can strain businesses lacking a robust engineering department.</p> <p>Enter serverless services offered by major cloud providers like AWS, promising to alleviate this operational burden. \" Serverless is how the cloud wants you to build applications,\" as Gregor Hohpe puts it. But is this approach practical? In practice, serverless architecture introduces a high level of runtime granularity, where each line in your domain model translates into network calls fraught with access control, latency, and security considerations. It requires a shift from operational competence to distributed system design proficiency.</p> <p>However, if your expertise lies in your domain rather than distributed systems intricacies, you likely prefer to reason about your domain holistically\u2014a \"Monolith,\" in industry terms. This is where Tracepaper shines, drawing inspiration from Domain Driven Design principles. We empower you to model your domain using distinct boxes connected by lines, representing various elements such as API access points, domain behaviors, and materialized views. Our modeling tool prioritizes data structures and the connections between these boxes, allowing you to focus on the essence of your domain.</p> <p>While we provide modeling concepts for the business logic within these boxes, the true strength lies in the ability to inject custom Python code. Transitioning from pseudo-code to Python is seamless, freeing you from concerns about the connections between your logic.</p> <p>Let us handle the lines, so you can concentrate on the boxes\u2014the core of your domain expertise. The model seamlessly converts into a Python project defined with CloudFormation, AWS's Infrastructure as Code specification. This ensures deployability within your AWS account, granting you full control over runtime parameters such as access and cost. Additionally, both the model and the generated project are stored in your GitHub account, offering complete oversight and ownership.</p> <p>With Tracepaper, we streamline the process of modeling your domain as a distributed system. Let us handle the intricacies, while you focus on what truly matters\u2014your domain.</p>"},{"location":"Concept/","title":"Why Tracepaper","text":""},{"location":"Concept/#elevate-your-development-with-tracepaper-by-draftsman","title":"Elevate Your Development with Tracepaper by Draftsman","text":"<p>Transform the way you develop, deploy, and manage your applications with Tracepaper by Draftsman\u2014a powerful modeling tool designed to streamline your workflow and enhance productivity.</p>"},{"location":"Concept/#key-features","title":"Key Features","text":"<p>1. Intuitive Web Modeler:</p> <ul> <li>Easily model your application's domain, logic, and interactions using a user-friendly interface. Simplify complex   systems with visual modeling and automatic code generation.</li> </ul> <p>2. Seamless Integration:</p> <ul> <li>Leverage seamless integration with your existing GitHub repositories and AWS environments. Maintain control of your   code and deployment processes while benefiting from automated workflows.</li> </ul> <p>3. Robust Build and Deployment Pipeline:</p> <ul> <li>Automatically convert models to Python and CloudFormation code. Conduct unit tests, package applications, and deploy   to staging and production environments with minimal manual intervention.</li> </ul> <p>4. Asynchronous Processing:</p> <ul> <li>Benefit from robust asynchronous command handling. Receive real-time updates on command status, ensuring responsive   and reliable application performance.</li> </ul> <p>5. Comprehensive Testing:</p> <ul> <li>Execute API tests to verify application behavior and data integrity before production deployment. Ensure your   application meets quality standards and performs as expected.</li> </ul> <p>6. Flexible Role-Based Access:</p> <ul> <li>Model complex multi-tenant systems with role-based access controls. Convert query variables to specific roles,   ensuring secure and tailored data access.</li> </ul> <p>7. Ownership Clarity:</p> <ul> <li>Maintain ownership of your GitHub repositories and deployment environments while utilizing Tracepaper's powerful   modeling and build capabilities managed by Draftsman.</li> </ul> <p>8. Generated GUI Assist Framework (Beta):</p> <ul> <li>Experience our new GUI Assist Framework, designed to enhance user interface development. Built with AlpineJS and   vimesh-ui, this framework offers pre-made components and easy customization. Integrate seamlessly with your GraphQL API   for a dynamic and responsive user experience.</li> </ul>"},{"location":"Concept/#why-choose-tracepaper","title":"Why Choose Tracepaper?","text":"<p>Efficiency:</p> <ul> <li>Save time with automated code generation and deployment, allowing your team to focus on innovation and quality.</li> </ul> <p>Reliability:</p> <ul> <li>Ensure consistent and error-free deployments with comprehensive testing and real-time monitoring of command   processing.</li> </ul> <p>Scalability:</p> <ul> <li>Easily scale your applications with infrastructure as code, supporting growing business needs and complex   environments.</li> </ul> <p>Collaboration:</p> <ul> <li>Facilitate collaboration between developers, domain experts, and operations teams with clear, visual models and shared   repositories.</li> </ul>"},{"location":"Concept/#get-started","title":"Get Started","text":"<p>Unlock the full potential of your development process with Tracepaper by Draftsman. Visit our website to learn more and start transforming your application development today.</p> <p>Visit Tracepaper by Draftsman.</p>"},{"location":"Concept/overview/","title":"Concept overview","text":"<p>In the diagram we mapped the relations between our concepts.</p> <p>Gray elements represent technical concepts included to provide cohesion to the overall design, while all colored sections depict modeling concepts. The red and white areas indicate custom code.</p>"},{"location":"Concept/overview/#commands","title":"Commands","text":"<p>Commands represent actions triggered by users or external systems. They encapsulate requests to perform specific operations within the system. They are part of the API model in green.</p>"},{"location":"Concept/overview/#aggregates-and-behavior-pale-red","title":"Aggregates and Behavior (pale red)","text":"<ul> <li>Event Sourcing: This approach ensures that domain events, which capture immutable truths about past actions,   remain unchanged over time. It separates the factual recording (event sourcing) from how we interpret and reason about   these events (projection).</li> <li>Technical Key: This key is immutable and crucial for maintaining consistency in the system.</li> </ul>"},{"location":"Concept/overview/#views-and-queries","title":"Views and Queries","text":"<ul> <li>Projection of Truth: While projecting the truth to execute domain logic is essential, querying involves a   different perspective. It allows deviations in the data model and supports querying based on functional keys that can   change over time (green).</li> <li>Materialized Views: These are separate read models optimized for queries. They are decoupled from the write model   and offer benefits like converting technical keys to functional ones, optimizing data aggregations, and establishing   relationships between entities that are otherwise decoupled (blue).</li> </ul>"},{"location":"Concept/overview/#automations-also-known-as-notifiers","title":"Automations (also known as Notifiers)","text":"<p>Notifiers orchestrate commands in response to domain events without maintaining state. They have the capability to invoke a wide range of web APIs, including AWS, REST, GraphQL, and the platform's own API (orange).</p>"},{"location":"Concept/overview/#projections","title":"Projections","text":"<p>Projections enhance queries by combining data from APIs, materialized views, and custom Python logic. They facilitate transformations such as calculations, advanced filtering, or generating time-sensitive attributes (tokens).</p>"},{"location":"Concept/overview/#custom-code","title":"Custom code","text":"<p>The models can be augmented by Python code (bright red) to create functionality beyond the modelling concepts.</p> <p>Advanced business logic should be implemented through programming because modeling it can add more complexity than it solves, and coding is faster for this purpose. However, it must be understandable for domain experts. Python is an ideal choice because it is widely used by academics in fields like chemistry, biology, and data science. This increases the likelihood that a domain expert can learn it, especially when we add \"liberating constraints\" from a methodology like \" tracepaper.\"</p>"},{"location":"Concept/01_Write_Domain/","title":"About the Write domain","text":""},{"location":"Concept/01_Write_Domain/#introduction","title":"Introduction","text":"<p>The Write Domain is a fundamental part of our model-driven development environment. It focuses on processing and recording data within a system. This domain encompasses all components responsible for invoking and executing behaviors that change the state of the application. The Write Domain is essential for maintaining the integrity and consistency of data.</p>"},{"location":"Concept/01_Write_Domain/#components-of-the-write-domain","title":"Components of the Write Domain","text":"<p>The Write Domain consists of several key components: Commands, Subdomains, Aggregates, Behavior Flows, Domain Events, and Automations. Each of these components plays a specific role in capturing and processing events and actions that affect the state of the application.</p>"},{"location":"Concept/01_Write_Domain/#commands","title":"Commands","text":"<p>Commands represent interactions from users or systems with the application. These events are triggered by actions of actors (such as users or external systems) and initiate changes in the application's state.</p>"},{"location":"Concept/01_Write_Domain/#subdomains","title":"Subdomains","text":"<p>Subdomains help to organize and structure the Write Domain by grouping related aggregates. This modular approach allows for better separation of concerns and clearer boundaries between different parts of the system. Each subdomain focuses on a specific aspect of the business logic, making it easier to manage and maintain.</p>"},{"location":"Concept/01_Write_Domain/#aggregates","title":"Aggregates","text":"<p>Aggregates are a fundamental concept within the Write Domain that represent a cluster of related objects treated as a single unit for data changes. Each aggregate has a document model that represents state with a defined boundary that ensures the consistency of its data changes. Aggregates encapsulate both data and behavior, providing a clear structure for managing complex business logic.</p>"},{"location":"Concept/01_Write_Domain/#behavior-flows","title":"Behavior Flows","text":"<p>Behavior Flows are the core of the Write Domain. They model a set of actions that take place after an event is received. These actions include data-validation rules or business rules, and they may alter the state of the application by publishing a domain event.</p>"},{"location":"Concept/01_Write_Domain/#domain-events","title":"Domain Events","text":"<p>Domain events represent significant occurrences within the system that reflect a change in state. They capture the essential information about what happened, providing a way to track and react to changes in the application.</p> <p>Event Handling: Domain events are used to update the state of aggregates. Event handlers process these events to apply the necessary changes and maintain consistency within the system. A domain event may also trigger other behavior flows or automations.</p>"},{"location":"Concept/01_Write_Domain/#automations","title":"Automations","text":"<p>Automations are responsible for performing specific actions when certain conditions or events occur. They are often used for system activities that need to take place in response to specific events within the Write Domain. Automations can fail silently if necessary.</p>"},{"location":"Concept/01_Write_Domain/#purpose-and-benefits-of-the-write-domain","title":"Purpose and Benefits of the Write Domain","text":"<p>The Write Domain plays a crucial role in ensuring the integrity and consistency of data within an application. By strictly separating data and behavior logic, the Write Domain offers the following benefits:</p> <ul> <li>Consistency: By encapsulating data and behavior within behavior flows and aggregates, it ensures that changes are   applied consistently and atomically.</li> <li>Traceability: Commands provide a clear and auditable trail of all actions and events that lead to changes in the   application's state.</li> <li>Maintainability: By separating different responsibilities (commands, behavior flows, aggregates, subdomains,   domain events, automations), complexity is reduced, making the code more maintainable and extendable.</li> <li>Modularity: Subdomains and aggregates provide a structured way to organize the system, promoting modularity and   reducing the risk of interdependencies that can lead to errors.</li> <li>Reactivity: Domain events enable the system to react to significant changes in state, allowing for more responsive   and dynamic behavior.</li> </ul>"},{"location":"Concept/01_Write_Domain/#conclusion","title":"Conclusion","text":"<p>The Write Domain is an essential concept within our model-driven development environment. It provides a clear structure and maintains the integrity of the application through well-defined components such as Commands, Behavior Flows, Aggregates, Subdomains, Domain Events, and Automations. By effectively using these components, we can build robust, consistent, and well-maintained applications.</p>"},{"location":"Concept/01_Write_Domain/01_commands/","title":"Commands","text":""},{"location":"Concept/01_Write_Domain/01_commands/#overview","title":"Overview","text":"<p>Commands are actions triggered by users or external systems to request the system to perform a specific operation. Commands encapsulate all the necessary information required to carry out the action, including authorization details and data fields. They are the primary means through which external entities interact with the system.</p>"},{"location":"Concept/01_Write_Domain/01_commands/#command-definition","title":"Command Definition","text":"<p>A command is defined by specifying its name, authorization requirements, and associated data fields. Commands are triggered by actors (users or external systems) and may lead to state changes within the system.</p>"},{"location":"Concept/01_Write_Domain/01_commands/#attributes-of-a-command","title":"Attributes of a Command","text":"<p>Name: A unique identifier for the command.</p> <p>Authorization: Defines who can trigger the command. This includes:</p> <ul> <li> <p>Authorization Type: Indicates if the command requires a specific role, can be triggered by any user (<code>anonymous</code>),   or requires the user to be authenticated (<code>authenticated</code>).</p> </li> <li> <p>Role: In case role-based access is selected, specifies the user role required (e.g., <code>administrator</code>).</p> </li> </ul>"},{"location":"Concept/01_Write_Domain/01_commands/#graphql-configuration","title":"GraphQL Configuration","text":"<p>Commands are exposed via a GraphQL API, allowing clients to interact with the system using standard GraphQL operations. The configuration for GraphQL exposure includes:</p> <p>GraphQL Namespace: Organizes related commands within a logical grouping.</p> <p>GraphQL Name: Defines the specific mutation name for the command.</p> <p>This configuration enables seamless integration and interaction with the system, providing a clear and structured API for external clients.</p>"},{"location":"Concept/01_Write_Domain/01_commands/#fields","title":"Fields","text":"<p>Fields represent the data required to execute the command. Each field is defined with the following attributes:</p> <p>Name: The name of the field.</p> <p>Type: The data type of the field. Possible types include:</p> <ul> <li>String: Textual data.</li> <li>Int: Integer values.</li> <li>Boolean: True/false values.</li> <li>Float: Floating-point numbers.</li> </ul> <p>Pattern: (Optional) A regular expression pattern that the field value must match (e.g., DateTime input in a String field).</p> <p>Default: (Optional) Default value for the field if none is provided. When a default is provided, the field will become optional in the GraphQL schema.</p> <p>Auto-fill: (Optional) Instructions for automatically generating the field value (e.g., <code>uuid</code> for unique identifiers or <code>username</code>). Using auto-fill will remove this field from the GraphQL schema.</p>"},{"location":"Concept/01_Write_Domain/01_commands/#nested-collections","title":"Nested Collections","text":"<p>Commands can have complex data structures, which can be represented using collections of nested objects. Nested objects allow the inclusion of sub-structures within a command, providing a way to encapsulate related data. A nested collection has a name and a set of fields.</p>"},{"location":"Concept/01_Write_Domain/01_commands/#summary","title":"Summary","text":"<p>Commands in an event-driven architecture encapsulate actions triggered by actors, specifying the necessary data and authorization requirements. By defining commands with fields, nested collections, and GraphQL configurations, systems can ensure a robust, secure, and structured interaction model for handling user and system interactions.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/","title":"Aggregates","text":"<p>An aggregate reflects a domain concept, a collection of data (state) and behavior to manipulate the data. The data is internally encapsulated, so the only way to induce a state change is by invoking behavior.</p> <p>Modeling an aggregate involves two activities:</p> <ol> <li>Modeling the data model.</li> <li>Modeling behavior on the model.</li> </ol> <p>A concept from DDD opinionated to optimize cognitive load, focusing on the concepts rather than the underlying technology. It abstracts the Command/Compute part of our CQRS architecture.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#aggregate-data-model","title":"Aggregate Data Model","text":"<p>The data model consists of two parts, the actual part: the events, and the projection: the document.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#the-document","title":"The Document","text":"<p>The document is a projection of the data, a mental model of the state so that we can implement validations in the behavior to ensure data integrity. Because it is a projection, this model can evolve over time without manipulating the underlying data. Events from the past are immutable by definition.</p> <p>Thus, the document serves both as a mental model for ourselves on how we want to think about state and as a way to model how we present data to the viewstore. It is the data contract between the aggregate and the viewstore.</p> <p>One of the document fields (String) will serve as the business key of the aggregate instance</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#the-events","title":"The Events","text":"<p>The domain events are the recording of facts. They reflect a bundle of data that encompasses the delta between two states. Therefore, the state is not stored in the database, merely a log of deltas that exist alongside each other on a timeline. This is called the event log.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#event-handling","title":"Event Handling","text":"<p>To populate our mental model, the projection, we need event handlers. These model the mapping between the event log and the document.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#side-notes-on-views","title":"Side Notes on Views","text":"<p>Views will be discussed later, but regarding the contract, the viewstore essentially models the external contract, the GraphQL API. So while the document is a data contract, this contract remains within the domain. The viewstore can do various things with the data:</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#store-as-is","title":"Store As Is","text":"<p>Essentially caching a snapshot of the aggregate state. In this case, the internal model becomes publicly accessible, being queryable and read-optimized.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#enrich-and-store","title":"Enrich and Store","text":"<p>Enrichment can take various forms, such as combining data from different aggregates into one document or modifying data for storage, i.e., determining derived data and caching it in the viewstore.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#enrich-during-data-reading","title":"Enrich During Data Reading","text":"<p>Modifying the API response before it is sent to the client. This involves executing logic on the combination of request data and cached data. Essentially, this is an on-the-fly projection where the view model is virtual. The logic has access to the request data and a fluent API to the viewstore, allowing the creation of a response object using Python scripting from this combination.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#behavior-flows","title":"Behavior Flows","text":""},{"location":"Concept/01_Write_Domain/02_aggregate/#overview","title":"Overview","text":"<p>Behavior flows define the lifecycle and interactions of aggregates within a system. They encapsulate the business logic required to handle events and commands, ensuring consistent state transitions and proper handling of complex workflows. By defining how an aggregate responds to various inputs, behavior flows help maintain system integrity and align with business rules.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#general","title":"General","text":"<p>Behavior is a response to an event emited within the domain this may be an ActorEvent (Command) or a DomainEvent (From an other entity), which can lead to changes in the state of an aggregate.</p> <ul> <li>Name: A unique identifier for the command.</li> <li>Create Command: (Optional) Indicates if this should be the initial behavior of a new instance of the aggregate. If   the instance with the specific business key is already present in the database the execution will fail.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#trigger","title":"Trigger","text":"<p>A trigger specifies the conditions under which a command is activated. It listens for specific events and uses the data from these events to execute the corresponding behavior flow.</p> <ul> <li>Source: The event that activates the command.</li> <li>Key Field: The primary field used to correlate the incoming event with the correct aggregate instance.</li> <li>Mapping: Transfers data from the event to the flow variable fields. Optionally event-fields can be marked   as <code>part of the idempotency key</code> giving you the ability to model functional idempotency.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#processors","title":"Processors","text":"<p>Processors are essential components within behavior flows that perform specific operations. They manage validations, data transformations, event emissions, and other logical actions to ensure that the behavior executes correctly and that aggregates maintain consistent state transitions. Each processor type serves a unique function and has specific attributes to support its operations.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#processor-types","title":"Processor Types","text":""},{"location":"Concept/01_Write_Domain/02_aggregate/#emit-event","title":"Emit Event","text":"<p>The <code>emit-event</code> processor is responsible for generating and emitting domain events, which signal that a significant change or action has occurred within the system. This processor type includes a reference specifying the event to be emitted, and a mapping mechanism that transfers flow-variables to the event fields.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#code","title":"Code","text":"<p>The <code>code</code> processor allows for custom logic to be executed within the behavior flow. This processor is used when predefined processor types do not cover the required operation. It includes the code (inline) or script (global) to be executed, and optionally, inputs or parameters required by the script.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#validator","title":"Validator","text":"<p>The <code>validator</code> processor checks specific conditions and ensures they are met before proceeding. If the condition fails, an exception or error is triggered, preventing the behavior from executing further. This processor type includes the condition to be checked, typically expressed as a logical expression, and the message or exception to be raised if the condition is not met. If a functional exception is raised, no events will be persisted to the event store. So a validator may also validate on the instance state after emit-event processors are already applied, it will still prevent the state transition.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#set-variable","title":"Set Variable","text":"<p>The <code>set-variable</code> processor assigns values to variables (in memory) within the behavior flow. This can be used to store intermediate results, configuration values, or other data needed for subsequent processing steps. This processor type includes the name of the variable to be set, and the expression or value used to determine the variable's value.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#update-key","title":"Update Key","text":"<p>The <code>update-key</code> processor updates the business key of an aggregate instance. This is useful for operations that require changing the identifier or key used to access an aggregate instance. This processor type includes the field in the aggregate to be updated, and the new value to be assigned to the key field.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#test-case","title":"Test Case","text":"<p>Test cases validate the behavior of a command by defining inputs, expected domain events, and resulting state changes. They ensure that the command performs as intended and that the aggregate's state transitions correctly.</p> <ul> <li>Name: A unique identifier for the test case.</li> <li>Trigger Event: The event that initiates the test case.</li> <li>Input: Defines the input data for the command.<ul> <li>Name: The name of the input field.</li> <li>Value: The value assigned to the input field.</li> <li>Type: The data type of the input field.</li> </ul> </li> <li>Expected Domain Event: Specifies the expected event to be emitted by the command.<ul> <li>Field: Defines the expected values for fields in the emitted event.</li> </ul> </li> <li>State: (Optional) The initial state of the aggregate before executing the command.</li> <li>Expected State: The expected state of the aggregate after executing the command.<ul> <li>Primary Key (pk): The key identifying the aggregate instance.</li> <li>State Data: The expected state data of the aggregate.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#summary","title":"Summary","text":"<p>Behavior flows are essential for managing the lifecycle and interactions of aggregates. By defining commands, triggers, mappings, processors, and test cases, behavior flows ensure that business logic is consistently applied, state transitions are correctly managed, and the system's behavior aligns with the intended domain model. Understanding and modeling behavior flows are crucial for implementing robust, maintainable, and scalable systems.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#advanced-features-of-an-aggregate","title":"Advanced Features of an Aggregate","text":""},{"location":"Concept/01_Write_Domain/02_aggregate/#event-store-time-to-live-ttl","title":"Event Store Time-to-Live (TTL)","text":"<p>The Event Store Time-to-Live (TTL) is an advanced configuration setting for aggregates that specifies the duration, in seconds, for which events associated with the aggregate should be retained in the event store. This feature is particularly useful in scenarios where certain events become irrelevant or obsolete after a specified period. The default TTL is -1 seconds which translates to indefinitely.</p> <p>The TTL setting allows system architects and developers to manage the lifecycle of events within the event store effectively. By setting an expiration time for events related to an aggregate, the system can automatically purge older events beyond the specified TTL. This helps in maintaining a lean and efficient event store by removing outdated data that no longer contributes to the current state or history of the aggregate.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#benefits","title":"Benefits","text":"<ul> <li> <p>Optimized Event Storage: Helps in managing storage space by automatically removing events that are no longer   relevant.</p> </li> <li> <p>Compliance and Governance: Supports compliance requirements by ensuring that sensitive or outdated data is removed   from the system in a timely manner.</p> </li> <li> <p>Performance: Improves performance by reducing the volume of data that needs to be processed and queried over time.</p> </li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#considerations","title":"Considerations","text":"<ul> <li> <p>Event Retention Policies: Define appropriate TTL values based on business needs and regulatory requirements.</p> </li> <li> <p>Data Archival: Consider integrating TTL with data archival strategies to maintain historical data beyond the event   store.</p> </li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#summary_1","title":"Summary","text":"<p>The Event Store Time-to-Live (TTL) configuration for aggregates enhances the management and efficiency of event-driven architectures. By setting TTL values, systems can automatically manage event data retention, ensuring that only relevant and current information is maintained in the event store while optimizing storage resources and improving overall system performance.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#snapshot-interval","title":"Snapshot Interval","text":"<p>The Snapshot Interval is an advanced configuration setting for aggregates that determines how frequently snapshots of the aggregate's state should be taken. Snapshots capture the current state of an aggregate at a specific point in time, providing a checkpoint that can optimize event sourcing and reconstruction processes.</p> <p>In event sourcing architectures, aggregates can accumulate a large number of events over time, which can impact performance during state reconstruction. Snapshots serve as efficient checkpoints by storing the aggregate's current state periodically. Instead of replaying all events from the beginning to reconstruct the aggregate's state, the system can load the latest snapshot and replay only the events that occurred after the snapshot was taken.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#usage","title":"Usage","text":"<p>When configuring an aggregate with Snapshot Interval:</p> <ul> <li>Snapshot Interval: Specifies the number of events after which a new snapshot of the aggregate's state should be   taken. For example, a snapshot interval of 100 means that every 100 events, a snapshot of the   aggregate's state will be captured.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#benefits_1","title":"Benefits","text":"<ul> <li> <p>Performance Optimization: Reduces the time and computational resources required for state reconstruction by   loading snapshots and applying only subsequent events.</p> </li> <li> <p>Efficient Event Handling: Improves overall system performance by minimizing the number of events that need to be   processed during state recovery.</p> </li> <li> <p>Scalability: Facilitates scalability by reducing the processing overhead associated with large aggregates and long   event histories.</p> </li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#considerations_1","title":"Considerations","text":"<ul> <li>Snapshot Size: Evaluate the size and complexity of aggregate states to determine an appropriate snapshot interval.</li> <li>Event Volume: Adjust the snapshot interval based on the frequency and volume of events generated by aggregates.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#default-value","title":"Default Value","text":"<p>The default Snapshot Interval is typically set to 100 events, providing a balance between capturing frequent snapshots and minimizing overhead. This default value can be adjusted based on specific application requirements and performance considerations.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#summary_2","title":"Summary","text":"<p>The Snapshot Interval configuration enhances the efficiency and performance of event-sourced aggregates by periodically capturing snapshots of their states. By reducing the computational effort required for state reconstruction, snapshots optimize event handling and support scalability in event-driven architectures. Adjusting the snapshot interval allows systems to achieve optimal performance while effectively managing aggregate states and event histories.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#backup-to-blob-storage-s3","title":"Backup to Blob Storage (S3)","text":"<p>The Backup to Blob Storage feature allows event-sourced aggregates to automatically back up their state to cloud storage, specifically Amazon S3. This capability ensures that critical data remains secure and accessible, providing resilience against data loss and supporting disaster recovery strategies.</p> <p>In event sourcing architectures, ensuring data durability and recoverability is paramount. Backup to Blob Storage leverages cloud infrastructure, such as Amazon S3, to store snapshots of aggregate states at regular intervals. Additionally, it manages the retention of these backups based on specified policies, providing flexibility in data management and compliance with retention requirements.</p> <p>The event store (DynamoDB) has point-in-time recovery enabled, therefore, this feature is disabled by default to reduce the area where data needs to be protected. If this feature should be enabled for a specific aggregate  is a business consideration.</p>"},{"location":"Concept/01_Write_Domain/02_aggregate/#configuration","title":"Configuration","text":"<ul> <li>Interval: Specifies the frequency, specified in days, at which snapshots of aggregate states should be backed up to S3. .</li> <li>Retention Period: Defines the duration for which backup snapshots should be retained in S3 storage, measured in   days. After the retention period expires, older snapshots are automatically deleted.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#usage_1","title":"Usage","text":"<p>The Backup to Blob Storage feature offers several advantages:</p> <ul> <li> <p>Data Resilience: Enhances data durability by securely storing aggregate state snapshots in Amazon S3, which is   designed for high availability and redundancy.</p> </li> <li> <p>Disaster Recovery: Facilitates rapid recovery of aggregate states in the event of data corruption, system   failures, or disasters by maintaining up-to-date backups. Although the required script to recover from cold storage    are not (yet) provided by Draftsman.</p> </li> <li> <p>Compliance: Supports compliance with regulatory requirements and data retention policies by managing backup   retention periods effectively.</p> </li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Security: Ensure proper access controls and encryption mechanisms are in place to protect sensitive data stored in   S3.</li> <li>Cost Management: Monitor storage costs associated with S3 backups and optimize usage based on storage needs and   budget constraints.</li> <li>Integration: Integrate with existing backup and recovery processes to streamline data management and operational   workflows.</li> </ul>"},{"location":"Concept/01_Write_Domain/02_aggregate/#summary_3","title":"Summary","text":"<p>Backup to Blob Storage (S3) is a critical feature in event-driven architectures that ensures the resilience and recoverability of event-sourced aggregates. By automatically backing up aggregate states to Amazon S3 at defined intervals and managing retention periods, this feature enhances data durability, supports disaster recovery strategies, and enables compliance with data retention policies. Configuring backup intervals and retention periods optimally balances data protection with operational efficiency, thereby safeguarding business-critical information in event sourcing environments.</p>"},{"location":"Concept/01_Write_Domain/03_automations/","title":"Automations","text":""},{"location":"Concept/01_Write_Domain/03_automations/#overview","title":"Overview","text":"<p>Automations, also referred to as notifiers, are predefined processes that are triggered in response to specific events or conditions within a system. They are designed to perform actions automatically without direct user intervention, often to notify users or other systems about important events or changes. Automations enhance system responsiveness and improve user experience by providing timely and relevant information.</p>"},{"location":"Concept/01_Write_Domain/03_automations/#automation-definition","title":"Automation Definition","text":"<p>An automation is defined by specifying its name, trigger conditions, and actions to be performed. Automations react to events occurring within the system, executing predefined actions when certain conditions are met.</p>"},{"location":"Concept/01_Write_Domain/03_automations/#attributes-of-an-automation","title":"Attributes of an Automation","text":""},{"location":"Concept/01_Write_Domain/03_automations/#name","title":"Name","text":"<p>A unique identifier for the automation.</p>"},{"location":"Concept/01_Write_Domain/03_automations/#trigger-conditions","title":"Trigger Conditions","text":"<p>Conditions or events that initiate the automation. These triggers can be system events, changes in data, or specific time-based conditions. Examples of trigger conditions include:</p> <ul> <li>Event-based: Automations can be triggered by specific events such as Commands (ActorEvents) originating from the   GraphQL API. Or domain events published by aggregates.</li> <li>Time-based: Automations can be set to trigger at specific times or after certain intervals.</li> <li>After-deployment: Automations can trigger after the application is deployed.</li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#actions","title":"Actions","text":""},{"location":"Concept/01_Write_Domain/03_automations/#automation-activity-types","title":"Automation Activity Types","text":"<p>Automations in a system can perform a variety of tasks, known as activities. Each activity type represents a specific action that the automation can execute when triggered. Below is a documentation of the possible activity types that can be used within automations (notifiers).</p>"},{"location":"Concept/01_Write_Domain/03_automations/#overview_1","title":"Overview","text":"<p>Automations consist of a series of activities that define the specific actions to be performed. These activities are triggered by events or conditions within the system and can interact with various system components or external services.</p>"},{"location":"Concept/01_Write_Domain/03_automations/#activity-types","title":"Activity Types","text":""},{"location":"Concept/01_Write_Domain/03_automations/#identity-and-access-management-iam-aws-cognito-activities","title":"Identity and Access Management (IAM - AWS Cognito) Activities","text":"<ul> <li> <p>create-iam-group</p> <ul> <li>Description: Creates a new IAM group within the system.</li> <li>Use Case: Used when a new group of users with specific permissions needs to be created.</li> </ul> </li> <li> <p>delete-iam-group</p> <ul> <li>Description: Deletes an existing IAM group.</li> <li>Use Case: Used when an IAM group is no longer needed and should be removed.</li> </ul> </li> <li> <p>add-user-to-iam-group</p> <ul> <li>Description: Adds a user to an IAM group.</li> <li>Use Case: Used to grant a user the permissions associated with the IAM group.</li> </ul> </li> <li> <p>remove-user-from-iam-group</p> <ul> <li>Description: Removes a user from an IAM group.</li> <li>Use Case: Used to revoke a user's permissions associated with the IAM group.</li> </ul> </li> <li> <p>retrieve-email-from-iam</p> <ul> <li>Description: Retrieves a user's email address from the IAM system.</li> <li>Use Case: Used to get the email address of a user for communication purposes.</li> </ul> </li> <li> <p>iam-create-systemuser</p> <ul> <li>Description: Creates a system user in the IAM system.</li> <li>Use Case: Used to create a non-human user that interacts with the system programmatically.</li> </ul> </li> <li> <p>iam-create-user</p> <ul> <li>Description: Creates a new user in the IAM system.</li> <li>Use Case: Used for onboarding new users.</li> </ul> </li> <li> <p>iam-delete-user</p> <ul> <li>Description: Deletes a user from the IAM system.</li> <li>Use Case: Used when a user account needs to be permanently removed.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#communication-and-notification-activities","title":"Communication and Notification Activities","text":"<ul> <li> <p>render-template</p> <ul> <li>Description: Renders a template with specified data.</li> <li>Use Case: Used to create personalized messages or documents.</li> </ul> </li> <li> <p>send-email</p> <ul> <li>Description: Sends an email to specified recipients. (AWS SES)</li> <li>Use Case: Used for sending notifications, alerts, or other communications.</li> </ul> </li> <li> <p>send-graphql-notification</p> <ul> <li>Description: Sends a notification via the GraphQL endpoint.</li> <li>Use Case: Used for notifying clients or systems through the GraphQL API.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#file-and-data-operations","title":"File and Data Operations","text":"<ul> <li> <p>write-file</p> <ul> <li>Description: Writes data to a file.</li> <li>Use Case: Used to save data in a remote file system (AWS S3).</li> </ul> </li> <li> <p>fetch-property</p> <ul> <li>Description: Fetches a specific property or data point from the system.</li> <li>Use Case: Used to retrieve configuration settings or other data.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#token-management-activities","title":"Token Management Activities","text":"<ul> <li> <p>get-token</p> <ul> <li>Description: Retrieves a token for authentication or authorization purposes.</li> <li>Use Case: Used to get tokens required for accessing secured resources.</li> </ul> </li> <li> <p>get-systemuser-token</p> <ul> <li>Description: Retrieves a token for a system user.</li> <li>Use Case: Used for system-to-system authentication.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#variable-and-state-management","title":"Variable and State Management","text":"<ul> <li>set-variable<ul> <li>Description: Sets a flow variable to a specified value.</li> <li>Use Case: Used to store temporary data or state within an automation.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#api-and-external-service-interactions","title":"API and External Service Interactions","text":"<ul> <li> <p>call-internal-api</p> <ul> <li>Description: Calls an internal GraphQL API endpoint.</li> <li>Use Case: Used to invoke internal system functions (Commands/Queries/Projections).</li> </ul> </li> <li> <p>HTTP</p> <ul> <li>Description: Makes an HTTP request to an external service.</li> <li>Use Case: Used to interact with external APIs or services.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#miscellaneous-activities","title":"Miscellaneous Activities","text":"<ul> <li> <p>code</p> <ul> <li>Description: Executes custom code.</li> <li>Use Case: Used for complex logic that cannot be handled by predefined activities.</li> </ul> </li> <li> <p>invalidate-cdn</p> <ul> <li>Description: Invalidates a CDN cache.</li> <li>Use Case: Used to ensure that updated content is served from the AWS CloudFront CDN.</li> </ul> </li> <li> <p>loop</p> <ul> <li>Description: Repeats a set of activities a specified number of times.</li> <li>Use Case: Used for iterating over a collection of items or repeating an action multiple times.</li> </ul> </li> </ul>"},{"location":"Concept/01_Write_Domain/03_automations/#summary","title":"Summary","text":"<p>Automations leverage a diverse set of activity types to perform various tasks automatically. By combining these activities, systems can create sophisticated workflows that enhance functionality, improve efficiency, and provide timely responses to events and conditions. Understanding these activity types is essential for designing effective automations that meet specific business needs.</p>"},{"location":"Concept/02_Read_Domain/","title":"About the Read domain","text":""},{"location":"Concept/02_Read_Domain/#overview","title":"Overview","text":"<p>The View domain within the architecture serves the primary function of providing optimized and tailored data access for querying purposes. It complements the write operations handled by the Command domain by offering a structured approach to retrieving and presenting data from the system. This separation ensures that querying operations do not impact the integrity or performance of the write operations, facilitating efficient data retrieval and presentation.</p>"},{"location":"Concept/02_Read_Domain/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Projection of Truth: Views in the architecture are designed to project the truth of the system's state into   optimized read models. While the command side focuses on recording domain events and maintaining the system's state,   the view side is concerned with interpreting this state for query purposes.</p> </li> <li> <p>Materialized Views: These are pre-computed views optimized for specific querying needs. Materialized views store   aggregated or transformed data in a way that enhances query performance, enabling faster retrieval compared to   recalculating data on the fly.</p> </li> <li> <p>Decoupling of Models: The View domain decouples the read model from the write model. This separation allows   flexibility in how data is structured and queried, accommodating different perspectives and needs without affecting   the core data integrity.</p> </li> </ul>"},{"location":"Concept/02_Read_Domain/#functional-aspects","title":"Functional Aspects","text":"<ul> <li> <p>Query Execution: Views facilitate efficient querying by providing access to materialized views and optimized data   structures. This capability supports complex querying scenarios, including aggregations, filtering, and combining data   from different sources.</p> </li> <li> <p>Functional Keys: Unlike technical keys used in the write model, views often employ functional keys that can evolve   over time to meet changing business requirements. This flexibility allows the system to adapt to new data access   patterns without disrupting existing operations.</p> </li> <li> <p>Just in time Projection: The core concept of a projection involves dynamically transforming and combining data   from various sources, such as materialized views and user inputs, to generate specialized views in real-time.   Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user   criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach   enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently   without relying solely on precomputed results.</p> </li> </ul>"},{"location":"Concept/02_Read_Domain/#benefits","title":"Benefits","text":"<ul> <li> <p>Performance Optimization: By pre-computing and optimizing data for specific queries, views enhance overall system   performance by reducing query latency and resource consumption.</p> </li> <li> <p>Flexibility and Adaptability: The decoupling of read and write models allows for independent evolution of data   access patterns and query optimizations, ensuring that the system can adapt to new requirements without extensive   refactoring.</p> </li> <li> <p>Improved Scalability: Separating read operations from write operations improves scalability by distributing the   workload more efficiently across different components or services. Our implementation relies on AWS AppSync to </p> </li> </ul>"},{"location":"Concept/02_Read_Domain/#conclusion","title":"Conclusion","text":"<p>In conclusion, the View domain plays a crucial role in the architecture by providing efficient, optimized, and flexible data access for querying purposes. By leveraging materialized views, decoupled models, and optimized query execution, the View domain ensures that querying operations are performant, scalable, and adaptable to evolving business needs.</p>"},{"location":"Concept/02_Read_Domain/01_views/","title":"Views","text":"<p>Views are defined entities that represent a set of attributes and may contain transformation of data from underlying aggregates (data sources). Each view is named and configured with specific parameters.</p>"},{"location":"Concept/02_Read_Domain/01_views/#fields","title":"Fields","text":"<p>Views represent collections of documents, fields are part of the document schema. Fields are configured with a name and a type (String, Int, Float, Boolean, StringList).</p>"},{"location":"Concept/02_Read_Domain/01_views/#relations","title":"Relations","text":"<p>The document schema of a view may have relations to other views. These may be OneToOne, OneToMany, ManyToOne, and ManyToMany. A special type of relation is the ObjectList. While the other relations point to other documents in the view store, ObjectList represents a nested object in the document.</p> <p>Relations are part of the document schema of a view and have a name, relation type, and referenced view-model as minimal configuration. Depending on the relation type, it may be mandatory to configure the foreign key (this may be a canonical key, an advanced feature explained later). Optionally, it is possible to require a specific role (authorization) to access the relational data. The default authorization method is <code>inherit from the API query</code>, meaning if the client has access to the view, they have access to the relational data.</p>"},{"location":"Concept/02_Read_Domain/01_views/#business-key","title":"Business Key","text":"<p>One of the fields may be appointed to serve as the business key (also called the primary key). The field that is used as the business key must be of the type <code>String</code>. Only when a business key is appointed, the view can be stored in the view-store as a root document. Only root documents may have queries exposed in the GraphQL API.</p> <p>If no business key is appointed, the view model can only serve as ObjectList relations in other views.</p>"},{"location":"Concept/02_Read_Domain/01_views/#data-mapping","title":"Data Mapping","text":"<p>Views with an appointed business key need to be filled with data published by aggregates. There are two types of event-handlers that are configured to listen for snapshot-events from a specific aggregate. A view may have multiple event handlers observing different aggregates or multiple handlers observing the same aggregate. This is useful when the aggregate instance has its business key updated, allowing one handler to create a new instance with the new key and another handler to remove the old view instance.</p> <p>A snapshot-event is streamed to the view-store every time an aggregate emits a domain-event.</p>"},{"location":"Concept/02_Read_Domain/01_views/#mapper","title":"Mapper","text":"<p>The first type of event-handler is a simple mapper. It enables you to map aggregate fields and collections to view-fields and ObjectList relations. Additionally, you must configure which aggregate field (String) serves as the business-key of the view-instance. Note that the business-key for the view may differ from the business-key from the aggregate. Furthermore, you can select a processing strategy, either <code>item</code> or <code>dictionary</code>.</p> <ul> <li>Item: The mapping is executed once for every aggregate snapshot.</li> <li>Dictionary: In this case, you select one of the aggregate's nested collections to be processed. This is useful   when a nested collection in an aggregate must be autonomously queryable.</li> </ul> <p>Additionally, you can configure multiple delete-expressions, causing deletion of the view instance when any of the expressions evaluate to true.</p>"},{"location":"Concept/02_Read_Domain/01_views/#custom-mapper","title":"Custom Mapper","text":"<p>Instead of relying on the standard mapping, you can use Python code to convert aggregate data into the view instance. This allows for advanced use cases like pre-filtering and enrichment beyond the capabilities of the standard mapper. For example, you could create multilevel nesting in the document (ObjectList relation) which the standard mapper only provides for one level. However, for maintainability, simpler is usually better.</p>"},{"location":"Concept/02_Read_Domain/01_views/#graphql-queries","title":"GraphQL Queries","text":"<p>Queries associated with views define access patterns for retrieving data via GraphQL. The GraphQL namespace is configured at the view level, meaning all queries in a specific view are bundled in the same namespace. Each query has a unique name in this namespace, and the needed access control method can be configured:</p> <ul> <li>anonymous: Requires an API key that can be configured in the client. Because this key may become publicly   available, the API may rate limit these requests to prevent DDOS attacks.</li> <li>authenticated: Requires a user token to access the data.</li> <li>user: Requires a predefined query parameter to match the signed-in user, making the data accessible only to the   current signed-in user.</li> <li>role-based: Requires the user to have a specific role to access the data.</li> </ul> <p>There are three types of queries:</p> <ul> <li>get: Returns one instance of the view based on the business key. This type can be configured once per view.</li> <li>filter: Returns a list of instances based on defined filter clauses, e.g., <code>field x equals a query parameter</code>.   During modeling, the possible filter clauses are defined. Filters may also provide canonical search, making the   key_begins_with query parameter mandatory. This type can be configured only once per view.</li> <li>named query: Similar to the filter query but with a configurable name during modeling, and the filter clauses are   mandatory for the client.</li> </ul>"},{"location":"Concept/02_Read_Domain/01_views/#data-retention-and-exclusion","title":"Data Retention and Exclusion","text":"<p>Views may specify:</p> <ul> <li>Data Retention: Period for which data within the view remains accessible. Defined in days, the default is -1,   translating to indefinitely.</li> <li>Notification Exclusion: By default, the view store will publish a GraphQL notification to which clients can   subscribe. The notification contains the view name and the primary key, allowing the client to refetch the data when   an update occurs. Subscriptions to the notification may be anonymous. This option prevents the publication of this   notification, e.g., because the business key contains identifiable data like a username or an email address.</li> </ul>"},{"location":"Concept/02_Read_Domain/01_views/#benefits","title":"Benefits","text":"<ul> <li>Data Consistency: Views provide consistent and up-to-date representations of data entities.</li> <li>Customization: Custom handlers enable tailored logic and functionalities within views.</li> <li>Integration: Seamless integration with underlying aggregates ensures data accuracy and synchronization.</li> <li>Security: Authorization controls ensure data access is restricted as per defined policies.</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/","title":"Projections","text":"<p>The core concept of a projection involves dynamically transforming and combining data from various sources, such as materialized views and user inputs, to generate specialized views in real-time. Projections facilitate tasks like aggregating multiple materialized views, applying advanced filters based on user criteria, and performing calculations using both user-provided data and precomputed views. This dynamic approach enables responsive and flexible querying, ensuring that applications can deliver tailored data views efficiently without relying solely on precomputed results.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#api-configuration","title":"API Configuration","text":"<p>To configure a projection, several key elements must be defined:</p>"},{"location":"Concept/02_Read_Domain/02_projections/#projection-name","title":"Projection Name","text":"<p>A unique identifier for the projection, which helps in managing and referencing the projection.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#graphql-namespace","title":"GraphQL Namespace","text":"<p>The namespace under which the projection's GraphQL queries will be bundled. This helps in organizing queries logically.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#graphql-method-name","title":"GraphQL Method Name","text":"<p>The method name used to invoke the projection via GraphQL. This name should be unique within the specified namespace.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#authorization-method","title":"Authorization Method","text":"<p>Specifies the access control for the projection. The following methods are supported:</p> <ul> <li>anonymous: Requires an API key that can be configured in the client. This key may be publicly available, and the   API may rate limit these requests to prevent DDOS attacks.</li> <li>authenticated: Requires a user token to access the data.</li> <li>role based: Requires the user to have a specific role to access the data.</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#return-object","title":"Return Object","text":"<p>References a view model that defines the structure of the data returned by the projection.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#return-type","title":"Return Type","text":"<p>Specifies whether the projection returns a single item or a result set. The possible values are:</p> <ul> <li>item: Returns a single instance of the view.</li> <li>result_set: Returns a list of instances.</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#arguments","title":"Arguments","text":"<p>Projections can accept various query variables, which are specified as follows:</p>"},{"location":"Concept/02_Read_Domain/02_projections/#name","title":"Name","text":"<p>The name of the query variable.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#type","title":"Type","text":"<p>The data type of the query variable. Supported types include:</p> <ul> <li>String</li> <li>Int</li> <li>Float</li> <li>Boolean</li> <li>StringList</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#required","title":"Required","text":"<p>Specifies whether the query variable is mandatory for the projection to execute. The possible values are:</p> <ul> <li>true</li> <li>false</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#data-preparation-logic","title":"Data Preparation Logic","text":"<p>The core logic for preparing data in a projection is written in Python. This logic involves accessing materialized views, applying filters, and transforming data as needed. The following example demonstrates how to implement the data preparation logic:</p> <pre><code>from draftsman.ViewStoreApi import Query\n\n\ndef transform(arguments, username):\n    # You have access to the username of the requestor and the arguments.\n    print(f\"Handle graph request [{arguments}/{username}]\")\n\n    # You have access to a fluent API to access materialized views\n    # Here are some examples:\n\n    # Access a specific object (most efficient method cost-wise)\n    data = Query('ViewName').get_item(\"key\").run()\n\n    # Get a list of all objects of a specific type\n    data = Query('ViewName').get_items().run()\n\n    # Filter a list of objects based on type and key, part of the canonical key Concept\n    data = Query('ViewName').get_items(\"key_starts_with_this_value\").run()\n\n    # Filter on content for a specific type (performs a scan on all views of type 'ViewName')\n    data = Query('ViewName').get_items().equals('key', 'value').between('key', 0, 100).run()\n\n    # Combine the two filter methods to filter within a subset\n    data = Query('ViewName').get_items(\"key_starts_with_this_value\").equals('key', 'value').between('key', 0, 100).run()\n\n    # Switch to an \"or\" operator for filters (default is \"and\")\n    data = Query('ViewName').get_items(key=\"key_starts_with_this_value\", filter_chain_method=\"or\").equals('key',\n                                                                                                          'value').between(\n        'key', 0, 100).run()\n\n    # Program data transformations with Python\n    # Ensure you add all fields that are defined in the return view object definition\n    return {\"field_name\": \"value\"}\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#fluent-api","title":"Fluent API","text":"<p>The Fluent API, which allows you to query and filter data from the DynamoDB view store in a flexible and efficient manner.</p> <p>The Fluent API provides a convenient way to construct and execute queries against the DynamoDB view-store-table. It supports various filtering methods and allows for the retrieval of individual items or sets of items based on specified criteria.</p>"},{"location":"Concept/02_Read_Domain/02_projections/#initialization","title":"Initialization","text":"<p>To start using the Fluent API, you need to initialize the <code>Query</code> class with the type of view you want to query.</p> <pre><code>from draftsman.ViewStoreApi import Query\n\n# Initialize a query for a specific view name\nquery = Query(\"YourViewName\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#methods","title":"Methods","text":""},{"location":"Concept/02_Read_Domain/02_projections/#retrieving-items","title":"Retrieving Items","text":""},{"location":"Concept/02_Read_Domain/02_projections/#get_item","title":"get_item","text":"<p>Retrieve a single item based on its key.</p> <pre><code>query.get_item(\"your_item_key\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#get_items","title":"get_items","text":"<p>Retrieve a set of items, optionally filtered by a key prefix.</p> <pre><code>query.get_items(key=\"key_prefix\", filter_chain_method=\"and\")\n</code></pre> <ul> <li><code>key</code> (optional): A prefix to filter items by their keys.</li> <li><code>filter_chain_method</code> (optional): Determines how multiple filters are combined. Can be \"and\" or \"or\". Default is \"   and\".</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#adding-filters","title":"Adding Filters","text":"<p>Filters can be applied to the query to narrow down the results. The following filter methods are available:</p>"},{"location":"Concept/02_Read_Domain/02_projections/#equals","title":"equals","text":"<p>Filter items where the attribute equals a specified value.</p> <pre><code>query.equals(\"attribute_name\", \"value\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#less_than","title":"less_than","text":"<p>Filter items where the attribute is less than a specified value.</p> <pre><code>query.less_than(\"attribute_name\", value)\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#less_than_equals","title":"less_than_equals","text":"<p>Filter items where the attribute is less than or equal to a specified value.</p> <pre><code>query.less_than_equals(\"attribute_name\", value)\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#greater_than","title":"greater_than","text":"<p>Filter items where the attribute is greater than a specified value.</p> <pre><code>query.greater_than(\"attribute_name\", value)\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#greater_than_equals","title":"greater_than_equals","text":"<p>Filter items where the attribute is greater than or equal to a specified value.</p> <pre><code>query.greater_than_equals(\"attribute_name\", value)\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#begins_with","title":"begins_with","text":"<p>Filter items where the attribute begins with a specified value.</p> <pre><code>query.begins_with(\"attribute_name\", \"value_prefix\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#between","title":"between","text":"<p>Filter items where the attribute is between two specified values.</p> <pre><code>query.between(\"attribute_name\", low_value, high_value)\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#not_equals","title":"not_equals","text":"<p>Filter items where the attribute does not equal a specified value.</p> <pre><code>query.not_equals(\"attribute_name\", \"value\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#exists","title":"exists","text":"<p>Filter items where the attribute exists.</p> <pre><code>query.exists(\"attribute_name\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#not_exists","title":"not_exists","text":"<p>Filter items where the attribute does not exist.</p> <pre><code>query.not_exists(\"attribute_name\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#contains","title":"contains","text":"<p>Filter items where the attribute contains a specified value.</p> <pre><code>query.contains(\"attribute_name\", \"value\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#attribute_type","title":"attribute_type","text":"<p>Filter items where the attribute is of a specified type.</p> <pre><code>query.attribute_type(\"attribute_name\", \"type\")\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#is_in","title":"is_in","text":"<p>Filter items where the attribute is in a specified list of values.</p> <pre><code>query.is_in(\"attribute_name\", [\"value1\", \"value2\", \"value3\"])\n</code></pre>"},{"location":"Concept/02_Read_Domain/02_projections/#running-the-query","title":"Running the Query","text":"<p>Once the query is constructed with the necessary filters, it can be executed using the <code>run</code> method.</p> <pre><code>result = query.run()\n</code></pre> <ul> <li>If <code>get_item</code> was used, <code>run</code> returns a single item.</li> <li>If <code>get_items</code> was used, <code>run</code> returns a list of items matching the query.</li> </ul>"},{"location":"Concept/02_Read_Domain/02_projections/#method-chaining","title":"Method chaining","text":"<p>The Fluent API supports method chaining, allowing you to construct complex queries in a readable and concise manner by chaining multiple method calls together. Method chaining enhances the clarity of query construction and reduces the need for intermediate variables.</p> <pre><code>resultset = query.get_items(\"key_prefix\").equals(\"status\", \"active\").greater_than(\"priority\", 1).equals(\"user\",\"j.doe\").run()\n</code></pre>"},{"location":"Concept/03_Utils/","title":"Globaly available concepts","text":"<p>Utils are globally available concepts that are modeled once and used across the system. They include:</p>"},{"location":"Concept/03_Utils/#expressions","title":"Expressions","text":"<p>Expressions are reusable logical constructs used to transform data on-the-fly before executing logic. There are two types of expressions:</p>"},{"location":"Concept/03_Utils/#api-authorization-expression","title":"API Authorization Expression","text":"<p>These expressions are used in models that define an access path in the GraphQL API. They convert query variables to specific roles, useful in multi-tenant systems where each tenant has a private set of roles.</p>"},{"location":"Concept/03_Utils/#trigger-key-expression","title":"Trigger Key Expression","text":"<p>The key expression is usable in all identity-based triggers: Aggregate behavior flows &amp; View data sources. It is used to convert an event attribute into a functional key.</p>"},{"location":"Concept/03_Utils/#dependencies","title":"Dependencies","text":"<p>Allows you to add pip packages to you runtime, the package will be available for importing in all custom python code in your project.</p>"},{"location":"Concept/03_Utils/#patterns","title":"Patterns","text":"<p>Patterns are regular expressions that can be used while modelling  commands to ensure a specific pattern for String fields e.g.</p>"},{"location":"Concept/03_Utils/#lowercasedonly","title":"LowercasedOnly","text":"<pre><code>^[a-z]+$\n</code></pre>"},{"location":"Concept/03_Utils/#date","title":"Date","text":"<pre><code>^(?:20)\\d{2}-\\d{2}-\\d{2}$\n</code></pre>"},{"location":"Concept/03_Utils/#extending-patters","title":"Extending patters","text":"<p>Patterns may reference each-other e.g.</p> <pre><code>^{{LowercasedOnly}}:arn:{{LowercasedOnly}}$\n</code></pre>"},{"location":"Concept/03_Utils/#roles","title":"Roles","text":"<p>Simply a list of role names available in the modeler, at this moment these roles are not automatically loaded into the the IAM system by Draftsman. The application owner needs to define these roles manually, or model an automation (@afterDeployment) that creates these roles.</p>"},{"location":"Concept/03_Utils/#python-modules","title":"Python modules","text":"<p>Python modules are usable scripts that define methods that are accessible from:</p> <ul> <li>Behavior flows</li> <li>Automations</li> </ul> <pre><code>def behavior_or_notifier_function(flow):\n    # May set a flow variable\n    flow.myVariable = \"Hello World!\"\n\n    # And has access to flow variables\n    print(flow.myVariable)\n\n    # And has also access to the aggregate document\n    print(flow.entity)\n    print(flow.entity.entityField)\n</code></pre> <p>It is a simple function but it does provide you access to all variables available to the flow execution.</p>"},{"location":"Concept/03_Utils/01_expressions/","title":"Expressions","text":""},{"location":"Concept/03_Utils/01_expressions/#api-authorization-expression","title":"API Authorization Expression","text":"<p>The authorization expression can be used in all components exposed in the API:</p> <ul> <li>Commands</li> <li>View queries</li> </ul> <p>It is used to convert an input parameter (command field or query filter field, e.g., key, key_begins_with, or a custom filter attribute) to a technical role that the API resolver will validate to determine if the requester has the specific role. This is useful for providing role-based access in a multi-tenant system.</p> <p>The expression has a name, e.g., <code>extractRoleFromArn</code>, which is used to access the expression from command models or view models.</p> <p>You model inputs for this function separated with a ; e.g.,</p> <pre><code>arn;role\n</code></pre> <p>And then use Velocity Template Language (VTL) syntax with basic JavaScript to model the logic:</p> <pre><code>${arn.split(':')[0]}:${arn.split(':')[1]}:role\n</code></pre> <p>In a command or view query, you can use this expression in the role field when you select role-based access:</p> <pre><code>#global.extractRoleFromArn(key, 'viewer')\n</code></pre> <p>When you execute, for example, a query where this is implemented, it will evaluate to:</p> <pre><code>#foreach($group in $context.identity.claims.get(\"cognito:groups\"))\n    #if($group == \"${ctx.args.key.split(':')[0]}:${ctx.args.key.split(':')[1]}:viewer\")\n        #set($inCognitoGroup = true)\n    #end\n#end\n#if($inCognitoGroup){\n    \"version\": \"2018-05-29\",\n    \"operation\": \"GetItem\",\n    \"key\": {\n        \"type\": $util.dynamodb.toDynamoDBJson(\"ViewName\"),\n        \"key\": $util.dynamodb.toDynamoDBJson($ctx.args.key)\n    }\n}\n#else\n$utils.unauthorized()\n#end\n</code></pre>"},{"location":"Concept/03_Utils/01_expressions/#trigger-key-expression","title":"Trigger Key Expression","text":"<p>The key expression is usable in all identity-based triggers:</p> <ul> <li>Aggregate behavior flows</li> <li>View data sources</li> </ul> <p>It is used to convert an event attribute into a functional key. The name of the expression, e.g., truncateArn, is used to access this expression from a behavior or data-source model.</p> <p>You can model input for the expression, e.g., <code>arn;length</code>.</p> <p>The expression itself is written in pure Python, e.g.,</p> <pre><code>':'.join(arn.split(':')[:int(length)])\n</code></pre> <p>You can access it by configuring a method call inside the key field of behavior or data-source. The input parameters will reference a trigger attribute. You can use literals, but they can only be strings. In our case, the expression will evaluate the string to an integer.</p> <pre><code>#global.truncateArn(childArn, '2')\n</code></pre> <p>Let's say that the trigger contains an attribute <code>childArn=draftsmanid:workspace:project</code>, then the resulting key for the flow will be <code>draftsmanid:workspace</code>.</p>"},{"location":"Technical%20Architecture/01_Pipeline/","title":"Deployment Pipeline","text":"<p>This documentation outlines the steps involved in our deployment pipeline, explaining the purpose and necessity of each step to ensure a smooth and reliable process from code generation to production deployment.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#generate-prepare-code","title":"Generate: Prepare Code","text":""},{"location":"Technical%20Architecture/01_Pipeline/#purpose","title":"Purpose","text":"<p>The <code>generate</code> step is crucial for converting high-level models into executable code and validating that code through unit testing. This ensures that the code is correctly generated and functions as intended before moving further along the pipeline.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#process","title":"Process","text":"<ol> <li> <p>Model Conversion: The high-level model, which defines the application's structure and behavior, is converted into    Python code and CloudFormation templates. This step translates abstract models into concrete, executable code and    infrastructure definitions.</p> </li> <li> <p>Unit Testing: After the conversion, unit tests are executed to validate the functionality of the generated code.    Unit tests help identify and fix issues early in the development process, ensuring that individual components of the    application work correctly in isolation. (These are the modeled test cases in the behavior-flow models).</p> </li> </ol>"},{"location":"Technical%20Architecture/01_Pipeline/#why-its-needed","title":"Why It's Needed","text":"<ul> <li>Accuracy: Ensures that the model is correctly translated into code.</li> <li>Early Issue Detection: Unit tests catch errors early, reducing the risk of bugs in later stages.</li> <li>Validation: Confirms that the generated code adheres to expected behaviors and standards.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/#package-prepare-deployment","title":"Package: Prepare Deployment","text":""},{"location":"Technical%20Architecture/01_Pipeline/#purpose_1","title":"Purpose","text":"<p>The <code>package</code> step involves preparing the application artifacts for deployment. These artifacts include all the necessary files and configurations that CloudFormation requires to deploy the application infrastructure.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#process_1","title":"Process","text":"<ol> <li>Artifact Creation: Collect and package the code, CloudFormation templates, and other necessary resources into    deployable artifacts.</li> <li>Versioning: Assign versions to the artifacts to manage releases and ensure consistency across different    environments.</li> </ol>"},{"location":"Technical%20Architecture/01_Pipeline/#why-its-needed_1","title":"Why It's Needed","text":"<ul> <li>Consistency: Ensures that the same artifacts are deployed across different environments, reducing discrepancies.</li> <li>Preparation: Packages the application into a format that CloudFormation can process, streamlining the deployment   process.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/#staging-deploy-staging","title":"Staging: Deploy Staging","text":""},{"location":"Technical%20Architecture/01_Pipeline/#purpose_2","title":"Purpose","text":"<p>The <code>staging</code> step involves deploying the application to a staging environment, which closely mirrors the production environment. This allows for comprehensive testing and validation before the application is deployed to production.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#process_2","title":"Process","text":"<ol> <li>Deployment: Use CloudFormation to deploy the application artifacts to the staging environment.</li> <li>Configuration: Configure the staging environment to match the production setup as closely as possible. (e.g.    manually load API secrets into the staging secrets manager)</li> </ol>"},{"location":"Technical%20Architecture/01_Pipeline/#why-its-needed_2","title":"Why It's Needed","text":"<ul> <li>Validation: Provides a realistic environment to test the application, ensuring it behaves as expected in a   production-like setting.</li> <li>Risk Mitigation: Identifies and resolves issues in a controlled environment, reducing the risk of problems in   production.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/#quality-gate-testing","title":"Quality Gate: Testing","text":""},{"location":"Technical%20Architecture/01_Pipeline/#purpose_3","title":"Purpose","text":"<p>The <code>quality gate</code> step involves executing API tests against the staging environment to verify that the application is stable, functional, and ready for production deployment. This step ensures that the code meets the required quality standards.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#process_3","title":"Process","text":"<ol> <li>Cleanup: Optionally, clear the environments databases and IAM role assignments.</li> <li>API Testing: Execute functional and integration tests on the staging environment, focusing on the application's    API endpoints. These tests are modeled using the Tracepaper tool.</li> <li>Validation: Validate that all tests executed successfully.</li> <li>Coverage: Validate that there is sufficient coverage regarding the published events in the domain and the view    queries.</li> </ol>"},{"location":"Technical%20Architecture/01_Pipeline/#why-its-needed_3","title":"Why It's Needed","text":"<ul> <li>Quality Assurance: Ensures that the application meets quality standards and is free of critical bugs.</li> <li>Verification: Validates that the application behaves correctly under various scenarios and edge cases.</li> <li>Readiness Check: Confirms that the application is ready for production deployment.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/#production-deploy-production","title":"Production: Deploy Production","text":""},{"location":"Technical%20Architecture/01_Pipeline/#purpose_4","title":"Purpose","text":"<p>The <code>production</code> step is the final stage, where the application is deployed to the production environment. This step is executed only after the application has passed all previous stages, ensuring a smooth and reliable deployment.</p>"},{"location":"Technical%20Architecture/01_Pipeline/#process_4","title":"Process","text":"<ol> <li>Deployment: Use CloudFormation to deploy the validated application artifacts to the production environment.</li> <li>Adviced: Monitoring: Immediately start monitoring the production environment to detect and address any issues    that may    arise post-deployment. This is not provided by Draftsman, we ourselves use Lumigo.io to monitor    our applications.</li> </ol>"},{"location":"Technical%20Architecture/01_Pipeline/#why-its-needed_4","title":"Why It's Needed","text":"<ul> <li>Go-Live: Makes the application available to end-users.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/","title":"Quality Gate","text":"<p>The testing concept in the modeling tool ensures that the application acts as expected by validating the modeled behavior, data access, and authorizations before deploying to production. This section covers how to define, structure, and execute functional scenarios using the provided DSL.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#purpose","title":"Purpose","text":"<p>The primary purpose of the test concept is to ensure application functionality, data integrity, and proper authorization by running tests against the GraphQL API of a deployed version of the application. These tests help verify that the application's behavior aligns with the modeled specifications.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#functional-scenario-definition","title":"Functional Scenario Definition","text":"<p>Functional scenarios are defined and describe a sequence of actions and validations to be executed against the GraphQL API. Each scenario consists of various activities that can include mutations, queries, and validation steps.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#available-actions","title":"Available Actions","text":"<p>The model supports the following actions:</p> <ul> <li>Grant Role to Test User: Assigns a specific role to the current test user.</li> <li>Execute Mutation: Performs a mutation (command) to change the state of the application.</li> <li>Validate Behavior and Automations: Checks that expected behavior flows and automations execute with a defined   status (success/error).</li> <li>Validate View Queries: Ensures that queries return the expected data and allows extraction of data for use in   later actions.</li> <li>Define Variables: Sets variables for use in other actions within the scenario.</li> </ul>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#graphql-queries-and-mutations","title":"GraphQL Queries and Mutations","text":"<p>All modeled commands are accessible as mutations, and all modeled queries and projections are accessible for testing purposes. This ensures comprehensive coverage of the application's functionality. The test runner has access to Track &amp; Trace data, meaning that you can instruct the test-runner to validate if a defined set of behavior-flows and automations are executed, and are finished in an success or error state.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#handling-authentication-and-authorization","title":"Handling Authentication and Authorization","text":"<p>For testing purposes, the tool creates test users in the staging environment. Users can insert actions to grant specific roles to the current test user or refresh the user's API token to ensure proper authorization during the test flow.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#results-and-outputs","title":"Results and Outputs","text":"<p>Tests provide a pass or fail result based on:</p> <ul> <li>Unexpected Behavior: Deviation from the expected behavior.</li> <li>Data Results: Mismatch in expected data results.</li> <li>Coverage: Low event or view coverage, indicating insufficient test coverage.</li> </ul> <p>A JSON report is generated and stored in the database. If a test fails, the application will not be deployed to production.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#validating-responses","title":"Validating Responses","text":"<p>Engineers can model response validations to ensure the application's responses meet the expected criteria. This includes checking the presence and values of specific fields in the response data.</p>"},{"location":"Technical%20Architecture/01_Pipeline/QualityGate/#managing-test-data","title":"Managing Test Data","text":"<p>Before tests are executed, the database is cleared to ensure tests run in a consistent and repeatable manner, avoiding interference from residual data.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/01_Technical_Architecture/","title":"Technical Architecture","text":"<p>This diagram outlines the following components and interactions and visualizes the technical architecture to where the model is converted to:</p> <ol> <li>Secrets Manager: Manages sensitive information and can be accessed by the Notifier Lambda function.</li> <li>Cognito (IAM): Manages user authentication and access permissions.</li> <li>AppSync (API Gateway): Handles GraphQL API requests and invokes other Lambda functions.</li> <li>EventBridge: Acts as the event bus, orchestrating events between various Lambda functions.</li> <li>Lambda Functions:<ul> <li>Notifier: Also known as Automations, they may interact with AWS resources, the AppSync API or other API's.</li> <li>T&amp;T (Track &amp; Trace): Processes and stores the trace messages.</li> <li>Projection Engine: Handles data projections.</li> <li>View Updater: Updates the View Store with the latest data.</li> <li>Domain Behavior: Executes the business logic.<ul> <li>Triggers: Activate Lambda functions based on specific ActorEvents or DomainEvents.</li> </ul> </li> </ul> </li> <li>DynamoDB Tables:<ul> <li>Trace Table: Holds trace events.</li> <li>View Store: Contains data for display.</li> <li>IdempotencyStore: Ensures idempotent actions.</li> <li>EventStore: Stores all events.</li> <li>Transactional Outbox: Manages transactional messages that need to be published to EventBridge.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/02_Technical_Concepts/01_Technical_Architecture/#mutation-workflow","title":"Mutation Workflow","text":"<ul> <li>AppSync receives GraphQL requests and invokes publishes an ActorEvent on the EventBridge.</li> <li>T&amp;T Lambda registers the request in track and trace.</li> <li>Domain Behavior Lambda<ul> <li>IdempotencyStore is checked to ensure the request is not already processed. In case it is a duplicate request,   a T&amp;T success/already procesed event is published.</li> <li>EventStore: The state is rehydrated from the event store.</li> <li>business logic is executed</li> <li>The IdempotencyStore, EventStore &amp; Transactional Outbox are updated.</li> </ul> </li> <li>T&amp;T Lambda: registers the behavior processing state in track and trace.</li> <li>EventFanout: The DomainEvent(s) are published on the EventBridge.</li> <li>Notifier: An automation may trigger on one of the domain events and take action accordingly. For example: querying the   App via AppSync, construct a command, and trigger a new mutation via AppSync (using the same trace id).</li> <li>View Updater Lambda reads snapshots from the Transactional Outbox and updates the View Store.</li> </ul>"},{"location":"Technical%20Architecture/02_Technical_Concepts/01_Technical_Architecture/#query-workflow","title":"Query Workflow","text":"<p>AppSync receives GraphQL requests and resolves all statements to DynamoDB queries using VTL scripting. This may invoke numerous DynamoDB queries that will be aggregated into 1 response. This results in fast response times.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/01_Technical_Architecture/#summary","title":"Summary","text":"<p>This architecture is designed to be serverless, scalable, and cost-effective, leveraging AWS managed services to minimize operational overhead and focus on application logic and performance.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/","title":"AWS services","text":""},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#dynamodb","title":"DynamoDB","text":"<p>Service Description: Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It supports both document and key-value data models, making it suitable for a wide range of applications.</p> <p>Selection Rationale: We chose DynamoDB for its seamless scalability and managed nature, which allows us to focus on application development rather than database management. Its integration with other AWS services and ability to handle large-scale workloads with low latency were key factors in our decision.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: DynamoDB is a fully managed service provided by AWS, eliminating the need for server   management.</li> <li>Minimizing Management and Cost: It offers pay-as-you-go pricing and automate tasks such as hardware provisioning,   setup, and configuration,   reducing operational overhead and potential costs.</li> </ul> <p>Documentation: AWS DynamoDB Documentation</p> <p>Service Limits: DynamoDB Service Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#amazon-cognito","title":"Amazon Cognito","text":"<p>Service Description: Amazon Cognito provides authentication, authorization, and user management for web and mobile applications. It supports social identity providers, such as Facebook, Google, and Amazon, as well as enterprise identity providers via SAML 2.0.</p> <p>Selection Rationale: We selected Amazon Cognito for its ease of integration with other AWS services and its comprehensive identity management features. It simplifies user authentication and access control while providing a secure and scalable solution.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: Amazon Cognito is a fully managed service by AWS, reducing operational complexity and   server management.</li> <li>Minimizing Management and Cost: It offers pay-as-you-go pricing and automates user management tasks, helping to   lower operational costs.</li> </ul> <p>Documentation: Amazon Cognito Documentation</p> <p>Service Limits: Amazon Cognito Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Service Description: AWS Secrets Manager helps you protect access to your applications, services, and IT resources without the upfront cost and complexity of managing your own infrastructure. It enables you to rotate, manage, and retrieve secrets throughout their lifecycle.</p> <p>Selection Rationale: We opted for AWS Secrets Manager to centrally manage and secure access to sensitive information, such as database credentials and API keys. Its integration with AWS services were pivotal in our decision.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: AWS Secrets Manager is a fully managed service, eliminating the need for infrastructure   management.</li> </ul> <p>Documentation: AWS Secrets Manager Documentation</p> <p>Service Limits: AWS Secrets Manager Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#amazon-eventbridge","title":"Amazon EventBridge","text":"<p>Service Description: Amazon EventBridge is a serverless event bus service that makes it easy to connect applications using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services.</p> <p>Selection Rationale: We selected Amazon EventBridge for its seamless integration capabilities across AWS services. It simplifies event-driven architectures by decoupling event producers from consumers and supporting event filtering and transformation. Especially the content based routing is pivotal in our architecture.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: Amazon EventBridge is serverless, handling all infrastructure provisioning and scaling   automatically.</li> <li>Minimizing Management and Cost: It reduces operational overhead by managing event routing and processing,   optimizing costs through pay-as-you-go pricing.</li> </ul> <p>Documentation: Amazon EventBridge Documentation</p> <p>Service Limits: Amazon EventBridge Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#aws-lambda","title":"AWS Lambda","text":"<p>Service Description: AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It automatically scales your application by running code in response to triggers and manages resources on your behalf.</p> <p>Selection Rationale: We chose AWS Lambda for its serverless architecture, which eliminates the need for server management and optimizes resource utilization based on workload demands. Its seamless integration with other AWS services facilitates event-driven computing models.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: AWS Lambda is fully managed by AWS, reducing operational complexity and administrative   overhead.</li> <li>Minimizing Management and Cost: It optimizes cost by charging only for compute time consumed and automatically   scaling resources as needed.</li> </ul> <p>Documentation: AWS Lambda Documentation</p> <p>Service Limits: AWS Lambda Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/02_AWS_services/#aws-appsync","title":"AWS AppSync","text":"<p>Service Description: AWS AppSync simplifies application development by letting you create scalable APIs that securely access data from multiple sources. It supports real-time and offline capabilities, making it suitable for mobile and web applications.</p> <p>Selection Rationale: We selected AWS AppSync for its managed GraphQL service, which reduces the complexity of API development and data synchronization. Its ability to handle real-time updates aligns with our application\u2019s requirements.</p> <p>Optimization Goals:</p> <ul> <li>AWS Native &amp; Serverless: AWS AppSync is fully managed by AWS, minimizing operational overhead and infrastructure   management.</li> <li>Minimizing Management and Cost: It optimizes cost by providing built-in scaling and pay-as-you-go pricing,   ensuring efficient resource utilization.</li> </ul> <p>Documentation: AWS AppSync Documentation</p> <p>Service Limits: AWS AppSync Limits</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/","title":"Asynchronous Command Handling","text":""},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#overview","title":"Overview","text":"<p>Our application utilizes an asynchronous command handling system through AWS AppSync. When a client sends a command (via GraphQL mutation), it is accepted and dispatched asynchronously, providing a trace ID for monitoring purposes. This approach ensures robust and responsive user interactions, accommodating the inherent challenges of distributed systems.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Command Dispatch: When a command is issued, it is accepted into the domain and dispatched asynchronously. AWS    AppSync abstracts this process, handling the details of asynchronous communication.</p> </li> <li> <p>Trace ID: Upon accepting a command, the system generates and returns a trace ID. This ID is crucial for clients    to monitor the status of their commands.</p> </li> </ol>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#monitoring-with-trace-id","title":"Monitoring with Trace ID","text":"<p>Clients can use the trace ID to monitor the status of their commands through GraphQL queries or subscriptions. The information retrievable includes:</p> <ul> <li>Component name</li> <li>Timestamp</li> <li>Status (accepted, success, error)</li> </ul>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#query-vs-subscription","title":"Query vs. Subscription","text":"<ul> <li>Query: Used when client technology does not support subscriptions, or when viewing information after the command   has been published. Queries are useful for retrospective checks and scenarios requiring fast processing.</li> <li>Subscription: Ideal for real-time monitoring, providing immediate updates on the command\u2019s status.</li> </ul>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#error-handling","title":"Error Handling","text":"<ul> <li>Retries: For technical errors (e.g., network issues), the system retries the command up to three times.</li> <li>Error Events: If retries fail, an error event is thrown, and the context is stored in the deadletter table for   further investigation and handling.</li> </ul>"},{"location":"Technical%20Architecture/02_Technical_Concepts/03_asynchronisity/#benefits-of-asynchronous-handling","title":"Benefits of Asynchronous Handling","text":"<ul> <li>Performance: Fast initial responses to clients improve user experience.</li> <li>Robustness: Handles the fallacies of distributed systems, ensuring reliable operation despite potential issues.</li> <li>Real-time Updates: Clients receive immediate feedback on the status of their commands, aiding in timely   decision-making and interactions.</li> </ul> <p>This asynchronous approach leverages the strengths of AWS AppSync to provide a responsive, robust, and user-friendly system for handling commands and ensuring that clients are well-informed about the status and outcomes of their requests.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/04_keychain/","title":"Understanding Keys and the Keychain Concept","text":"<p>In Draftsman-generated applications, the key concepts are crucial for efficient data management and access.  The important thing to know, is that during modelling you only have to think about the meanigfull business-keys.  We automate the management of the technical keys for you in our architecture. Here's a detailed explanation of the concepts behind key management:</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/04_keychain/#business-key","title":"Business Key","text":"<p>The business key is a meaningful identifier used in the view-store of the application. This key has semantic significance, allowing clients to request data based on recognizable identities. For instance, in a system tracking projects, the business key might encode project identifiers and related hierarchies, making it intuitive for users to query specific projects. This key helps structure the data in a way that aligns with real-world relationships, ensuring efficient querying and filtering in the read model.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/04_keychain/#technical-key","title":"Technical Key","text":"<p>The technical key, unlike the business key, is an internal identifier used primarily in the write model and event stores. It is typically a UUID (Universally Unique Identifier), which provides a unique, immutable reference for entities regardless of their semantic meaning. This key is essential for ensuring the integrity and consistency of the event log, as it remains unchanged even when the business context (and thus the business key) evolves. The technical key supports robust state determination by replaying events, essential for behavior-driven data models.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/04_keychain/#keychain","title":"Keychain","text":"<p>The keychain is a mechanism introduced to map business keys to technical keys seamlessly. It abstracts the complexity of translating user-friendly, meaningful identifiers into system-specific, immutable ones. This translation is critical because while business keys might change (e.g., due to renaming or restructuring), the underlying technical keys must remain consistent to maintain data integrity. The keychain ensures that users can interact with the system using business keys while the system internally manages and references the stable technical keys.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/04_keychain/#the-need-for-a-keychain","title":"The Need for a Keychain","text":"<p>A keychain is necessary for several reasons:</p> <ol> <li>Data Integrity and Consistency: By separating business and technical keys, the system maintains consistent    references in the event log, even when business keys change.</li> <li>User-Friendly Interactions: Users can work with meaningful identifiers (business keys) without worrying about    internal key management complexities.</li> <li>Flexibility and Evolution: As business requirements change, entities can be renamed or restructured without    affecting the underlying system's stability.</li> <li>Efficient Data Access and Management: The keychain enables efficient querying by leveraging meaningful keys for    data access while ensuring the robustness of technical keys for internal operations.</li> </ol> <p>In summary, the keychain concept in Draftsman applications bridges the gap between user-friendly, meaningful identifiers and the system's need for consistent, immutable references, ensuring both usability and reliability. For more details, refer to the original blog post on Draftsman.io.</p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/05_workflow_and_ownership/","title":"Workflow and Ownership","text":"<p>The provided diagram illustrates the workflow and components of the Tracepaper system, detailing the integration between modeling, code generation, and deployment processes, along with the ownership of each part of the infrastructure.</p> <p></p>"},{"location":"Technical%20Architecture/02_Technical_Concepts/05_workflow_and_ownership/#workflow-description","title":"Workflow Description","text":"<ol> <li> <p>Tracepaper: Web Modeler</p> <ul> <li>Users model their applications in the Tracepaper Web Modeler, which is used for project management and to set   build triggers.</li> <li>Ownership: Tracepaper</li> </ul> </li> <li> <p>Tracepaper Backend</p> <ul> <li>Manages project configurations and triggers builds based on the models created in the Web Modeler.</li> <li>Ownership: Tracepaper</li> </ul> </li> <li> <p>Draftsman Build Agent</p> <ul> <li>The build agent pulls the model from the repository, converts it to Python and CloudFormation code, and executes   unit tests. This generated code is then pushed to respective repositories.</li> <li>Ownership: Tracepaper</li> </ul> </li> <li> <p>Model Repo</p> <ul> <li>Contains the domain-specific language (XML) and any custom code. This repository is synced with the Tracepaper   backend and the Draftsman Build Agent.</li> <li>Ownership: Client</li> </ul> </li> <li> <p>Backend Repo</p> <ul> <li>Stores the generated domain logic in Python and the infrastructure-as-code scripts (CloudFormation). These   artifacts are prepared for deployment.</li> <li>Ownership: Client</li> </ul> </li> <li> <p>GUI Repo</p> <ul> <li>Holds the GUI assist framework and custom code, ensuring the user interface aligns with the backend logic.</li> <li>Ownership: Client</li> </ul> </li> <li> <p>AWS Pipeline</p> <ul> <li>This pipeline handles the entire deployment process:<ul> <li>Package: Prepare deployment - Packages the application artifacts.</li> <li>Staging: Deploy Staging - Deploys the application to a staging environment.</li> <li>Quality Gate: Testing - Runs API tests against the staging environment to ensure the application is   functioning correctly.</li> <li>Production: Deploy Production - Finally, deploys the application to the production environment if all   tests pass.</li> </ul> </li> <li>Ownership: Client</li> </ul> </li> </ol> <p>This architecture ensures a seamless and robust development-to-deployment workflow, emphasizing testing, error handling, and real-time client feedback.</p>"},{"location":"Technical%20Architecture/03_Why/01_python/","title":"Why Python","text":"<p>Our goal is to make system design accessible to domain experts, and Python is central to achieving this. Here\u2019s why we chose Python, especially in the context of our AWS-based infrastructure:</p>"},{"location":"Technical%20Architecture/03_Why/01_python/#benefits-of-python","title":"Benefits of Python","text":"<ol> <li> <p>Readability and Simplicity:</p> <ul> <li>Python\u2019s clean and easy-to-understand syntax is perfect for domain experts with limited programming experience.</li> <li>Emphasis on readability reduces the learning curve and helps in quickly grasping complex business-logic.</li> </ul> </li> <li> <p>Rich Ecosystem and Libraries:</p> <ul> <li>Python has a vast ecosystem of libraries.</li> <li>These libraries expedite development and provide powerful tools for solving domain-specific problems.</li> </ul> </li> <li> <p>Integration Capabilities:</p> <ul> <li>Python integrates seamlessly with AWS with the aid of the boto 3 library.</li> </ul> </li> <li> <p>Community Support:</p> <ul> <li>A large and active community provides extensive documentation, tutorials, and support.</li> <li>Community-driven development keeps Python up-to-date with the latest technological advancements and best   practices.</li> </ul> </li> <li> <p>AWS Lambda:</p> </li> <li>Python\u2019s lightweight and efficient nature is ideal for serverless functions, enabling scalable and   cost-effective execution.</li> </ol>"},{"location":"Technical%20Architecture/03_Why/01_python/#python-and-domain-driven-design-ddd","title":"Python and Domain-Driven Design (DDD)","text":"<p>Python\u2019s simplicity and readability align well with Domain-Driven Design (DDD) principles:</p> <ul> <li>Expressive Code: Python\u2019s syntax mirrors domain-specific language, facilitating understanding and contribution   from domain experts.</li> <li>Rapid Prototyping: Python\u2019s dynamic nature and extensive library support enable quick prototyping, allowing domain   experts to see and refine system designs swiftly.</li> </ul>"},{"location":"Technical%20Architecture/03_Why/01_python/#conclusion","title":"Conclusion","text":"<p>Choosing Python helps us bridge the gap between complex system design and domain expertise. Its readability, versatility, rich ecosystem, integration capabilities, and strong community support make it the ideal language for empowering domain experts to contribute effectively to system design, especially within our AWS-based infrastructure.</p> <p>For more detailed insights into why we love Python, please refer to this document.</p>"},{"location":"Technical%20Architecture/03_Why/02_cqrs/","title":"Why CQRS","text":""},{"location":"Technical%20Architecture/03_Why/02_cqrs/#what-is-cqrs","title":"What is CQRS?","text":"<p>Command Query Responsibility Segregation (CQRS) is a design pattern that separates read (query) and write (command) operations into distinct models. This segregation allows for optimized and scalable handling of commands and queries.</p> <p>Key Principles of CQRS:</p> <ul> <li>Commands: Modify the state of the application. Each command represents an action or a change.</li> <li>Queries: Retrieve information without modifying the state. Queries are optimized for read operations.</li> </ul>"},{"location":"Technical%20Architecture/03_Why/02_cqrs/#benefits-of-cqrs","title":"Benefits of CQRS","text":"<ol> <li> <p>Scalability:</p> <ul> <li>Separating reads and writes allows each to be independently scaled, which is particularly useful in high-load   scenarios. For example, read operations can be scaled out with replicas, while write operations can be handled   with dedicated resources.</li> </ul> </li> <li> <p>Performance Optimization:</p> <ul> <li>Queries can be optimized for fast read performance, while commands can be optimized for write efficiency. This   leads to better overall system performance.</li> </ul> </li> <li> <p>Flexibility and Extensibility:</p> <ul> <li>CQRS allows for the use of different data models for reading and writing. This means you can use a highly   normalized model for writes and a denormalized model for reads, optimizing each for their specific use cases.</li> </ul> </li> <li> <p>Simplified Complexity:</p> <ul> <li>By separating commands and queries, each part of the system becomes simpler and more focused. This separation   reduces the cognitive load on developers, making the system easier to understand and maintain.</li> </ul> </li> <li> <p>Enhanced Security:</p> <ul> <li>The separation allows for more granular security controls. Commands can have strict validation and authentication,   while queries can be more permissive.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/03_Why/02_cqrs/#why-cqrs-fits-our-architecture","title":"Why CQRS Fits Our Architecture","text":"<p>We draw inspiration from Domain-Driven Design (DDD) and event storming in our modeling tool. Adopting CQRS fits well with those concepts and aiding the domain narative. Additionally, we can leverage the benefits of CQRS and the good fit with event-driven architectures.</p> <p></p>"},{"location":"Technical%20Architecture/03_Why/02_cqrs/#conclusion","title":"Conclusion","text":"<p>Choosing CQRS allows us to build a robust, scalable, and maintainable system that leverages the strengths of both Event Sourcing and modern cloud infrastructure. This decision aligns with our goal of simplifying complex system design for domain experts, enabling them to focus on delivering business value.</p>"},{"location":"Technical%20Architecture/03_Why/03_event_sourcing/","title":"Why Event Sourcing","text":"<p>Event Sourcing is a powerful architectural pattern that has become an integral part of our system design. Here\u2019s why we chose Event Sourcing:</p>"},{"location":"Technical%20Architecture/03_Why/03_event_sourcing/#benefits-of-event-sourcing","title":"Benefits of Event Sourcing","text":"<ol> <li> <p>Historical Accuracy and Auditing:</p> <ul> <li>Complete History: Every change to the application state is stored as an event, preserving a complete history   of actions.</li> <li>Audit Trail: This provides an accurate and immutable audit trail, crucial for compliance and debugging.</li> </ul> </li> <li> <p>Improved Consistency and Reliability:</p> <ul> <li>Consistency: Event Sourcing ensures that state changes are consistently recorded and applied in the order they   occur.</li> <li>Reliability: By replaying events, systems can recover from failures and ensure reliable state reconstruction.</li> </ul> </li> <li> <p>Enhanced Scalability and Performance:</p> <ul> <li>Scalability: Events can be processed asynchronously and in parallel, enhancing system scalability.</li> <li>Performance: Read models can be optimized for query performance without affecting the write models, thanks to   the separation of concerns.</li> </ul> </li> <li> <p>Flexibility and Extensibility:</p> <ul> <li>Flexibility: Event Sourcing supports evolving requirements by allowing new views or projections to be created   from the event log.</li> <li>Extensibility: New functionality can be added without changing existing events, enabling seamless system   evolution.</li> </ul> </li> <li> <p>Compatibility with CQRS:</p> <ul> <li>CQRS Alignment: Event Sourcing naturally fits with the Command Query Responsibility Segregation (CQRS)   pattern, enhancing our ability to separate read and write concerns.</li> <li>Behavior Modeling: Commands capture intent and behavior, while events capture the results, aligning perfectly   with our DDD approach.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/03_Why/03_event_sourcing/#what-is-event-sourcing","title":"What is Event Sourcing?","text":"<p>Event Sourcing is an architectural pattern where all changes to an application's state are stored as a sequence of events. Instead of just storing the current state, every state-changing action (event) is saved, providing a complete history. This allows for the reconstruction of any past state by replaying these events.</p> <p>Key aspects of Event Sourcing include:</p> <ul> <li>Immutable Events: Each event is immutable and represents a single change in the state.</li> <li>Event Log: A central log where all events are stored in the order they occurred.</li> <li>State Reconstruction: The current state can be rebuilt by replaying the stored events.</li> </ul>"},{"location":"Technical%20Architecture/03_Why/03_event_sourcing/#example","title":"Example:","text":"<p>Consider a bank account:</p> <ul> <li>Event: \"Deposit of $100\" and \"Withdrawal of $50\" are recorded as events.</li> <li>Reconstruction: To get the current balance, the system replays these events starting from an initial balance.</li> <li>initial: $0</li> <li>replay deposit -&gt; $0 + $100</li> <li>replay withdrawal -&gt; $100 - $50</li> <li>balance: $50</li> </ul> <p>Event Sourcing is especially powerful when combined with Command Query Responsibility Segregation (CQRS) and Domain-Driven Design (DDD), providing a robust foundation for complex applications.</p>"},{"location":"Technical%20Architecture/03_Why/03_event_sourcing/#conclusion","title":"Conclusion","text":"<p>Event Sourcing provides a robust framework for building reliable, scalable, and maintainable systems. Its alignment with CQRS and DDD principles makes it an ideal choice for our architecture. This pattern ensures that our systems are not only capable of handling current demands but are also well-prepared for future growth and changes.</p>"},{"location":"Technical%20Architecture/03_Why/04_graphql/","title":"Why GraphQL","text":"Feature/Aspect GraphQL REST Alignment with CQRS Perfect fit with CQRS; commands (mutations) and queries are distinct, reducing cognitive load because no additional level of abstraction is needed. Less aligned; REST has a good fit with CRUD, while we deal with commands (CUD) and queries (R), merging those concepts together increases complexity and cognitive load (adds just another thing that you should think about). Modeling Simplifies modeling by mirroring existing structures; focus on behavior, not data. Requires explicit API modeling, adding a layer of abstraction and complexity. Client Interaction Commands modeled as GraphQL mutations; async processing monitored via subscriptions. CRUD operations on resources; requires inventing virtual resources for modeling, less intuitive. Learning Curve Familiarity with GraphQL required, but benefits in aligning with CQRS and reducing overall complexity. REST is widely known but might not fit well with our implementation of CQRS, requiring additional abstractions."},{"location":"Technical%20Architecture/03_Why/04_graphql/#benefits-of-using-graphql-for-modeling","title":"Benefits of Using GraphQL for Modeling:","text":"<ol> <li> <p>Conceptual Fit with CQRS:</p> <ul> <li>Commands and Mutations: In our modeling tool, commands represent client interactions or requests for changes,   aligning perfectly with GraphQL mutations. The API accepts these commands, returning a trace ID that can be   monitored via GraphQL subscriptions, allowing clients to track the asynchronous processing of these mutations.</li> <li>Materialized Views and Queries: Our tool uses materialized views and projections to handle data viewing,   aligning seamlessly with GraphQL queries.</li> </ul> </li> <li> <p>Reduced Cognitive Load:</p> <ul> <li>Natural API Structure: With GraphQL, the API mirrors the existing structure of the model. This means   developers only need to decide what to expose and configure authentication/authorization methods, significantly   reducing the cognitive load.</li> <li>Behavior Over Data: Our approach focuses on behavior rather than just data manipulation. REST, being more   resource-centric, would require us to invent virtual resources and simulate CRUD operations, which adds   unnecessary complexity.</li> </ul> </li> <li> <p>Potential Challenges:</p> <ul> <li>Less Common Usage: While GraphQL is powerful, it may be less commonly used compared to REST. However, the   benefits in reducing cognitive load and fitting naturally with our CQRS architecture make it a superior choice for our   needs.</li> </ul> </li> </ol> <p>By choosing GraphQL, we ensure that our modeling tool aligns perfectly with our CQRS architecture, providing a more intuitive, efficient, and effective way to design APIs, reducing the complexity and cognitive load on developers. This alignment allows for a more straightforward configuration of commands, queries, and access patterns, leading to a smoother and more productive development experience.</p>"},{"location":"Technical%20Architecture/03_Why/05_serverless/","title":"Why Serverless","text":""},{"location":"Technical%20Architecture/03_Why/05_serverless/#what-is-serverless","title":"What is Serverless?","text":"<p>Serverless computing is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers. AWS offers a range of serverless services such as AWS Lambda, AWS AppSync, and Amazon EventBridge, which allow developers to build and run applications without having to manage infrastructure.</p>"},{"location":"Technical%20Architecture/03_Why/05_serverless/#benefits-of-serverless-on-aws","title":"Benefits of Serverless on AWS","text":"<ol> <li> <p>Scalability:</p> <ul> <li>Automatic Scaling: AWS serverless services automatically scale to handle the demands of your application. AWS   Lambda, for instance, scales in response to incoming traffic, ensuring consistent performance during peak loads   without manual intervention.</li> </ul> </li> <li> <p>Cost Efficiency:</p> <ul> <li>Pay-as-You-Go: With serverless, you only pay for the compute time you consume. AWS charges you based on the   number of requests and the duration your code runs, which can significantly reduce costs compared to traditional   server-based architectures.</li> </ul> </li> <li> <p>Reduced Operational Overhead:</p> <ul> <li>No Server Management: AWS handles the infrastructure management, including server maintenance, patching, and   scaling. This allows your development team to focus on writing code and delivering features rather than managing   servers.</li> </ul> </li> <li> <p>Enhanced Agility:</p> <ul> <li>Faster Deployment: Serverless architecture enables rapid development and deployment cycles. AWS services such   as Lambda and AppSync integrate seamlessly with CI/CD pipelines, allowing for continuous integration and delivery.</li> </ul> </li> <li> <p>Event-Driven Design:</p> <ul> <li>Integration with AWS Services: AWS serverless services are designed to work well together. For example, you   can use Amazon EventBridge to trigger AWS Lambda functions, enabling an event-driven architecture that is highly   responsive to changes and real-time data.</li> </ul> </li> <li> <p>High Availability and Reliability:</p> <ul> <li>Built-in Fault Tolerance: AWS serverless services offer built-in fault tolerance and availability. AWS Lambda   functions are replicated across multiple availability zones, ensuring that your application remains resilient and   highly available.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/03_Why/05_serverless/#why-serverless-fits-our-architecture","title":"Why Serverless Fits Our Architecture","text":"<ol> <li> <p>Alignment with CQRS and Event Sourcing:</p> <ul> <li>Our choice of CQRS and event sourcing patterns fits perfectly with serverless architecture. AWS Lambda functions   handle commands (writes) and queries (reads) efficiently, while services like EventBridge facilitate event-driven   communication.</li> </ul> </li> <li> <p>Seamless API Management:</p> <ul> <li>Using AWS AppSync for our GraphQL API management aligns with our need for flexible and scalable API solutions.   AppSync simplifies the creation and management of GraphQL APIs, allowing us to focus on delivering business value.</li> </ul> </li> <li> <p>Optimized for Domain Experts:</p> <ul> <li>Serverless architecture abstracts away infrastructure complexities, allowing domain experts to focus on system   design and business logic. This aligns with our goal of enabling domain experts to contribute directly to the   system without needing deep technical expertise in infrastructure management.</li> <li>While serverless icreases configuration complexity, this is largly abstracted away by the Tracepaper modeling   concepts.</li> </ul> </li> <li> <p>Optimized TCO: </p> </li> <li>By only paying for what you use and eliminating the need for server maintenance, the Total Cost of       Ownership (TCO) is significantly reduced, making it a financially efficient option.</li> </ol>"},{"location":"Technical%20Architecture/03_Why/05_serverless/#conclusion","title":"Conclusion","text":"<p>Choosing a serverless architecture on AWS provides us with scalability, cost efficiency, reduced operational overhead, and enhanced agility. These benefits align perfectly with our design patterns (CQRS and event sourcing) and our goal of empowering domain experts to focus on delivering business value. AWS's comprehensive suite of serverless services ensures that our applications are robust, scalable, and maintainable, supporting our long-term objectives.</p>"},{"location":"Technical%20Architecture/03_Why/06_aws/","title":"Why AWS","text":""},{"location":"Technical%20Architecture/03_Why/06_aws/#overview","title":"Overview","text":"<p>AWS (Amazon Web Services) is the leading cloud service provider, offering a comprehensive and mature suite of cloud services that cater to various business needs. Our decision to use AWS for our serverless architecture was driven by several key factors that align with our technical and business requirements.</p>"},{"location":"Technical%20Architecture/03_Why/06_aws/#key-benefits-of-aws","title":"Key Benefits of AWS","text":"<ol> <li> <p>Comprehensive Service Offering:</p> <ul> <li>AWS provides a vast array of services that cover all aspects of modern cloud computing, including computing power,   storage, databases, machine learning, and analytics. This comprehensive suite allows us to build, deploy, and   scale our applications efficiently.</li> </ul> </li> <li> <p>Mature Serverless Ecosystem:</p> <ul> <li>AWS has a robust and mature serverless ecosystem, including AWS Lambda for compute, Amazon EventBridge for   event-driven integrations, and AWS AppSync for GraphQL APIs. These services enable us to implement a fully   serverless architecture that is scalable, cost-effective, and easy to manage.</li> </ul> </li> <li> <p>Scalability and Performance:</p> <ul> <li>AWS is designed to handle workloads of any size and complexity. It offers automatic scaling and high availability   across multiple regions, ensuring our applications can meet demand spikes and maintain performance.</li> </ul> </li> <li> <p>Security and Compliance:</p> <ul> <li>AWS provides a secure and compliant cloud infrastructure with numerous certifications and compliance programs.   This ensures that our applications meet industry standards and regulatory requirements, giving us and our   customers confidence in our security posture.</li> </ul> </li> <li> <p>Cost Management:</p> <ul> <li>AWS's pay-as-you-go pricing model allows us to optimize costs by only paying for the resources we use.   Additionally, tools like AWS Cost Explorer and AWS Budgets help us monitor and manage our spending effectively.</li> </ul> </li> <li> <p>Global Reach:</p> <ul> <li>With a global network of data centers, AWS allows us to deploy applications closer to our users, reducing latency   and improving the user experience. This global reach supports our expansion into new markets and regions.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/03_Why/06_aws/#why-aws-fits-our-architecture","title":"Why AWS Fits Our Architecture","text":"<ol> <li> <p>Serverless Capabilities:</p> <ul> <li>Our architecture leverages AWS Lambda for executing code in response to events, Amazon EventBridge for integrating   services, and AWS AppSync for managing GraphQL APIs. This serverless approach reduces operational overhead and   allows us to focus on developing business logic.</li> </ul> </li> <li> <p>Support for CQRS and Event Sourcing:</p> <ul> <li>AWS services align well with our CQRS and event sourcing patterns. AWS Lambda functions handle commands and   queries efficiently, while EventBridge enables event-driven communication. This integration supports our   architectural principles and enhances system scalability and performance.</li> </ul> </li> <li> <p>Integration with AWS Ecosystem:</p> <ul> <li>AWS offers seamless integration with other AWS services, such as S3 for storage, DynamoDB for NoSQL databases, and   IAM for access management. This integration streamlines our development process and ensures a cohesive and   efficient infrastructure.</li> </ul> </li> <li> <p>Developer Tools and Ecosystem:</p> <ul> <li>AWS provides a rich set of developer tools, including AWS CodePipeline, AWS CodeBuild, and AWS CloudFormation,   which support our CI/CD workflows and infrastructure as code practices. These tools enhance our development   efficiency and deployment automation.</li> </ul> </li> </ol>"},{"location":"Technical%20Architecture/03_Why/06_aws/#why-we-dont-support-other-cloud-vendors","title":"Why We Don't Support Other Cloud Vendors","text":"<p>We have chosen AWS as our exclusive cloud provider due to its unmatched combination of services, scalability, and integration capabilities, which perfectly align with our technical and business needs. While other cloud vendors offer competitive services, none match the maturity and breadth of AWS's offerings in the serverless ecosystem. Transitioning to or supporting multiple cloud providers would introduce unnecessary complexity, increase operational overhead, and dilute our focus on optimizing our AWS-based architecture. Consequently, we do not foresee supporting other cloud vendors in the short term.</p>"},{"location":"Technical%20Architecture/03_Why/06_aws/#conclusion","title":"Conclusion","text":"<p>Choosing AWS as our cloud provider offers numerous benefits, including a comprehensive service offering, a mature serverless ecosystem, scalability, security, and global reach. These advantages align with our architectural principles and business objectives, enabling us to build robust, scalable, and secure applications while focusing on delivering value to our customers. AWS's extensive suite of services and tools supports our goal of simplifying system design for domain experts and achieving operational excellence.</p>"},{"location":"inspirations/01_ddd/","title":"Domain-Driven Design (DDD)","text":"<p>Domain-Driven Design (DDD) is an approach to software development that emphasizes collaboration between technical and domain experts to model complex domains effectively. The core concept is to create a shared understanding of the domain through ubiquitous language and to structure the software around the domain model. DDD helps in aligning the software design with business needs and making the system more maintainable and adaptable to changes.</p> <p>For further explanation, visit: Domain-Driven Design</p>"},{"location":"inspirations/01_ddd/#mapping-ddd-concepts-to-tracepaper-concepts","title":"Mapping DDD Concepts to Tracepaper Concepts","text":"DDD Concept Description Tracepaper Concept Description Entities Objects with a distinct identity Aggregate Document Clustered entities managed as a single unit Value Objects Objects defined by their attributes Not explicitly mapped N/A Aggregates Cluster of entities and value objects Aggregates, Behavior Flows Clustered entities with business rules Repositories Mechanisms for accessing aggregates Abstracted by technical framework Handled through event sourcing Services Operations that don't belong to entities Automations Logic and operations Factories Methods for creating aggregates Abstracted by technical framework Handled through event sourcing Domain Events Events indicating something has happened Events Significant occurrences that trigger changes Commands Instructions to perform an action Commands Requests for state changes or operations Bounded Contexts Logical boundaries within the domain Subdomains Defines scope and boundaries of the model <p>This table shows how the core concepts of Domain-Driven Design are integrated into the Tracepaper modeling tool,  facilitating a clear and structured approach to system design.</p>"},{"location":"inspirations/02_event_storming/","title":"Event Storming","text":"<p>Event Storming is a workshop-based approach to software modeling and process discovery. It involves gathering stakeholders, developers, and domain experts to collaboratively explore complex business domains and processes through the visualization of domain events.</p> <p>Key Principles of Event Storming:</p> <ol> <li>Events as Core Elements: Events represent significant occurrences in the domain that drive business processes.</li> <li>Collaborative Discovery: Engaging all relevant stakeholders ensures a shared understanding and uncovers hidden    requirements.</li> <li>Big Picture and Detail: The approach can be used to map out high-level processes or dive into detailed workflow    steps.</li> </ol>"},{"location":"inspirations/02_event_storming/#why-event-storming-fits-our-modeling-tool","title":"Why Event Storming Fits Our Modeling Tool","text":"<ol> <li> <p>Alignment with Domain-Driven Design (DDD):</p> <ul> <li>Event Storming is deeply rooted in DDD principles, emphasizing the importance of understanding domain events and   behaviors. Our modeling tool, inspired by DDD, naturally aligns with this approach by focusing on commands,   events, and projections.</li> </ul> </li> <li> <p>Enhancing Communication:</p> <ul> <li>By involving domain experts and stakeholders in the modeling process, Event Storming ensures that the resulting   models accurately reflect business needs and processes. This collaborative approach reduces misunderstandings and   ensures a shared vision.</li> </ul> </li> <li> <p>Improving Model Accuracy:</p> <ul> <li>Visualizing the entire process through events allows for a comprehensive understanding of the domain. This   holistic view ensures that our models are not only accurate but also adaptable to changing business requirements.</li> </ul> </li> <li> <p>Facilitating Agile Development:</p> <ul> <li>Event Storming supports iterative and incremental development. The insights gained from these sessions inform our   modeling tool, making it easier to adapt and refine models as the project evolves.</li> </ul> </li> </ol>"},{"location":"inspirations/02_event_storming/#the-workshop","title":"The workshop","text":"<ol> <li> <p>Events (Orange sticky notes):</p> <ul> <li>What: Describe the significant occurrences within the system.</li> <li>Example: \"Order Placed\", \"Payment Received\".</li> <li>Role: They identify the key actions happening in the system and form the core of the model.</li> </ul> </li> <li> <p>Commands (Blue sticky notes):</p> <ul> <li>What: Describe the actions taken by a user or system to trigger an event.</li> <li>Example: \"Place Order\", \"Cancel Booking\".</li> <li>Role: They indicate which commands are necessary to trigger events and help define the interactions within the   system.</li> </ul> </li> <li> <p>Aggregates (Yellow sticky notes):</p> <ul> <li>What: Represent the collection of data and logic involved in handling events.</li> <li>Example: \"Order\", \"Customer\".</li> <li>Role: They structure the data and ensure that events are handled correctly.</li> </ul> </li> <li> <p>Read Models/Views (Green sticky notes):</p> <ul> <li>What: Describe how data is presented to the user.</li> <li>Example: \"Order History\", \"Product Catalog\".</li> <li>Role: They show the system's output and help understand how data is displayed.</li> </ul> </li> <li> <p>Policies (Purple sticky notes):</p> <ul> <li>What: Describe rules or processes that respond to events and may trigger other commands.</li> <li>Example: \"When Order Placed, Send Confirmation Email\".</li> <li>Role: They define the business logic and automated reactions within the system.</li> </ul> </li> <li> <p>External Systems (Red sticky notes):</p> <ul> <li>What: Represent systems outside your own system that you integrate with.</li> <li>Example: \"Payment Gateway\", \"Customer Support System\".</li> <li>Role: They help identify dependencies and integrations with external systems.</li> </ul> </li> <li> <p>Issues (Pink sticky notes):</p> <ul> <li>What: Describe problems or questions that arise during the modeling process.</li> <li>Example: \"How to handle failed payments?\", \"What happens if a customer cancels?\".</li> <li>Role: They ensure that open issues are noted and addressed later.</li> </ul> </li> </ol> <p>How to Use These Sticky Notes in Event Storming:</p> <ol> <li> <p>Identify Events: Start by writing down all the key events (orange sticky notes) that occur in your system and    place them in chronological order on a large surface like a wall or whiteboard.</p> </li> <li> <p>Add Commands: Identify the actions (blue sticky notes) that trigger these events and place them before the    corresponding events.</p> </li> <li> <p>Define Aggregates: Add yellow sticky notes to show which entities are involved in the events and commands.</p> </li> <li> <p>Design Read Models/Views: Use green sticky notes to illustrate how data is presented to users.</p> </li> <li> <p>Describe Policies: Add purple sticky notes to capture business rules and automated responses.</p> </li> <li> <p>Identify External Systems: Use red sticky notes for systems you integrate with.</p> </li> <li> <p>Mark Issues: Use pink sticky notes for questions or problems that need to be addressed.</p> </li> </ol> <p>By using this method, you create a visual and structured overview of your system, which helps in understanding and communicating the complexity and interactions within it.</p>"},{"location":"inspirations/02_event_storming/#mapping-to-tracepaper-concepts","title":"Mapping to Tracepaper concepts","text":"Event Storming Sticky Note Description Tracepaper Concept Description Domain Events Significant occurrences in the domain Events Represent occurrences that trigger changes Commands Actions that trigger domain events Commands Requests for a state change or operation Aggregates Entities that encapsulate business rules Aggregates, Behavior Flows Clustered entities with transactional consistency Policies Rules or business logic Automations, Python Modules (Reactive) behavior Read Models Projections of the current state Materialized Views, Projections Optimized data structures for queries Actors People or systems interacting with the domain N/A N/A External Systems Systems outside the domain boundary Automations Connections to external systems and services Bounded Contexts Logical boundaries within the domain Subdomains Bundle related aggregates"},{"location":"inspirations/02_event_storming/#conclusion","title":"Conclusion","text":"<p>Event Storming is a powerful technique for discovering and modeling complex business processes. Its alignment with Domain-Driven Design and collaborative nature make it an ideal fit for our modeling tool, ensuring accurate, flexible, and shared understanding of domain models. This approach enhances our ability to deliver robust, business-aligned software solutions.</p> <p>For more detailed information, you can refer to this Event Storming guide which provides a comprehensive overview of the methodology.</p>"}]}